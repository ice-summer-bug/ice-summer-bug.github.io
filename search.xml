<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[mysql InnoDB 锁]]></title>
    <url>%2F2019%2F07%2F01%2F%E6%8A%80%E6%9C%AF%2Fmysql%2Fmsyql-innodb-locking%2F</url>
    <content type="text"><![CDATA[InnoDB 支持的锁 共享锁(S)和排它锁(X) 意向锁 行锁 间隙锁（GAP 锁） Next-Key Locks 插入意向锁 自增长字段锁 Predicate Locks for Spatial Indexes 行级锁行级锁，实质上是锁定在一个索引记录上，select ... for update 会加锁, delete, update 也会加锁 GAP 锁间隙锁也是作用在索引记录上，但是不同的是， GAP 锁锁定的是两条索引记录之间的间隙，而没有锁定索引记录本身 Next-Key 锁Next-Key 锁是行级锁和 GAP 锁的集合，它会锁定命中的索引记录以及这个索引记录前的间隙。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic Search 集群管理之动态扩容收缩]]></title>
    <url>%2F2019%2F03%2F16%2F%E6%8A%80%E6%9C%AF%2Felastic-search%2Felastic-search%E9%9B%86%E7%BE%A4%E5%88%9D%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Elastic Search 集群简单介绍Elastic Search 节点重启注意：本人使用的是 Elastic Search 的版本是 5.6.x, 不同版本的操作 API 可能不一致 禁止分配分片 1234567curl -X PUT "localhost:9200/_cluster/settings" -H 'Content-Type: application/json' -d'&#123; "transient": &#123; "cluster.routing.allocation.enable": "none" &#125;&#125;' 停止不必要的索引的创建，并发出同步刷新请求（非必要步骤） 在重启节点的过程中，可以不停止索引的创建；但是如果你停止不必要的索引的创建，并发出同步刷新请求的话，分片的回复将会更加快速 1curl -X POST "localhost:9200/_flush/synced" 同步刷新操作只能是尽最大努力执行成功，如果有挂起的索引操作，这个操作将会失败，建议多次执行同步刷新请求 修改节点配置，重启节点 重启完节点之后可以查看节点是否重启成功 1curl -X GET "localhost:9200/_cat/nodes" 恢复节点分配 1234567curl -X PUT "localhost:9200/_cluster/settings" -H 'Content-Type: application/json' -d'&#123; "transient": &#123; "cluster.routing.allocation.enable": "all" &#125;&#125;' 等待节点回复 使用如下指令查看集群状态，知道集群状态从 yellow 变为 green 1curl -X GET "localhost:9200/_cat/health" 重复上述步骤，重启其他需要重启的节点 Elastic Search 集群动态新增节点1. 准备在新的机器启动 Elastic Search 节点1.1 从原有节点上拷贝文件到新机器上1.2 修改如下配置示例如下：将 discovery.zen.minimum_master_nodes 加一，在 discovery.zen.ping.unicast.hosts: 中新增 host new.ip.3 12discovery.zen.minimum_master_nodes: 3discovery.zen.ping.unicast.hosts: ["ip1","ip2", "new.ip.3"] 1.3 启动此节点1./bin/elasticsearch -d 1.4 查看节点启动情况1curl -X GET "localhost:9200/_cat/nodes" 1.5 等待集群回复使用如下指令查看集群状态，知道集群状态从 yellow 变为 green 1curl -X GET "localhost:9200/_cat/health" 2. 逐个重启其他节点具体步骤见上述重启步骤，重启 步骤3 中修改 config/elasticsearch.yml 文件 将 discovery.zen.minimum_master_nodes 加一，在 discovery.zen.ping.unicast.hosts: 中新增 host new.ip.3 12discovery.zen.minimum_master_nodes: 3discovery.zen.ping.unicast.hosts: ["ip1","ip2", "new.ip.3"] Elastic Search 集群动态下线节点1. 告知集群分配分配的时候排除待下线节点示例：排除 ip 10.0.0.1 12345curl -XPUT localhost:9200/_cluster/settings -H 'Content-Type: application/json' -d '&#123; "transient" :&#123; "cluster.routing.allocation.exclude._ip" : "10.0.0.1" &#125;&#125;';echo 2. 查看分片分配情况1$curl -XGET 'http://localhost:9200/_nodes/NODE_NAME/stats/indices?pretty' 执行上述指令得到 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139&#123; "_nodes": &#123; "total": 1, "successful": 1, "failed": 0 &#125;, "cluster_name": "CLUSTER_NAME", "nodes": &#123; "H447JUPgRSKZKQdzQ6H0gg": &#123; "timestamp": 1551617699642, "name": "e93d15417", "transport_address": "IP:9300", "host": "IP", "ip": "IP:9300", "roles": [ "master", "data" ], "indices": &#123; "docs": &#123; "count": 0, "deleted": 0 &#125;, "store": &#123; "size_in_bytes": 0, "throttle_time_in_millis": 0 &#125;, "indexing": &#123; "index_total": 0, "index_time_in_millis": 0, "index_current": 0, "index_failed": 0, "delete_total": 0, "delete_time_in_millis": 0, "delete_current": 0, "noop_update_total": 0, "is_throttled": false, "throttle_time_in_millis": 0 &#125;, "get": &#123; "total": 0, "time_in_millis": 0, "exists_total": 0, "exists_time_in_millis": 0, "missing_total": 0, "missing_time_in_millis": 0, "current": 0 &#125;, "search": &#123; "open_contexts": 0, "query_total": 0, "query_time_in_millis": 0, "query_current": 0, "fetch_total": 0, "fetch_time_in_millis": 0, "fetch_current": 0, "scroll_total": 0, "scroll_time_in_millis": 0, "scroll_current": 0, "suggest_total": 0, "suggest_time_in_millis": 0, "suggest_current": 0 &#125;, "merges": &#123; "current": 0, "current_docs": 0, "current_size_in_bytes": 0, "total": 0, "total_time_in_millis": 0, "total_docs": 0, "total_size_in_bytes": 0, "total_stopped_time_in_millis": 0, "total_throttled_time_in_millis": 0, "total_auto_throttle_in_bytes": 4320133120 &#125;, "refresh": &#123; "total": 641, "total_time_in_millis": 66, "listeners": 0 &#125;, "flush": &#123; "total": 237, "total_time_in_millis": 1 &#125;, "warmer": &#123; "current": 0, "total": 0, "total_time_in_millis": 0 &#125;, "query_cache": &#123; "memory_size_in_bytes": 0, "total_count": 0, "hit_count": 0, "miss_count": 0, "cache_size": 0, "cache_count": 0, "evictions": 0 &#125;, "fielddata": &#123; "memory_size_in_bytes": 0, "evictions": 0 &#125;, "completion": &#123; "size_in_bytes": 0 &#125;, "segments": &#123; "count": 0, "memory_in_bytes": 0, "terms_memory_in_bytes": 0, "stored_fields_memory_in_bytes": 0, "term_vectors_memory_in_bytes": 0, "norms_memory_in_bytes": 0, "points_memory_in_bytes": 0, "doc_values_memory_in_bytes": 0, "index_writer_memory_in_bytes": 0, "version_map_memory_in_bytes": 0, "fixed_bit_set_memory_in_bytes": 0, "max_unsafe_auto_id_timestamp": -9223372036854775808, "file_sizes": &#123;&#125; &#125;, "translog": &#123; "operations": 0, "size_in_bytes": 0 &#125;, "request_cache": &#123; "memory_size_in_bytes": 0, "evictions": 0, "hit_count": 0, "miss_count": 0 &#125;, "recovery": &#123; "current_as_source": 0, "current_as_target": 0, "throttle_time_in_millis": 90997 &#125; &#125; &#125; &#125;&#125; 可以看到查询结果中 indices 大部分都是0 ，这是因为步骤 1 排除了当前节点后触发了分配的重新分配 3. 查看集群健康情况1curl -XGET 'http://127.0.0.1:9200/_cluster/health?pretty' 确保集群集状态是 green 4. 重启现有节点4.1 禁止分配分片1234567curl -X PUT "localhost:9200/_cluster/settings" -H 'Content-Type: application/json' -d'&#123; "transient": &#123; "cluster.routing.allocation.enable": "none" &#125;&#125;' 4.2 停止不必要的索引的创建，并发出同步刷新请求（非必要步骤）在重启节点的过程中，可以不停止索引的创建；但是如果你停止不必要的索引的创建，并发出同步刷新请求的话，分片的回复将会更加快速 1curl -X POST "localhost:9200/_flush/synced" 4.3 修改节点配置，重启节点修改 config/elasticsearch.yml 文件 示例如下：将 discovery.zen.minimum_master_nodes 减一，在 discovery.zen.ping.unicast.hosts: 中去除 host new.ip.3 12discovery.zen.minimum_master_nodes: 2discovery.zen.ping.unicast.hosts: ["ip1","ip2"] 重启完节点之后可以查看节点是否重启成功 1curl -X GET "localhost:9200/_cat/nodes" 4.4 恢复节点分配12345678curl -X PUT "localhost:9200/_cluster/settings" -H 'Content-Type: application/json' -d'&#123; "transient": &#123; "cluster.routing.allocation.enable": "all", "cluster.routing.allocation.exclude._ip" : "new.ip.3" &#125;&#125;' 4.5 等待节点回复使用如下指令查看集群状态，直到集群状态从 yellow 变为 green 1curl -X GET "localhost:9200/_cat/health" 4.6 重复上述步骤，逐步重启其他线上节点5. 下线节点12ps -ef | grep elasticsearchkill PID 6. 修改分片分配配置删除配置 &quot;cluster.routing.allocation.exclude._ip&quot; : &quot;new.ip.3&quot; 1234567curl -X PUT "localhost:9200/_cluster/settings" -H 'Content-Type: application/json' -d'&#123; "transient": &#123; "cluster.routing.allocation.exclude._ip" : null &#125;&#125;' 注意排除分配分片的节点时候除了指定 ip 还可以指定节点名称 name 以及 host 还可以指定ip 匹配规则 如： 1234567891011curl -XPUT localhost:9200/_cluster/settings -H 'Content-Type: application/json' -d '&#123; "transient" :&#123; "cluster.routing.allocation.exclude._ip" : "10.0.0.*" &#125;&#125;';echocurl -XPUT localhost:9200/_cluster/settings -H 'Content-Type: application/json' -d '&#123; "transient" :&#123; "cluster.routing.allocation.exclude._name" : "NODE_NAME" &#125;&#125;';echo 参考资料Rolling upgradesHow to remove node from elasticsearch cluster on runtime without down timeShard Allocation FilteringElasticsearch: HOW-TO delete a (cluster) setting]]></content>
      <categories>
        <category>elastic-search</category>
      </categories>
      <tags>
        <tag>中间件</tag>
        <tag>elastic search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何修改jar包中文件]]></title>
    <url>%2F2019%2F03%2F01%2F%E6%8A%80%E6%9C%AF%2Fjava%2Fjar%E5%8C%85%E4%B8%AD%E6%96%87%E4%BB%B6%E4%BF%AE%E6%94%B9%2F</url>
    <content type="text"><![CDATA[修改jar 包中的文件1234567891011121314151617#1.列出jar包中的文件清单jar tf genesys_data_etl-0.0.1-SNAPSHOT.jar#2.提取出内部jar包的指定文件jar xf genesys_data_etl-0.0.1-SNAPSHOT.jar BOOT-INF/classes/realtime/t_ivr_data_bj.json#3.然后可以修改文件vim BOOT-INF/classes/realtime/t_ivr_data_bj.json#4.更新配置文件到内部jar包.(存在覆盖，不存在就新增)jar uf genesys_data_etl-0.0.1-SNAPSHOT.jar BOOT-INF/classes/realtime/t_ivr_data_bj.json #4.1更新内部jar包到jar文件jar uf genesys_data_etl-0.0.1-SNAPSHOT.jar 内部jar包.jar #5.可以查看验证是否已经更改vim genesys_data_etl-0.0.1-SNAPSHOT.jar]]></content>
      <categories>
        <category>jar</category>
      </categories>
      <tags>
        <tag>jar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 探究之路 ———— State]]></title>
    <url>%2F2019%2F01%2F26%2F%E6%8A%80%E6%9C%AF%2Fflink%2Fflink%E4%B9%8Bstate%2F</url>
    <content type="text"><![CDATA[Flink 中的 state(状态)flink 提供了完善的数据保存机制，那就是 state，flink 中的 Function 和 Operator 在处理输入的过程中，将计算结果或存储到 state 中 state 的分类flink 中的 state 分为两种类型： Key State 和 Operater State。 Key StateKey State 总是和 key 相关联，只有 KeyedStream 中的 Funciton 和 Operator 能使用它。 你可以将 Key State 看作是已经被分割的 Operator State，而且每个 state 分区将对应一个 key，每一个 Key State 在逻辑上绑定一个唯一的结构复杂的 key，这个 key 的组成是 &lt;parallel-operator-instance, key&gt;，这里的 key 可以在代码中自有指定生成方法；而且因为每一个 key 都“属于”一个 keyed-operaotr 的并行实例（也可称为 sub-task）；我们可以将这个 key 的机构简化成 &lt;operator, key&gt;。 Key State 可以更进一步被组织成 Key Groups, Key Group 是 flink 重新分配 Key State 的最小单位；Key Group 的数量和定义的最大并行度的数量一致（PS: 这里的最大并行度是当前 Operator 的并行度吗？还是当前运行环境中的最大并行度？），在运行时的 keyed-operator 的每一个并行实例会和 key 一起为一个或多个 Key Group 工作。 *对于任何一个支持并行的 Operator，key 相同的事件，将被同一个并行实例处理，也只会出现在同一个 task slot 中，Flink 将 Key State 组织成 Key Groups，每个 Key 及其对应的 Key State 永久和一个特定的 Key group 相绑定，并且，每一个 task slot 负责处理一个或者多个 Key group 中的 key。 当 Flink job 被重新调节时候，Operator 的并行度发生变化，需要对 state 进行重新分配，Flink 承诺 state 的重新分配，将按照 Key group 为最小单位进行重新分配 * Operator State每一个 Operator State 都会和一个 Operator 的并行实例绑定，Kafka Consumer 就是一个很好的使用flink 的 Operator State 的示例，每一个 Kafka Consumer 的并行实例都维护了一个 Map 类型的 Operator State，Map 的构成是 Kafka 的 Topic Partition 对应的 Offset。 Operator State 接口支持当并行度发生变化的时候，在并行示例之间进行重新分配，这里有多种不同的重新分配的方法。 疑问：`Key State` 和 `Operator State` 在多个并行实例中是共享的？还是各自维护Raw and Managed State（原生和托管的状态)Key State 和 Operator State 有两种存在方式 Raw State(原生状态) 和 Managed State(托管状态)。 Manager State（托管状态）：托管状态有flink 运行时控制的数据结构表示，比如内部哈希表或者 RocksDB，举例说明，ListState(List 结构的状态)、MapState（Map 结构的状态），flink 运行是将对状态进行编码并写入到 checkpoint 中。 Raw State(原生状态)：原生状态将被 Operator 保存在它本身的数据结构中，当 checkpoint 被触发的时候，原生状态将以字节队列的形式写入到 checkpoint 中，flink 不知道原生状态的数据结构，仅能看到原生的字节数组。 所有数据流函数都能使用托管状态，但是只有实现 operators 才能使用原生状态。推荐使用托管状态，因为当并行度发生变化的时候，flink 可以重新分配托管状态，同时还能更好的管理内存。 使用托管的 Key State托管的 Key State 接口支持方位当前输入元素的 Key 范围内的不同类型的状态， 这意味着 Key State 只能在 KeyedStream 上使用，通过调用 keyBy(...) 方法获得 KeyedStream。 下面是各种类型的 Key State 的介绍，然后再看在程序中该如何使用它们。 ValueState&lt;T&gt;：这里维护了单个 T 类型的可更新、可查看的状态值（由于上述输入元素的 key 的限定，每个 key 对应一个值），使用 update(T t) 方法更新状态， 使用 value() 方法获取状态值。 ListState&lt;T&gt;：这里维护一个 T 类型的状态数组，通过 update(List&lt;T&gt; list) 更新整个状态数组，通过 add(T t) 来追加状态，通过 addAll(List&lt;T&gt; list) 来追加多个状态，通过 get() 来获取整个状态数组。 ReduceState&lt;T&gt;：维护一个 T 类型的聚合状态值，聚合通过 add(T t) 方法添加的所有状态值，状态值的聚合通过自定义 ReduceFunction 来实现，通过 get() 来获取聚合状态值 org.apache.flink.api.common.functions.ReduceFunction&lt;T&gt; T ReduceFunction#reduce(T value1, T value2): 将两个 value 值进行合并，返回合并结果 AggregatingState&lt;IN, OUT&gt;：通过 add(IN in) 方法添加状态输入值，通过 自定义的 AggregateFunction&lt;IN, ACC, OUT&gt; 对输入值进行合并之后返回 OUT 类型的状态值，AggregatingState 和 ReduceState 类似，区别在于 AggregatingState 合并操作的输入数据类型和查询结果的数据类型可以不一致。 org.apache.flink.api.common.functions.AggregateFunction&lt;IN, ACC, OUT&gt; &lt;IN&gt;：输入类型 &lt;ACC&gt;：自定义累计计数器的类型 &lt;OUT&gt;：累计结果的类型 ACC createAccumulator()：创建一个累计计数器，开始合并数据 ACC add(IN value, ACC accumulator)：将输入数据累加到累计计数器上 OUT getResult(ACC accumulator)：查询累计计数结果 ACC merge(ACC a, ACC b)：合并两个累计计数器的数据 MapState&lt;UK, UV&gt;：维护 map 类型的状态集合，支持 map 类型数据结构的基本操作，支持 put(UK k, UV v)，putAll(Map&lt;UK, UV&gt;)，get(UK k)，entries()，keys()，values() 等操作。 上述所有 State 都支持 通过 clear() 方法类清除已经保存的状态数据 需要注意的是： 上述数据对象都是用来和状态数据交互的，无论状态数据是在内存中还是在硬盘中，或者其他存储介质中。 Key State 中，所有状态数据都和 key 关联，状态数据查询结果取决于输入数据的key，所以在用户自定义函数（UDF, User Defined Function）中查询状态数据的时候，如果输入数据的 key 不一致，查询的状态数据结果也会不一致。 在创建上述类型的 Key Stated 的时候需要用到 StateDescriptor，StateDescriptor 包含了状态的 名称，状态值数据类型，有时候还会包含一个 UDF，例如 ReduceFunction。 我们可以在 RichFunction 中通过 RuntimeContext 访问状态数据。RuntimeContext 提供如下方法创建各种类型的状态数据。 ValueState getState(ValueStateDescriptor) ReducingState getReducingState(ReducingStateDescriptor) ListState getListState(ListStateDescriptor) AggregatingState&lt;IN, OUT&gt; getAggregatingState(AggregatingStateDescriptor&lt;IN, ACC, OUT&gt;) MapState&lt;UK, UV&gt; getMapState(MapStateDescriptor&lt;UK, UV&gt;) 状态生存时间(State Time-To-Live, TTL)每一个 Key State 都可以设置一个生存时间，如果一个状态数据设置了生存时间，当状态数据过期的时候，转态数据将被清除。 所有类型的状态数据都支持 生存时间(Time-To-Live)，对于 List 和 Map 类型的状态数据，生存时间 的粒度为集合中的单个数据。 我们通过 StateTtlConfig 来配置 状态数据的生存时间。 123456789101112import org.apache.flink.api.common.state.StateTtlConfig;import org.apache.flink.api.common.state.ValueStateDescriptor;import org.apache.flink.api.common.time.Time;StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) .build();ValueStateDescriptor&lt;String&gt; stateDescriptor = new ValueStateDescriptor&lt;&gt;("text state", String.class);stateDescriptor.enableTimeToLive(ttlConfig); StateDescriptor 默认是不支持设置状态生存时间的，通过 enableTimeToLive(StateTtlConfig ttlConfig) 方法来设置状态生存时间。 StateTtlConfig 中提供如下属性 StateTtlConfig.UpdateType： 配置状态生存时间的 上次访问时间 的计算方式，当前时间和上次访问时间的时间间隔决定状态是否过期，取值如下： Disabled：不设置状态生存时间 OnCreateAndWrite：将创建或者写入时间作为 上次访问时间，默认使用该选项 OnReadAndWrite：将读取或者写入时间作为 上次访问时间 StateTtlConfig.StateVisibility：配置失效状态数据的可见性，决定当获取的状态数据已经失效时，返回什么？配置项取值如下： NeverReturnExpired 不返回任何过期的状态数据，默认使用该选项 ReturnExpiredIfNotCleanedUp 返回过期但是没有被清除的状态数据 StateTtlConfig.TimeCharacteristic：配置状态数据生存时间使用的时间类型，目前只支持 ProcessingTime 注意： 1. `State backends` 将状态数据的最后修改时间戳和用户状态数据一起存储，这意味着`状态生存时间` 这一特性将增加状态存储的资源消耗。 堆存储的 `State backends` 在内存中存储一个的额外的 java 对象，它包含一个long 类型的时间戳和一个用户状态数据对象。 而 `RocksDB` 存储的`State backend` 则是为每一个状态数据（包含List或Map中的单个数据）额外存储 8 bytes 长度的时间戳数据 2. 目前状态生存时间只支持 `ProcessingTime` 时间类型 3. 当恢复状态的时候，如果状态数据之前没有设置状态生存时间，现在改为设置了状态生存时间； 或者于此相反的情况下，将出现 `兼容性错误`，抛出 `StateMigrationException` 4. 状态生存时间的配置数据，不是 `checkpoint` 和 `savepoint` 的一部分。但是它决定了当前运行任务如何对待 `checkpoint` 或者 `savepoint`。 5. 声明生存时间的 map 类型的状态数据支持可序列话的null 类型的状态数据。如果 null 数据不支持序列化，可以使用 `NullableSerializer` 来包装数据，代价是在 `Serialized from` 增加额外的字节。状态数据的清除目前状态数据只有通过 ValueState#value() 方法被读取的状态数据会被删除，也就是说，如果状态数据一直没有被读取，就不会被删除！！！当然这个问题后期应该会被修复。目前的API 只支持在获取完成状态快照的时候清理状态数据的大小（也就是这时候清除过期的状态数据），具体配置方法如下： 12345678import org.apache.flink.api.common.state.StateTtlConfig;import org.apache.flink.api.common.time.Time;StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupFullSnapshot() .build(); 但是需要注意的是，当我们使用增量型 checkpoint，不支持cleanupFullSnapshot()！ 问题：8 bytes 不够存储时间戳吧？？？ 使用托管的 Operator State想要使用托管的 Operator State，Operator 或者 Function 需要实现 CheckpointedFunction 接口，或者 ListCheckPointed&lt;T extend Serializable&gt; 接口。 CheckpointedFunctionCheckpointedFunction 提供以下两个方法： 12345678910111213/** * 每次需要生成的时候调用该方法 */void snapshotState(FunctionSnapshotContext context) throws Exception;/** * 当用户自定义函数(User-Defined-Function,UDF) 初始化的时候调用此方法 * 分为两种被调用的场景 * 1. 自定义函数第一次被初始化的时候被调用 * 2. 从历史 checkpoint 中恢复数据的时候会被调用 * 因此此方法被用于初始化状态数据或者从checkpoint中恢复状态数据 */void initializeState(FunctionInitializationContext context) throws Exception; 现在 Operator State 只支持 list 类型的状态数据，其中保存的都必须是可序列化的状态数据，而且list 中的数据相互独立，这意味着 Operator State 支持重新分配。不同的 Operator State 访问方法决定了不同的重新分配的方式： Even-split redistribution：每个算子返回一个状态数据的list集合。 当从 checkpoint 中恢复状态数据或者进行重新分配的时候，List 类型的状态数据将被切割成和并行示例数量一致的子链表，每个算子获得一个子链表，子链表可能为空，也可能包含一个或多个数据。例如，当一个并行度为1 算子，拥有一个 ListState，其中包含数据 e1、e2，当算子的并行改成 2 的时候，发生重新分配，ListState 被切分成两个子链，算子的并行实例1 获得包含 e1 的ListState, 并行实例2 获得包含 e2 的ListState。 Union redistribution：每个算子返回一个状态数据的 List 集合，包含所理由的状态数据。当发生重新分配或者从 checkpoint 中恢复状态数据的时候，每个算子都将获取到全部的状态数据。 Operator State 的使用方法和 Key State 类似，都是通过 StateDescriptor 来初始化状态数据，下面这个例子使用了 Even-split redistribution 类型的重新分配模式的 Operator State 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class BufferingSink implements SinkFunction&lt;Tuple2&lt;String, Integer&gt;&gt;, CheckpointedFunction &#123; private final int threshold; private transient ListState&lt;Tuple2&lt;String, Integer&gt;&gt; checkpointedState; private List&lt;Tuple2&lt;String, Integer&gt;&gt; bufferedElements; public BufferingSink(int threshold) &#123; this.threshold = threshold; this.bufferedElements = new ArrayList&lt;&gt;(); &#125; @Override public void invoke(Tuple2&lt;String, Integer&gt; value) throws Exception &#123; bufferedElements.add(value); if (bufferedElements.size() == threshold) &#123; for (Tuple2&lt;String, Integer&gt; element: bufferedElements) &#123; // send it to the sink &#125; bufferedElements.clear(); &#125; &#125; @Override public void snapshotState(FunctionSnapshotContext context) throws Exception &#123; checkpointedState.clear(); for (Tuple2&lt;String, Integer&gt; element : bufferedElements) &#123; checkpointedState.add(element); &#125; &#125; @Override public void initializeState(FunctionInitializationContext context) throws Exception &#123; ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor = new ListStateDescriptor&lt;&gt;( "buffered-elements", TypeInformation.of(new TypeHint&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;&#125;)); checkpointedState = context.getOperatorStateStore().getListState(descriptor); /** * 当从历史 checkpoint 中恢复状态数据的时候，需要读取保存历史状态数据 * isRestored 方法可以查询当前是否缓存了历史状态数据 */ if (context.isRestored()) &#123; for (Tuple2&lt;String, Integer&gt; element : checkpointedState.get()) &#123; bufferedElements.add(element); &#125; &#125; &#125;&#125; 获取 Operator State 的入口是 OperatorStateStore 提供的方法，这些方法的命名能够见名之意，如果要使用 Union redistribution 重新分配模式的 Operator State，我们将使用 getUnionListState(StateDescriptor stateDescriptor) 访问状态数据。如果只是使用 Even-split redistribution 重新分配模式的 Operaotor State，调用 getListState(StateDescriptor stateDescriptor) 方法。 顺便说一句，Key State 也可以在 initializeState 方法中被初始化。 ListCheckpointed相对于 CheckpointedFunction 接口，ListCheckPointed 接口只支持 Even-split redistribution（偶切分重新分配模式）的 ListState。ListCheckpointed 接口包含如下两个接口: 12345678910/*** 返回一个状态数据的 List 用于生成 checkpoint，* 如果状态数据保证不会被重新分区，可以永远返回 `Collections.singletonList(MY_STATE)`*/List&lt;T&gt; snapshotState(long checkpointId, long timestamp) throws Exception;/*** 重历史 checkpoint 恢复历史状态数据*/void restoreState(List&lt;T&gt; state) throws Exception; CheckpointListener如果生成 checkpoint 的时候需要周知其他服务，可以使用 CheckpointListener。 待补充 广播状态(Broadcast State)关于广播状态，简而言之，就是一个输入数据对下游的所有处理流程都有影响，需要周知所有下游处理算子。例如一个低流速数据处理规则输入流是高流速的数据数据流，规则输入流需要广播给处理数据的所有算子。 广播状态与其他 Operator State 之间有三个主要区别。与其余的 operator state 相反，广播状态： Map 的格式 每个算子只能有一条广播的输入流和一个非广播输入流 算子可以有多个不同名字的广播状态 广播状态相关 API对于广播状态的使用待补充，这里重点突出如下注意事项。 重要注意事项在使用广播状态时要记住以下4个重要事项： 使用广播状态，Operator Task 之间不会相互通信这也是为什么只有状态广播方 (Keyed)-BroadcastProcessFunction 能修改广播状态数据的内容；此外，用户需要保证所有并发实例上对于广播状态的输入源的处理逻辑是幂等的，否则不同的并发实例将拥有不一致的广播状态，导致处理结果等数据不一致。 不同的 Operator Task 中的广播状态的顺序可能不一致虽然 Flink 保证广告状态都会下发给所有算子，不会丢失，但是并不保证广播状态的顺序一致性。因此对于广播状态不能依赖于输入数据的顺序。 所有并行实例都会快照一份广播状态数据虽然所有并行实例中的广播状态都是一致的（正常使用的情况下），但是每个并行实例都会快找一份自己的广播数据，而不是只快照一份。这种设计是为了避免多个并行示例在恢复期间从单个文件读取数据而造成热点问题。但是也导致了随着并行度的增大，checkpoint 数据大小也会膨胀。Flink 保证数据恢复/扩容的时候不会产生重复的数据，也不会丢失数据。在以相同或者更小的并行度恢复时，每个 task 读取对应的 checkpoint，在以更大的并行度恢复时，每个 task 读取自己的 checkpoint，剩余新增的 task 会循环读取 checkpoint。 RocksDB state backend 不支持广播状态广播状态目前在运行时保存在内存中。因为当前，RocksDB state backends还不支持广播状态。 这里谨期望广播状态能够进一步优化。 参考文档https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/state/https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/state/state.htmlhttps://stackoverflow.com/questions/45738021/flink-state-backend-keys-atomicy-and-distribution 本人 flink 小白一枚，如有错漏之处，敬请斧正！]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
        <tag>state</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 探究之路 ———— 容错机制，Checkpoint 和 Savepoint]]></title>
    <url>%2F2019%2F01%2F26%2F%E6%8A%80%E6%9C%AF%2Fflink%2Fflink%E4%B9%8Bsavepoints%E5%92%8Ccheckpoints%2F</url>
    <content type="text"><![CDATA[Flink 数据流容错机制译文Flink 最吸引使用者的地方就是它提供的容错机制保证可以持续性的恢复数据流应用程序的状态。Flink 保证即使在失败的情况下，数据流中的每一条数据最终也能确保只会对状态数据响应一次（exactly once）。响应一次 的机制可以手动降级到 至少响应一次(at least once)。 容错机制 对分布式流式数据持续性的产生快照(snapshot)并存储。对于持有小型数据状态的数据流应用来说，产生 快照 的过程是很轻量级的，对于数据流的正常处理过程的影响微乎其微。数据流应用的 状态 数据可以存储到一个可配置的环境(Master节点中，或者 HDFS 中）。 当程序失败（机器、网络或者软件故障）的时候，Flink 将停止分布式数据流应用。然后再从最后一次成功的 checkpoint 中保存的 状态(state) 数据中恢复应用的所有 算子（Operator）。输入数据也被重置到最后一次成功的快照数据中保存的位置 。 Flink 保证并行数据流在重启之后处理的所有数据都不会是最近一次成功的 checkpoint 之前的数据。 注意： 1. `checkpointing` 功能默认是关闭的，需要手动配置，指定开启 `checkpointing`，具体操作说明详见：[Checkpointing 说明文档](https://ci.apache.org/projects/flink/flink-docs-master/dev/stream/state/checkpointing.html) 2. 在 `Flink` 完成保证的基础上，数据流输入源 (`streaming source`)需要保障能回退到指定的最近一个位置。在 `Apache Kafka ` 提供这个能力的基础上，Flink 适配 Kafka 的 connector 利用这个能力实现容错机制。Flink 连接器(Connectors)对容错机制的支持详见：[数据输入源和输出流的容错机制](https://ci.apache.org/projects/flink/flink-docs-master/dev/connectors/guarantees.html)CheckpointingFlink 的 容错机制 简而言之就是持续不断的对 分布式数据流 和 算子状态(Operator state) 产生 一致性 的 快照 数据。这些 快照 数据系统遇到故障时，用于从错误状态中恢复的 检查点 (checkpoints)。 Flink 产生 快照 数据的机制的详细描述如下： Lightweight Asynchronous Snapshots for Distributed Dataflows，该算法是在参考 Chandy-Lamport algorithm 算法的基础上进行改进的，并针对 Flink 执行模型 进行量身定做。 Barriers (栅栏)Flink 的分布式快照的核心组成部分就是 Barriers(栅栏)，这些 Barriers(栅栏) 被插入到数据流中，和数据一起往下流。Barriers(栅栏) 不会影响数据流中数据的顺序，数据流保证严格有序。Barriers(栅栏) 将数据切分成两部分，前一部分的数据进入当前的快照数据(snapshot)中，后一部分的数据进入下一快照数据。每个 Barriers(栅栏) 都有一个 ID，这个 ID 就是 Barriers(栅栏) 前一个 snapshot 的 ID。Barriers(栅栏) 不会影响数据流的处理，所以非常轻量级。多个不同 快照 的多个 Barriers(栅栏) 可以在数据流中同时存在，即多个 快照 可以同时创建。 问题：数据流中的数据也会进入 快照 ？？？不应该是只包含状态数据吗？ Barriers(栅栏) 被插入到 数据源的并行数据流中。为快照 n 产生的 Barriers 注入的位置 Sn 就是在源数据中包含这些快照数据的位置。例如，在 Apache Kafka 中这个位置就是在分区(partition) 中最后一条已消费数据的偏移位置。 这个位置 Sn 将被上报给检查点协调器(checkpoint coordinator)，也就是 Flink 的 Job Manager。 然后 Barriers(栅栏) 流向下游数据流，当中间的算子(Operator) 从所有的上游输入流都接收到了 快照 n 的 栅栏 之后，向所有下游算子下发 快照 n 的 栅栏。当 输出算子(sink operator) （flink 有向无环图[DAG] 的尾节点）从它的所有上游输入流都接收到了 快照 n 的 栅栏 之后会检查点协调器发起 ACK 确认已接收到 快照 n。当所有的 输出算子(sink operator) 都发出了ACK 确认之后，快照 n 的数据被认为已经被处理完成了。 当 快照 n 已经被确认处理完成了，当前任务不会再向输入流请求获取 快照 n 之前的数据，因此这些数据将已经完成通过真个拓扑数据流。 接收多个输入流的算子需要在快照的栅栏上对齐输入流，上图描述了如下特性： 当算子接收到其中一个上游输入流的 快照 n 的栅栏的时候，算子不会处理这个栅栏之后的任何数据，直到它从剩下的所有输入流都接收到 快照 n 的栅栏。否则 快照 n 和 快照 n+1 的数据将被混合在一起。 数据流向检查点协调器报告栅栏的时候会被缓存并搁置，这个数据流的数据不会被处理，而是放置到输入缓存中。 一旦从最后一个流收到了barrier n，这个算子会发送所有积压的记录（个人注：将barrier之前的数据都发送出去），然后发送快照n的barrier。 然后，它继续处理从所有输入流中的数据，先处理输入缓存中的数据，然后处理流中的数据。 状态 State如果一个算子包含状态，那这个状态数据一定是 快照 的一部分，算子状态有不停的形式： 自定义状态：通过转化函数（如 map() 或 filter()）来创建和修改状态数据，详见 State in Streaming Applications 系统状态：这种状态时指的是算子计算过程中的一部分缓存数据。典型的例子就是 窗口缓存，系统收集窗口对应的数据到缓存，直到窗口计算或者发射。 算子在接收到所有上游输入流的栅栏之后，在向所有输出流发射栅栏之前对状态数据进行快照。此时栅栏之前的数据对状态的更改已经生效，并且栅栏之后的数据对状态的修改不会发生。由于快照的状态的数据可能会比较大，它可以存储到一个可配置的状态后端存储系统中。默认状态下，状态数据存储在 JobManager 的内存中，但是在生产环境还是需要配置成一个 可靠 的分布式存储系统（例如 HDFS）. 状态被存储之后，算子会确认其检查点完成，将 快照 的 栅栏 的数据发送给下游。 现在我们可以看一下 快照 中包含的数据。 对于并行的输入数据源，快照建立时数据流的偏移位置。 对于算子，快照包含了一个指向装填实际存储位置的指针。 Exactly Once vs. At Least Once 只响应一次还是还是至少响应一次对齐操作可能会增大数据流应用的延时，一般来说，对齐产生的额外延时只有几毫秒的数量级，但是我们也发现过延迟显著增加的异常情况。对于要求延时非常低（几毫秒）的数据流应用，flink 提供在产生检查点的时候关闭对齐的开关。如果关闭对齐步骤，算子会在接收到一个上游的栅栏的时候就会产生一个快照，而不是等到其他上游的栅栏都到齐了再来生成快照。 当对齐被关闭的时候，算子在收到栅栏的时候也会持续的处理输入数据。也就是说：算子在会在产生 检查点 n 的时候，会处理属于 检查点 n+1 的数据。所以当故障恢复的时候，这部分数据会被重复处理，因为这些数据都属于 检查点 n 的快照数据，同时在 检查点 n 之后也会被回放而被再次处理。 注意： 对齐操作只会发生在多输入运算（join）或者多输出的算子（例如重分区，分流）的场景下。 因此，对于普通的并行数据流操作（`map()`, `flatMap()`, fliter() 等）， 及时在 `至少响应一次（at least once）` 的模式下，也会保证 `只响应一次（exactly once）`Asynchronous State Snapshots 异步状态快照上面所述的机制表明算子在存储快照数据到后端存储系统的时候会停止处理输入数据，这种同步产生状态快照的模式每次产生的快照的时候都会引入额外的延时。 我们完全可以让算子在快照数据的同时继续处理输入数据，让快照的存储在后台异步进行。为了做到异步状态快照，算子必须能保证产生一个状态数据对象被存储之后，后续对状态的修改不会影响这个状态数据对象。例如 RocksDB 中使用的 写时复制（ copy-on-write ） 类型的数据结构。 接收到输入数据的栅栏的时候，算子开始异步的快照复制出它的状态。算子立即向输出发射栅栏，并继续处理输入数据。当后台异步快照完成时，算子会向 检查点协调器（checkpoints coordinator, 也就是 Job Manager）确认检查点完成，现在检查点完成的充分条件是：所有的 输出算子（sink） 都接收到了栅栏，而且所有有状态的算子确认完成了状态数据的备份（这个确认操作可能会晚于栅栏到达 输出算子（sink））。 详细的状态快照见： State Backends 故障恢复(Recovery)这种机制下的故障恢复就很简单：当发生故障的时候，Flink 选择最新完成的检查点 k。然后系统重新部署整个分布式数据流，给所有的算子提供快照在检查点中的状态数据用于恢复。输入流的读取位置被设置到从 Sk 开始读取，对于 Apache Kafka 来说就是通知 consumer 从偏移位置 Sk 开始消费消息。 算子快照的实现算子产生快照的过程分为两个部分：同步部分 和 异步部分。 算子和状态后端存储系统(State Backends) 提供 Java FutureTask 用于快照。这个任务包含同步部分已经完成，异步部分还在等待的状态，检查点的异步部分在后台线程中被执行。 完成同步的算子仅仅返回一个已经完成的 FutureTask。如果需要异步执行，FutureTask 中的 run() 方法将被会调用。 这些 FutureTask 是可以取消的，这样就可以释放流和其他资源的消耗。 比较一下 Checkpoint 和 SavepointCheckpoint 和 Savepoint 都是 flink 提供的容错恢复机制，两个不管是命名还是使用方式都很类似，这里分别对两个进行一个简单的介绍并且对二者进行对比。 保存点(Savepoint)保存点是通过 Flink 的 检查点机制(checkpointing mechanism) 创建的，包含数据流任务的运行状态的一个一致性的快照数据。你可以用保存点去停止并重启任务、复制任务或者更新任务。 保存点由两个部分组成 存储在一个稳定的存储介质（HDFS、S3等）上的，一般来说比较大的包含二进制文件的文件夹。这些二进制文件纯粹的保存任务的运行状态的快照数据。 一个元数据文件（一般来说比较小），文件中保存了指向存储在稳定存储介质上的保存点的所有文件的指针（文件路径）。 算子命名的重要性保存点中维护了一个 map 格式的数据 Operator ID | State ------------+------------------------ source-id | State of StatefulSource mapper-id | State of StatefulMapper通过算子的ID 能够获取到起对应的状态数据，在新增、删除算子或者修改算子的顺序的时候如果没有自定义命名，而是使用 flink 的默认命名方式，算子的数量和顺序的改变会影响重新启动的算子的ID，可能会导致算子ID 的状态数据匹配错误。强烈建议给每个算子设置一个 ID。 从保存点重启任务的时候算子修改会有什么结果?新增一个有状态的节点When you add a new operator to your job it will be initialized without any state. Savepoints contain the state of each stateful operator. Stateless operators are simply not part of the savepoint. The new operator behaves similar to a stateless operator. 新增的有状态的算子在重启的时候变现的和无状态的算子一样，因为保存点中没有这个算子对应的装填数据。 删除一个有状态的节点默认情况下删除一个有状态的节点会导致重启失败，因为这个过程默认需要确保所有状态数据都要对应一个算子。如果你真的需要删除一个状态的节点，你需要在启动参数中加上参数 --allowNonRestoredState (short: -n) 1$ bin/flink run -s :savepointPath -n [:runArgs] 重新排序有状态的节点 &amp;&amp; 删除或者重新排序无状态的节点算子指定ID 的情况下没问题，否则可能会导致状态数据匹配错误而重启失败 修改算子的并行度flink 1.2 以上版本并且没有使用任何 State 相关的 过时API 没有问题。如果保存点是在 flink 1.2 以下版本或者使用了 State 相关的 过时API 的代码中生成的，你需要升级 flink 版本并且替换 State 相关的 过时API，详见 upgrading jobs and Flink versions guide 保存点(Savepoint) 和 检查点(Checkpoint) 的区别从在概念上说，保存点(Savepoint) 和 检查点(Checkpoint) 的不同比较像备份不同于传统数据库从日志恢复。 下面是他们的不同点： 检查点 保存点 目的 为出现异常情况 Flink 任务提供一个恢复机制确保任务能从部分故障中恢复 用户手动维护任务的时候触发应用重启等操作 设计要求 作为一个用于恢复的，被周期调用的方法，它的实现要求: 1. 创建过程很轻量级 2. 可以快速的从检查点恢复故障 保存点的设计要求更着重于备份数据的便携性，而不是很关注创建过程的轻量级和快速恢复。 生命周期 flink 直接管理检查点的生命周期，从创建到释放都不需要维护人用手动触发 维护人员手动触发保存点的创建和删除，所有者是维护人员。 删除 用户停止任务之后被删除（除非编码或配置声明保留） 维护人员手动删除 具体实现 虽然目前保存点和检查点的代码实现和产生的文件格式都是一样的，但是使用 RocksDB 的检查点使用的保存格式不是flink 定义的，而是 RocksDB 自定义的格式，而且 RocksDB 的检查点支持增量检查点不支持 rescaling 不支持增量，支持 rescaling * rescaling 的翻译存疑，猜测是改变并行度* 译者注本人对文档的翻译过程中，对Flink 相关名词翻译如下： 英文名词 译者翻译 Connector 连接器 Data Sources 数据输入源 Data Sinks 数据输出流 rescaling - 参考文档https://ci.apache.org/projects/flink/flink-docs-release-1.7/internals/stream_checkpointing.htmlhttps://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/state/checkpoints.htmlhttps://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/state/savepoints.html 本人 flink 小白萌新一枚，如有错漏之处，敬请斧正！]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 探究之路 ———— Flink Job 重启策略]]></title>
    <url>%2F2019%2F01%2F26%2F%E6%8A%80%E6%9C%AF%2Fflink%2Fflink%E4%B9%8B%E9%87%8D%E5%90%AF%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[Flink Job 重启策略flink 提供多种重启策略，可以在 flink-conf.yaml 中通过配置 restart-strategy 参数设置默认使用的重启策略，也可以在 job 中指定重启策略。 flink 提供如下重启策略 固定延时重启(Fixed delay) 故障率重启(Failure rate) 不重启(No restart) 当 job 没有开启 checkpoint 的时候，一定是使用 不重启 策略，如果 job 开启了 checkpoint 但是没有设置重启策略的时候，将使用 固定延时重启策略 固定延时重启(Fixed delay)job 发生故障后，尝试重启 n 次，每次重启间隔固定时间 t，n 次之后失败。 在 flink-conf.yaml 中设置重启策略 123restart-strategy: fixed-delayrestart-strategy.fixed-delay.attempts: 3 // 故障发生后尝试重启次数restart-strategy.fixed-delay.delay: 10 s // 重启的间隔时间 job 中直接指定重启策略 12345ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // number of restart attempts Time.of(10, TimeUnit.SECONDS) // delay)); 故障率重启(Failure rate)故障率重启策略在 job 发生故障后尝试重启，但是当在固定时间failure rate interval 内故障次数超过 failure rate 次后，job 被认定为故障。 举例说明：job 故障停止运行之后，在 5min 内重试 3次，每次间隔10s，如果3次之后依旧失败，则认定为故障 在 flink-conf.yaml 中设置重启策略 1234restart-strategy: failure-raterestart-strategy.failure-rate.max-failures-per-interval: 3 // 故障后尝试重启次数restart-strategy.failure-rate.failure-rate-interval: 5 min // 故障后检查故障率的时间间隔restart-strategy.failure-rate.delay: 10 s // 故障后两次尝试重启的间隔时间 job 中直接指定重启策略 123456ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.failureRateRestart( 3, // max failures per interval Time.of(5, TimeUnit.MINUTES), //time interval for measuring failure rate Time.of(10, TimeUnit.SECONDS) // delay)); 不重启策略(No Restart) 在 flink-conf.yaml 中设置重启策略 1restart-strategy: none job 中直接指定重启策略 12ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.noRestart()); 参考文档https://ci.apache.org/projects/flink/flink-docs-stable/dev/restart_strategies.html 本人 flink 小白一枚，如有错漏之处，敬请斧正！]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mogilefs 基础介绍]]></title>
    <url>%2F2019%2F01%2F10%2F%E6%8A%80%E6%9C%AF%2Fmogifs%2Fmogilefs-base-desc%2F</url>
    <content type="text"><![CDATA[mogilefsmogilefs 介绍MogileFS是一个开源的分布式文件存储系统，由LiveJournal旗下的DangaInteractive公司开发。Danga团队开发了包括 Memcached、MogileFS、Perlbal 等多个知名的开源项目。目前使用MogileFS 的公司非常多，如日本排名先前的几个互联公司及国内的yupoo(又拍)、digg、豆瓣、1号店、大众点评、搜狗和安居客等，分别为所在的组织或公司管理着海量的图片。 mogilefs 基本概念mogilefs 组成架构tracker节点: tracker节点 是应用程序 mogilefsd，通过数据库来保存元数据，提供API 响应客户端 storage节点: storage节点 是应用程序 mogstored，本质上是一个 WebDAV 服务，默认监听 7500 端口，接受客户端的文件存储请求 database: 用于存储元数据的 mysql 数据库 mogilefs 数据存储相关概念domain: mogilefs 划分数据存储的容器，每一个 domain 中的数据的 key 不能重复，但是不同 domain 中数据的 key 可以重复；domain 的划分可以按照数据类型划分，也可以按照业务类型划分。host: mogilefs 的每一个存储节点被称为一个 host，每个 host 有自己的 IDdevice: 每个存储节点可以挂靠多个 device， 每个 device 对应一个时间存储数据的存储设备（硬盘），每个 device 有自己的IDclass: 对于存储在mogilefs 上的文件进行分类，同一 domain 下的文件可以分类不同类型，但是key 不同重复，上传文件时，可指定 class， 默认为 default mogilefs 基本操作mogadmmogadm check检查 mogilefs 集群状态 $mogadm check Checking trackers... 127.0.0.1:7001 ... OK Checking hosts... [ 1] storage1 ... OK Checking devices... host device size(G) used(G) free(G) use% ob state I/O% ---- ------------ ---------- ---------- ---------- ------ ---------- ----- [ 1] dev1 374.702 29.018 345.684 7.74% writeable 0.0 ---- ------------ ---------- ---------- ---------- ------ total: 374.702 29.018 345.684 7.74%mogadm domainmogilefs 集群 domain 管理 123mogadm domain add domain1 # 新增 domainmogadm domain delete domain1 # 删除 domainmogadm domain list # 列出所有 domain mogadm hostmogilefs 集群 host 管理 1234# 新增 hostmogadm --trackers=127.0.0.1:7001 host add host1 --ip=[$ip] --port=7500 --status=alive mogadm host delete host1 # 删除 host, 不指定tracker 的时候默认使用本机 `127.0.0.1:7001`mogadm host list # 列出所有 host mogadm devicemogilefs 集群 device 管理 device 列出1mogadm device list # 列出所有 host 新增 device下面的例子中新增 device 进入 /etc/mogilefs/mogstored.conf 文件中配置的 docroot，例如 cd /home1/mogdata 如果待挂载的 device（ID 命名为 dev2） 所在的硬盘和 /home1 所在硬盘不一致(例如 /home2)，需要在 /home2 中新建目录 /home2/mogdata/dev2，并在 /home1/mogdata 中创建软链 1234mkdir -pv /home2/mogdata/dev2;cd /home1/mogdata;ln -s /home2/mogdata/dev2 dev2;mogadm --trackers=127.0.0.1:7001 device add host1 dev2 device 状态修改12345# dev2 磁盘故障mogadm --trackers=127.0.0.1:7001 device mark host1 dev2 dead # 将 dev2 置为死亡，不可用# dev2 磁盘故障恢复，无法直接从 dead 更换到 alive，需先过渡到 down 状态mogadm --trackers=127.0.0.1:7001 device mark host1 dev2 down # 将 dev2 置为 down 状态mogadm --trackers=127.0.0.1:7001 device mark host1 dev2 alive # 将 dev2 置为 alive 状态 device 列表状态查询1mogadm --trackers=127.0.0.1:7001 device summary 文件上传1mogupload --trackers="127.0.0.1:7001" --domain=[$&#123;domain&#125;] --key=[$&#123;key&#125;] --file=[$&#123;file&#125;] --class=[$&#123;class&#125;] --domain: 指定上传文件的 domain--key: 指定上传文件的 key--file: 指定上传文件的路径--class: 指定上传文件的 class，可选项，默认值为 default 文件下载1mogfetch --trackers=host --domain=[$&#123;domain&#125;] --key=[$&#123;key&#125;] --file=[$&#123;file&#125;] --domain: 指定上传文件的 domain--key: 指定上传文件的 key--file: 指定下载文件的路径及名称 文件查看1mogfileinfo --trackers="127.0.0.1:7001" --domain=[$&#123;domain&#125;] --key=[$&#123;key&#125;] --domain: 指定上传文件的 domain--key: 指定上传文件的 key 输出结果： - file: [$file] class: default devcount: 1 domain: [$domain] fid: 30 key: [$key] length: 608 - http://127.0.0.1:7500/dev1/0/000/000/0000000030.fid]]></content>
      <categories>
        <category>mogilefs</category>
      </categories>
      <tags>
        <tag>mogilefs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink DateStream API]]></title>
    <url>%2F2018%2F11%2F19%2F%E6%8A%80%E6%9C%AF%2Fflink%2FFlink-DateStream-API-Desc%2F</url>
    <content type="text"><![CDATA[checkpointChandy Lamport algorithm barriercheckpoint configurationscheckpoint mode exactly-once at-least-once 是否可以给单个算子指定 exactly-once 或者 at-least-once checkpoint intervalcheckpoint timeoutminimum time between checkpointsnumber of concurrent checkpointsexternalized checkpoints ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION: 当 flink job 被取消的时候保存 checkpoint，也就是说当我们主动取消 job 的时候需要我手动删除 ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION: 当 flink job 被取消的时候删除 checkpoint， 只有 job 状态是故障失败时 checkpoint 才会被保存。 retain state checkpoint1state.checkpoints.dir: hdfs:///checkpoints/ statekeyed stateKeyed Stream 的状态 ListStateList 类型的状态集合 ValueState单个状态 MapStateMap 类型的状态集合 ReducingState合并统一类型的多个状态数据到一个状态数据 AggregatingStateoperator stateorg.apache.flink.api.common.state.OperatorStateStore 用于注册 operator state getUnionListState: 获取分布式集群中的 ListState getListState: 获取单点的 ListState state backend asynchronus synchronus memory state backends缺点 每个 state 大小限制是 5M state 的大小不能超过 akka frame size aggregate state 必须存放在 Job Manager 的内存中 fs state backends该模式需要配置文件系统URL，支持 hdfs(hdfs://namenode:40010/flink/checkpoints) 或 本地文件(file:///data/flink/checkpoints)将未完成的数据存储在 TaskManager 的内存中，而将 state snapshot 写入到文件系统或者文件夹中，最小化元数据被保存在JobManager的内存中（在高可用模式下，被保存在元数据checkpoint中）。该模式默认 异步 写入 state backend，也可以改为 同步 写入 1new FsStateBackend(path, false); // true: 异步，false：同步 rocksdb state backends该模式需要配置文件系统URL，支持 hdfs(hdfs://namenode:40010/flink/checkpoints) 或 本地文件(file:///data/flink/checkpoints)将未完成的数据存储在 rocksDB 中，而将 state snapshot 写入配置的文件系统或者文件夹中，最小化元数据被保存在JobManager的内存中（在高可用模式下，被保存在元数据checkpoint中）。 RocksDBStateBackend 只支持异步快照模式 缺点： 因为RocksDB的JNI的API基于byte[]，状态中每个key和每个value所支持的最大值各为2^31字节。 注意：state使用了RocksDB的合并算子（如ListState），状态的大小很容易累积超过2^31字节，下一次状态恢复就会失败。这是当前RocksDB JNI的局限性。 RocksDBStateBackend是当前唯一一种提供增量checkpoint的state backend. restart strategies（重启策略）Event TimeProcessing timeflink 开始处理事件的时间 Event time时间发生的原始时间，由事件发生器自主设置 Ingestion timeFlink Source 接收到事件的时间 operatoroperator lifecycle12345678910111213141516171819// initialization phaseOPERATOR::setup UDF::setRuntimeContextOPERATOR::initializeStateOPERATOR::open UDF::open// processing phase (called on every element/watermark)OPERATOR::processElement UDF::runOPERATOR::processWatermark// checkpointing phase (called asynchronously on every checkpoint)OPERATOR::snapshotState// termination phaseOPERATOR::close UDF::closeOPERATOR::dispose 注意：initializeState()包含operator state的初始化（例如register keyed state），也包含任务失败后从checkpoint中恢复state的逻辑。]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 基本概念入门]]></title>
    <url>%2F2018%2F11%2F19%2F%E6%8A%80%E6%9C%AF%2Fflink%2Fflink%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Flink 基本概念基本名词Job以 jar 包的形式在 flink 中提交的可运行程序 taskstreamflink 作为一个流数据处理的引擎，就是针对一个或多个 stream 进行流计算处理，再输出到一个或多个 stream 中去，这里的 stream 可以使 mq，也可以是文件、也可以直接是控制台输入\输出。 operator &amp;&amp; taskflink 流处理流程中的每个操作(如 map, keyBy, sink, source等)都是 operator operator subtask每个 operator 可以分成多个 operator subtask，一个 operator 的并行度就是 operator subtask 的数量 operator chainflink 作为分布式运行系统，会将多个 operator subtask 关联成一个 operator task，这个过程就是 operator chain。 两个 operator subtask 能否关联起来，需要满足下列要求 上下游的并行度一致 下游节点的入度为1 （也就是说下游节点没有来自其他节点的输入） 上下游节点都在同一个 slot group 中（下面会解释 slot group） 下游节点的 chain 策略为 ALWAYS（可以与上下游链接，map、flatmap、filter等默认是ALWAYS） 上游节点的 chain 策略为 ALWAYS 或 HEAD（只能与下游链接，不能与上游链接，Source默认是HEAD） 两个节点间数据分区方式是 forward（参考理解数据流的分区）用户没有禁用 chain 分布式运行时环境JobManagerflink 集群服务的 master 节点，用来协调分布式计算，负责进行任务调度，协调 checkpoints，协调错误恢复等等。一个集群中至少有一个 JobManager，如果有多个 JobManager，其中一个作为 leader，其余处于备用的状态。 TaskManagerflink 集群的 worker 节点，真正执行 dataflow 中的 tasks，并且对 streams 进行缓存和交换，集群中至少需要一个 TaskManager，每个 TaskManager 是一个 JVM 进程。 Clients连接 flink 集群 的客户端，向 flink 集群 提交计算任务 Task Slots每个 TaskManager 都是一个 JVM 进程，可以在不同的线程运行一个或多个线程，每个 TaskManager 通过 Task Slots 来控制可以接收多少个tasks。每个 Task Slots 代表 TaskManager 中一个固定的资源子集，如果 1 个 TaskManager 有 3 个 Task Slots，它会将他的内存资源划分为 3 份分配给每个 slot。通过调整 Task Slots 的数量而调整 subtasks 之间的隔离方式。当每个 TaskManager 只有一个 Task Slot 的时候，意味着每个 task group 运行在独立的的JVM 中。当一个 TaskManager 有多个 slot 的时候，意味着多个 在同一 JVM 进程中的 task 将共享 TCP 链接和心跳信息，他们也能共享。 DateStream API source: 数据来源 sink: 处理结果输出 window: 窗口]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 安装和运行]]></title>
    <url>%2F2018%2F11%2F19%2F%E6%8A%80%E6%9C%AF%2Fflink%2Fflink%E5%AE%89%E8%A3%85%E5%92%8C%E8%BF%90%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[Flink 安装和运行单点安装下载 flink 安装包在 官网下载地址 选择适当版本的flink 安装包，这里我选择 Apache Flink 1.6.2 only；下载安装包到 /path/to/flink-1.6.2 中 解压运行 flink1234cd /path/to/tar -xvf flink-1.6.2.tgzcd flink-1.6.2sh ./bin/start-cluster.sh 到这里我们就已经把 单机版flink 成功的运行起来了，我们可以访问 localhost:8081 看到 flink 管理页面 flink 集群搭建准备工作JAVA 环境配置这里不赘述如何配置 JAVA 环境，只需要注意使用 JAVA 1.8+ 即可 集群机器互信不管是 standalone 部署模式还是依赖 hadoop yarn 搭建集群，都需要在集群机器之间设置互信，实现 ssh 相互免密登录 生成公钥/秘钥 1ssh-keygen -t rsa #一直回车即可 生成了 ~/.ssh/id_rsa.pub 和 ~/.ssh/id_rsa 公钥认证 将机器A 上面的 ~/.ssh/id_rsa.pub 追加到机器B 的公钥认证文件 ~/.ssh/authorized_keys 里面去；再将机器B 上面的 ~/.ssh/id_rsa.pub 追加到机器A 的公钥认证文件 ~/.ssh/authorized_keys 里面去 这样我们就可以在机器A、B之间互相免密码登陆了 flink on yarn 集群搭建在 10.0.0.1，10.0.0.2，10.0.0.3 三台机器上尝试搭建 flink 集群 hadoop yarn 安装配置下载解压 hadoop 安装包这里我用的 cdh 版本的 hadoop，可以在这里下载，然后解压 1tar -xvf hadoop-2.6.0-cdh5.11.0.tar.gz 配置 hadoop在 /path/to/hadoop-2.6.0-cdh5.11.0/etc/hadoop/ 文件夹下配置如下七个文件 hadoop-env.sh，yarn-env.sh，slaves，core-site.xml，hdfs-site.xml，mapred-site.xml，yarn-site.xml 在 hadoop-env.sh1export JAVA_HOME=/path/to/java/home 在 yarn-env.sh 中配置 JAVA_HOME1export JAVA_HOME=/path/to/java/home 在 slaves 中配置 slave 节点的ip 或者host1210.0.0.210.0.0.3 修改 core-site.xml配置 hadoop 集群文件系统主机和端口、hadoop 临时目录 123456789101112&lt;configuration&gt; &lt;property&gt; &lt;!-- hadoop文件系统主机和端口 --&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://10.0.0.1:9000/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 配置 hadoop 临时目录 --&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/path/to/hadoop-2.6.0-cdh5.11.0/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 hdfs-site.xml配置 hadoop 集群文件系统主机和端口、hadoop 临时目录 123456789101112&lt;configuration&gt; &lt;property&gt; &lt;!-- hadoop文件系统主机和端口 --&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://10.0.0.1:9000/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 配置 hadoop 临时目录 --&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/path/to/hadoop-2.6.0-cdh5.11.0/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; standalone 部署模式集群搭建举例说明，搭建 下载、解压安装包修改 flink-conf.yaml 文件配置修改 /path/to/flink-1.6.2/conf 文件中的 flink-conf.yaml 文件，参数说明如下： 1234567891011121314151617181920212223242526272829303132333435363738# java安装路径，如果没有指定则默认使用系统的$JAVA_HOME环境变量。建议设置此值，因为之前我曾经在standalone模式中启动flink集群，报找不到JAVA_HOME的错误。config.sh中（Please specify JAVA_HOME. Either in Flink config ./conf/flink-conf.yaml or as system-wide JAVA_HOME.）env.java.home: /path/to/java/home# 定制JVM选项，在Flink启动脚本中执行。需要单独执行JobManager和TaskManager的选项。env.java.opts: -DXms=1024MB# 执行jobManager的JVM选项。在Yarn Client环境下此参数无效。env.java.opts.jobmanager:# 执行taskManager的JVM选项。在Yarn Client环境下此参数无效。env.java.opts.taskmanager:# Jobmanager的IP地址，即master地址。默认是localhost，此参数在HA环境下或者Yarn下无效，仅在local和无HA的standalone集群中有效。jobmanager.rpc.address: localhost# JobMamanger的端口，默认是6123。jobmanager.rpc.port: 6123# JobManager的堆大小（单位是MB）。当长时间运行operator非常多的程序时，需要增加此值。具体设置多少只能通过测试不断调整。jobmanager.heap.mb: 1024# 每一个TaskManager的堆大小（单位是MB），由于每个taskmanager要运行operator的各种函数（Map、Reduce、CoGroup等，包含sorting、hashing、caching），因此这个值应该尽可能的大。如果集群仅仅跑Flink的程序，建议此值等于机器的内存大小减去1、2G，剩余的1、2GB用于操作系统。如果是Yarn模式，这个值通过指定tm参数来分配给container，同样要减去操作系统可以容忍的大小（1、2GB）。taskmanager.heap.mb: 8196# 每个TaskManager的并行度。一个slot对应一个core，默认值是1.一个并行度对应一个线程。总的内存大小要且分给不同的线程使用。taskmanager.numberOfTaskSlots: 5# 每个operator的默认并行度。默认是1.如果程序中对operator设置了setParallelism，或者提交程序时指定了-p参数，则会覆盖此参数。如果只有一个Job运行时，此值可以设置为taskManager的数量 * 每个taskManager的slots数量。即NumTaskManagers * NumSlotsPerTaskManager 。parallelism.default: 3# 设置默认的文件系统模式。默认值是file:///即本地文件系统根目录。如果指定了hdfs://localhost:9000/，则程序中指定的文件/user/USERNAME/in.txt，即指向了hdfs://localhost:9000/user/USERNAME/in.txt。这个值仅仅当没有其他schema被指定时生效。一般hadoop中core-site.xml中都会配置fs.default.name。fs.default-scheme: hdfs://localhost:9000/# HDFS的配置路径。例如：/home/flink/hadoop/hadoop-2.6.0/etc/hadoop。如果配置了这个值，用户程序中就可以简写hdfs路径如：hdfs:///path/to/files。而不用写成：hdfs://address:port/path/to/files这种格式。配置此参数后，Flink就可以找到此路径下的core-site.xml和hdfs-site.xml了。建议配置此参数。fs.hdfs.hadoopconf: /home/flink/hadoop/hadoop-2.6.0/etc/hadoop# flink 服务的 http 端口rest.port: 8081 flink on Yarn启动 yarn session 运行 flink job1bin/yarn-session.sh -n 4 -jm 1024 -tm 4096 -s 32 yarn-session.sh 使用说明123456789101112131415161718192021222324Usage: Required -n,--container &lt;arg&gt; Number of YARN container to allocate (=Number of Task Managers) Optional -D &lt;property=value&gt; use value for given property -d,--detached If present, runs the job in detached mode -h,--help Help for the Yarn session CLI. -id,--applicationId &lt;arg&gt; Attach to running YARN session -j,--jar &lt;arg&gt; Path to Flink jar file -jm,--jobManagerMemory &lt;arg&gt; Memory for JobManager Container with optional unit (default: MB) -m,--jobmanager &lt;arg&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -n,--container &lt;arg&gt; Number of YARN container to allocate (=Number of Task Managers) -nl,--nodeLabel &lt;arg&gt; Specify YARN node label for the YARN application -nm,--name &lt;arg&gt; Set a custom name for the application on YARN -q,--query Display available YARN resources (memory, cores) -qu,--queue &lt;arg&gt; Specify YARN queue. -s,--slots &lt;arg&gt; Number of slots per TaskManager -sae,--shutdownOnAttachedExit If the job is submitted in attached mode, perform a best-effort cluster shutdown when the CLI is terminated abruptly, e.g., in response to a user interrupt, such as typing Ctrl + C. -st,--streaming Start Flink in streaming mode -t,--ship &lt;arg&gt; Ship files in the specified directory (t for transfer) -tm,--taskManagerMemory &lt;arg&gt; Memory per TaskManager Container with optional unit (default: MB) -yd,--yarndetached If present, runs the job in detached mode (deprecated; use non-YARN specific option instead) -z,--zookeeperNamespace &lt;arg&gt; Namespace to create the Zookeeper sub-paths for high availability mode yarn 会话管理123456789101112131415161718192021222324$yarn application --helpusage: application -appStates &lt;States&gt; Works with -list to filter applications based on input comma-separated list of application states. The valid application state can be one of the following: ALL,NEW,NEW_SAVING,SUBMITTED,ACCEPTED,RUN NING,FINISHED,FAILED,KILLED -appTypes &lt;Types&gt; Works with -list to filter applications based on input comma-separated list of application types. -help Displays help for all commands. -kill &lt;Application ID&gt; Kills the application. -list List applications. Supports optional use of -appTypes to filter applications based on application type, and -appStates to filter applications based on application state. -movetoqueue &lt;Application ID&gt; Moves the application to a different queue. -queue &lt;Queue Name&gt; Works with the movetoqueue command to specify which queue to move an application to. -status &lt;Application ID&gt; Prints the status of the application. 直接提交 flink job 到 yarn 集群1bin/flink run -m yarn-cluster -yn 2 -yjm 1024 -ytm 1024 ./examples/batch/WordCount.jar yarn 日常维护查看 yarn session 中的 flink job1./bin/flink list -m yarn-cluster -yid &lt;Yarn Application Id&gt; -r yarn 日志查看1yarn logs -applicationId &lt;applicationId&gt; yarn application 下线1yarn application -kill &lt;applicationId&gt; 触发 savepoints 取消 flink job1./bin/flink cancel -s [savepointDirectory] &lt;jobID&gt; 执行上述指令将得到如下提示 12Cancelling job &lt;jobID&gt; with savepoint to &lt;savepointDirectory&gt;.Cancelled job &lt;jobID&gt;. Savepoint stored in &lt;savepointDirectory&gt;/&lt;savepointID&gt;. 从 savepoints 恢复启动 flink job1./bin/flink run -s &lt;savepointDirectory&gt;/&lt;savepointID&gt; -m yarn-cluster -yn 2 -yjm 1024 -ytm 1024 ./examples/batch/WordCount.jar 这里 -s 参数值是执行cancel 指令的时候得到的 savepoint 保存地址&lt;savepointDirectory&gt;/&lt;savepointID&gt; flink run 指令1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586$./bin/flink run --helpAction "run" compiles and runs a program. Syntax: run [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt; "run" action options: -c,--class &lt;classname&gt; 当 flink job 的 jar包中没有指定 mainfest 时，通过这个 参数来指定包含 main() 方法，或者 getPlan() 方法的主类。 -C,--classpath &lt;url&gt; 新增 flink 类加载器的 Adds a URL to each user code classloader on all nodes in the cluster. The paths must specify a protocol (e.g. file://) and be accessible on all nodes (e.g. by means of a NFS share). You can use this option multiple times for specifying more than one URL. The protocol must be supported by the &#123;@link java.net.URLClassLoader&#125;. -d,--detached 以后台模式运行 flink job -n,--allowNonRestoredState Allow to skip savepoint state that cannot be restored. You need to allow this if you removed an operator from your program that was part of the program when the savepoint was triggered. -p,--parallelism &lt;parallelism&gt; The parallelism with which to run the program. Optional flag to override the default value specified in the configuration. -q,--sysoutLogging If present, suppress logging output to standard out. -s,--fromSavepoint &lt;savepointPath&gt; Path to a savepoint to restore the job from (for example hdfs:///flink/savepoint-1537). -sae,--shutdownOnAttachedExit If the job is submitted in attached mode, perform a best-effort cluster shutdown when the CLI is terminated abruptly, e.g., in response to a user interrupt, such as typing Ctrl + C. Options for yarn-cluster mode: -d,--detached If present, runs the job in detached mode -m,--jobmanager &lt;arg&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -sae,--shutdownOnAttachedExit If the job is submitted in attached mode, perform a best-effort cluster shutdown when the CLI is terminated abruptly, e.g., in response to a user interrupt, such as typing Ctrl + C. -yD &lt;property=value&gt; use value for given property -yd,--yarndetached If present, runs the job in detached mode (deprecated; use non-YARN specific option instead) -yh,--yarnhelp Help for the Yarn session CLI. -yid,--yarnapplicationId &lt;arg&gt; Attach to running YARN session -yj,--yarnjar &lt;arg&gt; Path to Flink jar file -yjm,--yarnjobManagerMemory &lt;arg&gt; JobManager Container 的内存大小，默认单位是 MB -yn,--yarncontainer &lt;arg&gt; 指定分配的 yarn container 的数量，等同于 flink Task Managers 的数量 -ynl,--yarnnodeLabel &lt;arg&gt; Specify YARN node label for the YARN application -ynm,--yarnname &lt;arg&gt; Set a custom name for the application on YARN -yq,--yarnquery Display available YARN resources (memory, cores) -yqu,--yarnqueue &lt;arg&gt; Specify YARN queue. -ys,--yarnslots &lt;arg&gt; Number of slots per TaskManager -yst,--yarnstreaming Start Flink in streaming mode -yt,--yarnship &lt;arg&gt; Ship files in the specified directory (t for transfer) -ytm,--yarntaskManagerMemory &lt;arg&gt; 单个 taskManager 的内存大小，默认单位是 MB -yz,--yarnzookeeperNamespace &lt;arg&gt; Namespace to create the Zookeeper sub-paths for high availability mode -z,--zookeeperNamespace &lt;arg&gt; Namespace to create the Zookeeper sub-paths for high availability mode Options for default mode: -m,--jobmanager &lt;arg&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -z,--zookeeperNamespace &lt;arg&gt; Namespace to create the Zookeeper sub-paths for high availability mode]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yarn 集群日常问题汇总]]></title>
    <url>%2F2018%2F11%2F19%2F%E6%8A%80%E6%9C%AF%2Fflink%2Fyarn%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[yarn 日志如何确定 yarn 日志位置 当 yarn.log-aggregation-enable = true 时，yarn 集群中的 application 的日志将被聚合到 yarn.nodemanager.remote-app-log-dir 指向的目录中去，保留时间为 yarn.log-aggregation.retain-seconds yarn.nodemanager.remote-app-log-dir 指向的目录是 hdfs 目录 当 yarn.log-aggregation-enable = false 时，yarn 集群中的 application 的日志将被存储到 yarn.nodemanager.log-dirs 属性指向的在 nodemanager nodes 上的文件夹中，保留时间为 yarn.nodemanager.log.retain-seconds 为什么会有 application 没有日志1yarn application does not have any log files.]]></content>
      <categories>
        <category>yarn</category>
      </categories>
      <tags>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat 日志配置说明]]></title>
    <url>%2F2018%2F09%2F28%2F%E6%8A%80%E6%9C%AF%2Ftomcat%2Ftomcat%E7%9A%84%E6%97%A5%E5%BF%97%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[前言日志应该是除了代码之外，程序员最好的朋友了，它可以帮助我们定位问题、修复bug，或者是确认服务是否正常运转；很多时候我们做一次部署只是为了加几行日志； 而 Tomcat 作为经久畅销的web 服务器，一直是web 开发者首选，而 Tomcat 的原生日志是我们判断这个服务器是否正常运转的重要数据。 Java 日志组件这里我们按照历史顺序简单介绍一下 Java 常用的日志组件， JUL(Java Util Logging): 是 jdk 自带的log 实现组件，虽然是官方出品但是它并没有被广泛使用，主要是下面几个原因 JUL 出现的太晚了，2002年它才被放到 jdk1.4 中，当时已经有很多第三方的日志组件被广泛使用了 JUL 早期性能问题太明显，到 JDK1.5 才有所改善，但是它和其他第三方日志组件logback或log4j2相比也还是有差距 JUL 提供的功能不如第三方组件logback或log4j2完善 log4j 是在 logback 之前被广泛使用的日志实现组件，log4j 在设计上十分优秀，对后期的Java 日志框架有深远的影响，但是它在性能上存在缺陷；logback 出现之后就取代了 log4j JCL(Apache Commons Logging): apache 提出的 Log Facade，只提供日志api，不提供实现，通过不同的 Adapter 来使用 JUL 或者 log4j；在打印日志的时候调用的都是 JCL 指定的api ，具体实现是看当前的 classpath 中有什么实现，如果什么都没有 slf4j(The Simple Logging Facade For Java): slf4j 是 Ceki Gülcü 开发的 Log Facade，主要是因为Ceki Gülcü 觉得作为日志统一接口的 JCL 设计的不合理： 下面这种写法不管是否输出 debug 级别的时候都需要做一次字符串拼接，如果这种代码被反复调用就会产生很多无用的字符串拼接，影响性能。 1logger.debug("log:" + log); 而官方给出的最佳时间方式是这样的： 123if (logger.isDebugEnabled()) &#123; logger.debug("log:" + log);&#125; 怎么看都是反人类的设计，所以在 slf4j 中，设计的api 是这样的 1logger.debug("log:&#123;&#125;", log); logback: logback 也是Ceki Gülcü 开发的日志实现，在 log4j 的基础上进行了改进，提供了更好的性能实现，异步logger，Filter 等更能多的特性。 Ceki Gülcü 给我们开发了很好用的日志组件，但是现在有了两个 Log Facade 和三个流行的 Log Implementation，事情变的复杂了；Ceki Gülcü 作为一个完美主义者，为了我们能在不同的log 之间自由切换，他又开发了各种 Adapater 和 Bridge 来连接，这里盗用一张 slf4j 官网的图片 log4j2: log4j2 的开发维护人员不想看着 log4j 被 slf4j/logback 所取代，在设计上很大程度的模仿了 slf4j/logback，完全脱离log4j1.x，在性能上实现了很大的提升，作为一个高仿品这里不多介绍。 Tomcat 的日志实现方法Tomcat 内部整合的日志模块是 JULI，JULI是从 JCL fork 过来的一个重命名分支，默认被硬编码使用 JUL 作为日志实现，从而保证 Tomcat 本身的日志和业务日志实现完美隔离。而Tomcat的日志的配置文件默认位置是 ${catalina.base}/conf/logging.properties，如果无法读取或不存在的时候，就会去找${java.home}/lib/logging.properties；在web应用的范围内也有一个日志配置文件 WEB-INF/classes/logging.properties 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# Licensed to the Apache Software Foundation (ASF) under one or more# contributor license agreements. See the NOTICE file distributed with# this work for additional information regarding copyright ownership.# The ASF licenses this file to You under the Apache License, Version 2.0# (the &quot;License&quot;); you may not use this file except in compliance with# the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.## 全局申明，tomcat 可以使用的 Handlerhandlers = 1catalina.org.apache.juli.FileHandler, 2localhost.org.apache.juli.FileHandler, 3manager.org.apache.juli.FileHandler, 4host-manager.org.apache.juli.FileHandler, java.util.logging.ConsoleHandler## 在.handlers = 1catalina.org.apache.juli.FileHandler, java.util.logging.ConsoleHandler############################################################# Handler specific properties.# Describes specific configuration info for Handlers.############################################################## catalina.out catalina.yyyy-MM-dd.log 日志的级别、日志文件位置、日志文件名称前缀配置1catalina.org.apache.juli.FileHandler.level = FINE1catalina.org.apache.juli.FileHandler.directory = $&#123;catalina.base&#125;/logs1catalina.org.apache.juli.FileHandler.prefix = catalina.## localhost.yyyy-MM-dd.log 日志的级别、日志文件位置、日志文件名称前缀配置2localhost.org.apache.juli.FileHandler.level = FINE2localhost.org.apache.juli.FileHandler.directory = $&#123;catalina.base&#125;/logs2localhost.org.apache.juli.FileHandler.prefix = localhost.## manager.yyyy-MM-dd.log 日志的级别、日志文件位置、日志文件名称前缀配置3manager.org.apache.juli.FileHandler.level = FINE3manager.org.apache.juli.FileHandler.directory = $&#123;catalina.base&#125;/logs3manager.org.apache.juli.FileHandler.prefix = manager.## host-manager.yyyy-MM-dd.log 日志的级别、日志文件位置、日志文件名称前缀配置4host-manager.org.apache.juli.FileHandler.level = FINE4host-manager.org.apache.juli.FileHandler.directory = $&#123;catalina.base&#125;/logs4host-manager.org.apache.juli.FileHandler.prefix = host-manager.## console 日志级别及格式设置java.util.logging.ConsoleHandler.level = FINEjava.util.logging.ConsoleHandler.formatter = java.util.logging.SimpleFormatter############################################################# Facility specific properties.# Provides extra control for each logger.############################################################org.apache.catalina.core.ContainerBase.[Catalina].[localhost].level = INFOorg.apache.catalina.core.ContainerBase.[Catalina].[localhost].handlers = 2localhost.org.apache.juli.FileHandlerorg.apache.catalina.core.ContainerBase.[Catalina].[localhost].[/manager].level = INFOorg.apache.catalina.core.ContainerBase.[Catalina].[localhost].[/manager].handlers = 3manager.org.apache.juli.FileHandlerorg.apache.catalina.core.ContainerBase.[Catalina].[localhost].[/host-manager].level = INFOorg.apache.catalina.core.ContainerBase.[Catalina].[localhost].[/host-manager].handlers = 4host-manager.org.apache.juli.FileHandler# For example, set the org.apache.catalina.util.LifecycleBase logger to log# each component that extends LifecycleBase changing state:#org.apache.catalina.util.LifecycleBase.level = FINE# To see debug messages in TldLocationsCache, uncomment the following line:#org.apache.jasper.compiler.TldLocationsCache.level = FINE]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
        <tag>Java Web</tag>
        <tag>log</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase 简介和使用 Sqoop 同步 Mysql 数据到 HBase]]></title>
    <url>%2F2018%2F08%2F26%2F%E6%8A%80%E6%9C%AF%2Fhbase%2Fhbase-sqoop%2F</url>
    <content type="text"><![CDATA[HBase 数据模型Namespace: 命名空间类似于关系型数据库中的 database schema Table: 表一个 Namespace 下有多个表，一个表可以包含多个行 Row: 行在 HBase 中 Row 由一个 Row Key 和一个或多个列及其值组成，数据值的存储按照 Row Key 的字典顺序存储的。 Column: 列在 HBase 中， 每个列有它所属的 Column Family(列簇)， 以及Column Qualifier(列修饰符), 列名组成是 Column Family:Column Qualifier Column Family: 列簇在 HBase 中将列进行分类，每个列都有它所属的列簇，列簇 把列和相应的值物理上联合在一起。创建表的时候，必须指定至少一个 列簇。每个列出是一个存储属性的集合， Column Qualifier: 列修饰符列簇 和 列修饰符 才是实际意义上的列唯一标识，假设存在 列簇 content, 可以存在 列修饰符 xml, 组成一个唯一的列标识 content:xml；创建表的时候，列簇 已经被指定了，但是 列修饰符 是可变的，可以再 put 指令中随意指定属于 列簇 的 列修饰符。 Cell一个Cell是行，列簇和列修饰符的组合，并且包含一个值和时间戳，时间戳代表着值的版本。 Timestamp（时间戳）一个时间戳是连同值一起被写入的，是值版本的唯一标识，默认情况下，时间戳表示数据写入时RegionServer的时间，但是当你在写数据到Cell的时候，你可以指定一个不同的时间戳。 HBase 常用指令12345678910111213141516&gt; create_namespace 'n1' // 创建一个 namespace n1&gt; list_namespace // 列出所有的 namespace&gt; create 'n1:t1', 'CF1', 'CF2' // 创建表 t1&gt; list_namespace_tables 'n1' // 列出 namespace n1 下的所有 table&gt; describe 'n1:t1' // 查看表结构&gt; put 'n1:t1', 'rk', 'CF1:name', 'test' // 往表 n1:t1 中 row key 是 rk 的行中插入列名称是 CF1:name 的值 'test'&gt; get 'n1:t1', 'rk' // 获取表 'n1:t1' 中 row key 是 rk 的所有数据&gt; scan 'n1:t1' // 模糊查看表 n1:t1&gt; scan 'n1:t1', FILTER=&gt;"ColumnPrefixFilter('name') AND ValueFilter(=,'substring:test')" // 模糊查询，列修饰符前缀为name 且值中包含字段 test 的数据&gt; delete 'n1:t1', 'rk', 'CF1:name' // 删除 row key 是 rk 列 `CF1:name` 的数据&gt; disable 'n1:t1' // 禁用表 n1:t1，被被删除之前必须先被禁用&gt; is_enabled 'n1:t1' // 查看表 n1:t1 是否可用&gt; is_disabled 'n1:t1' // 查看表 n1:t1 是否被禁用&gt; enable 'n1:t1' // 启用表 n1:t1&gt; drop 'n1:t1' // 删除表 n1:t1，注意：只能删除被禁用的表&gt; drop_namespace 'n1' // 删除命名空间 n1，注意：只能删除没有表的 namespace sqoop 导出 mysql 数据到 HBase12export HADOOP_CLASSPATH=/absolute/path/to/mysql-connector-java-5.1.15.jar sqoop import --connect jdbc:mysql://ip:port/database_name --username 'username' --password 'password' --table 'table_name' --columns "id,name,code,description" --hbase-table 'test:hbase_table_name' --hbase-create-table --hbase-row-key 'id,code' --column-family info 上述命令行解析 设置 HADOOP_CLASSPATH首先需要设置 HADOOP_CLASSPATH，值是 mysql-connector-java-5.1.15.jar 的绝对路径，否则会报错：java.lang.RuntimeException: Could not load db driver class: com.mysql.jdbc.Driver --connect连接数据库的url，从这个数据库中导出数据 --username数据库用户名 --password数据库密码 --table导出数据的源数据库表 --columns本次导出的数据，可以一次导出多列，用逗号分隔，导出的列在hbase 中属于 --column family 参数指定的列簇，列名称是 column family:mysql表中的列名，需要注意的是，如果没有指定参数 --hbase-row-key，在hbase 表中的row key 将是 --columns 中第一列。 --hbase-table本次导入数据的 hbase 表，需要注意的是导入数据的hbase 表可以不存在，但是hbase 表所属的 namespace 必须是存在的，否则会报错： Import failed: org.apache.hadoop.hbase.NamespaceNotFoundException: org.apache.hadoop.hbase.NamespaceNotFoundException: &apos;namespace&apos; --hbase-create-table如果导入数据的表不存在，则创建该表 --hbase-row-key设置 hbase 中的 Row Key，参数值是mysql 表中的列名，可以设置多个列合并成 Row Key, 用逗号分隔 --column-family指定导入数据所属的列簇，每次导入数据只能导入属于同一个列簇 的数据，如果 mysql 表中数据属于多个 列簇，只能通过多条指令分批导入。 注意：上述指令没有指定列分隔符和行分隔符，默认的列分隔符是 &#39;\001&#39;，在less 中显示是 ^A；默认的行分隔符是 &#39;\n&#39;。]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
        <tag>Sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《程序化广告——个性化精准投放实用手册》常见问题导读]]></title>
    <url>%2F2018%2F08%2F01%2F%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%2F%E7%A8%8B%E5%BA%8F%E5%8C%96%E5%B9%BF%E5%91%8A-%E4%B8%AA%E6%80%A7%E5%8C%96%E7%B2%BE%E5%87%86%E6%8A%95%E6%94%BE%E5%AE%9E%E7%94%A8%E6%89%8B%E5%86%8C-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E5%AF%BC%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[问：与传统广告购买相比，程序化广告有什么优势？答： 传统广告购买模式下，广告主需要和总多媒体或者广告代理逐个谈判购买媒体资源；程序化购买模式下，广告主可以通过 DSP、TD 或广告代理来对接 Ad Exchang(ADX) 或 SSP；对于程序化广告，通过 DMP 的数据可以做到精准定向投放，还可以结合 PCP(程序化创意平台) 进行个性化广告制作，实现 “千人千面”；借助 广告验证平台 进行广告验证；还可以满足广告主对 品牌安全，反作弊过滤，广告可见度分析等方面的要求。对于广告主，程序化广告无需耗费人力一一对接各个媒体渠道，直接加入交易市场即可；程序化广告的精准投放能减少预算浪费并提升转化率；实时竞价和实时投放是的广告数据的收集、分析和优化过程变动更简单、高效。对于媒体，程序化广告无需媒体方好美人力资源和广告主或广告代理逐一谈判、对接，只需要对接交易市场；程序化广告能关注每个广告流的销售，将流量进行分级售卖，提高变现能力；程序化广告能更贴合用户的需求，改善媒体用户的用户体验。 问：开发和运营需方平台有什么门槛？答：开发和运营需求方平台有 技术、资金、流量资源、广告主资源等方面的能力1. 技术：DSP 需要有一下技术能力1.1 参与RTB，实时竞价的能力，保证100ms 内作出响应。1.1.1 数据收集能力：及时收集竞价、曝光和点击数据1.1.2 数据分析能力：竞价过程中DSP 需要分析大量流量数据数据，分析流量是否符合广告投放需求。1.1.3 日志数据处理能力：竞价过程中 DSP 需要出价，DSP 需要对日志进行实时统计和离线统计，然后进行 CTR预估、转化率预估和点击价值预估。1.1.4 用户识别能力：精准定向能力来自DSP 的用户识别能力和定向能力，DSP 需要支持Cookie 和移动设备ID映射去识别用户，DSP 需要收集用户行为数据，清晰的刻画用户画像。2. 资金：DSP 系统需要足够的资金去维持各项硬件设备；还需要足够的运营资金支撑专业团队以及大客户业务3. 流量资源：DSP 需要满足广告主对流量的需求4. 广告主资源：DSP 需要有足够的广告主资源保证广告填充率，同时足够的广告预算去满足资金需求5. 专业人才：技术人才、运营人才5.1 DSP 需要技术人才高效的开发和维护DSP 系统5.2 DSP 需要有专业的运营人才在数据分析的基础上，根据经验，及时调整投放过程中的各项决策 问：QPS 是什么意思？流量、竞价请求、QPS 三者之间是什么关系？答：QPS(Query Per Second) 是每秒查询率，是对服务器在规定时间内所处理流量多少的衡量标准一个竞价请求一般只携带一个竞价请求，但是为了节省网络流量等原因，部分渠道会通过一个竞价请求写到同一页面不同广告位的多个流量竞价服务会根据DSP的消耗能力(广告填充率、出价和竞得率)设置固定的QPS阈值，或者根据DSP出价和竞得率自动调整QPS阈值DSP 也会在ADX/SSP后台设置它能接收到QPS阈值，防止DSP系统过载 问：如何选择需求方平台，有哪些评估指标？答：评估需求方有如下指标：媒体资源、技术能力、数据实力、算法能力、公司背景、服务能力、收费模式 问：开发和运营广告交易平台有什么门槛？答：开发和运营广告交易平台需要 技术完整性、运营规范化、足够的填充率技术完整性：完整的流量管理、竞价规则、审核管理等技术机制，保证RTB 流程的高效、完整性运营规范化：规范的DSP技术对接文档、规范化的运营流程足够的填充率：媒体方要求交易平台保证足够的填充率，这取决于交易平台对接的流量质量和规模、价格政策还有数据开放性 问：程序化广告常用的第三方技术服务有哪些？提供这些服务的公司有哪些？答：程序化广告需要的第三方服务有 ****程序化创意平台(Programmatic Creative Platform)：通过技术自动生成海量创意，并利用算法和数据对不同受众动态的展示广告并进行创意优化，提供服务的有舜飞科技、筷子科技、Sizmek广告验证平台(Ad Verification Platform)：足够的填充率：媒体方要求交易平台保证足够的填充率，这取决于交易平台对接的流量质量和规模、价格政策还有数据开放性 问：品牌广告如何考核投放效果？*答：品牌广告有如下考核标准 * 品牌广告投放效果的考核主要从广告效果，成本收益，投放保障三个方面进行 ****1. 广告效果：衡量广告受众的活跃度和对广告的接受度。从广告效果的层级来说，传播效果可以用广告曝光维度衡量；广告达到的心里效果可以通过受众的行为效果进行衡量；行为效果可以从落地页/网站/APP访问以及用户互动角度进行考核。2. 成本收益：衡量广告投放整体消耗、各渠道等维度的性价比3. 投放保障：一般是衡量广告投放的真实性和安全性，用于保障广告效果以及成本收益。 * 具体的考核标准如下：**** 独立访客数(UV, Unique Visitor)，在特定时间内访问页面的虚拟自然人（用客户端标识）数量 **** 频次(Frequency)，同一波广告投放活动中，每个独立访客接触广告的次数 **** 广告可视度(Viewability)，广告出现在窗口可见区域的广告曝光量占广告总曝光量的占比；具体可见曝光定义见可见曝光定义表 **** 广告可见的TA浓度(Viewable TA%，Target Audience)，TA% = 目标市场中看到广告的TA / 目标市场的广告受众总数 **** 广告可见的TA到达率(Viewable TA N+Search)，目标市场中看到广告N次及以上的的TA数量 / 目标市场广告受众总数 **** 广告可见的互联网总收视点(Viewable IGRP, Internet Gross Rating Points)， IGRP = 平均曝光频次 \ 到达率 * 100 ** ** 点击率(CTR, Click Through Rate)，广告点击次数占广告展示次数的占比** 互动率(Engagement Rate)，用户衡量广告投放中用户在广告素材或者站内(网站或APP)的交互度和参与度** 回搜率(Search Conversion)，广告受众接触广告后在各个平台中搜索广告主相关关键词的用户占比，用于衡量品牌广告对用户品牌认知的提升程度** ** 每点击成本(CPC, Cost Per Click)，广告被点击一次对应的价格 ** 每千人成本(CPM, Cost Per Mille\Cost Per Thousand Impressions)，广告被展示1000次对应的价格 **** 每UV成本(CPUV, Cost Per UV)，广告曝光中每获取一个 UV 的价格；计算公式为 CPUV = 广告消耗金额 / UV数量 **** 单次交互成本(CPE, Cost Per Engagement) 或 (CPEV, Cost Per Engaged Visit)，每获得一个受众互动行为对应的价格 **** 受众对广告的浏览\互动时长的成本(CPH, Cost Per Hour)，受众对广告的浏览\互动时长的成本，一定程度上代表了广告对受众品牌意识的影响力疑问点：广告主在多个渠道投放了品牌广告，如何确认回搜行为关联的品牌广告是在哪个渠道投放的？ ** 跳出率(Bounce Rate)，指只浏览了入口页面就离开的访问流量占进入该页面的总流量的比例，用于衡量用户点击广告后进入页面的访问质量 **** 二跳率(2nd-click Rate)，二跳指用户进入落地页之后在页面的首次有效点击，二跳率是二跳量占进入该页面的总访问量的比例 ** ** 品牌安全(Brand Safety)，品牌广告的投放环境要求非常高，不允许出现品牌广告出现在某些网站或媒体上，也不允许通过敏感\非法的关键词触发广告；广告主一般通过广告验证平台过滤敏感或非法关键词/页面。** 反作弊(Anti-Fraud)，品牌广告主的考核标准更容易被作弊，因此广告主需要考量服务方的反作弊能力** 无效流量验证(Invalid Traffic Verification)，作弊流量只是无效流量的一部分，无效流量的定义见 MMA（中国无线营销联盟）移动互联网广告无效流量验证标准V.1.0 ；**** 第三方监控差异(Discrepancy)，第三方数据和DSP平台统计数据之间的差异值 ** 问：OTV 是指什么？OTV广告的考核指标有哪些？*答：OTV 是 Online TV（网络电视或网络视频），主要的考核指标包含上一问题中的 广告可见度, Viewable TA%, Viewable TA N+Reach, Viewable IGRP 以及基本的广告考核标准 * 问：广告可视度的标准是什么？答：广告可视度(Viewability) 指广告出现在窗口可见痊愈的广告曝光量占广告总曝光率的比例。 MMA(中国无线营销联盟) 制定的 移动互联网广告可见性验证标准 V.1.0 中对可见性的定义标准遵从 IBA(互动广告局) 和 MRC(媒体分级委员会) 对可视度的计算标准的定义，具体内容如下：首先需要注意的是，能够被评价广告可视度的广告都必须是有效流量，无效流量是不需要计算可视度的1. 对测量对象的一般要求** &nbsp;&nbsp;&nbsp;&nbsp;1) 客户端计数** &nbsp;&nbsp;&nbsp;&nbsp;2) 过滤非人类流量和无效流量** &nbsp;&nbsp;&nbsp;&nbsp;3) 缓存清除技术** &nbsp;&nbsp;&nbsp;&nbsp;4) 区分明显的（机器人）自动刷新行为与人为活动** &nbsp;&nbsp;&nbsp;&nbsp;5) 区分被遮挡及不在显示区域的曝光** &nbsp;&nbsp;&nbsp;&nbsp;6) 公开内部材料和流量的传输** &nbsp;&nbsp;&nbsp;&nbsp;7) 媒体，门户网站，广告服务器，广告网络和交易所的完整公开**2. 对可见广告的曝光要求 广告类型 面积 时长 窗口要求 PC 展示广告（普通） &gt;= 50% &gt;= 1秒 浏览器的可视空间 PC 展示广告（大） &gt;= 30% &gt;= 1秒 浏览器的可视空间 PC 视屏广告 &gt;= 50% &gt;= 2秒 浏览器的可视空间 移动展示广告 &gt;= 50% &gt;= 1秒 移动浏览器或App终端的可视空间 移动视屏广告 &gt;= 50% &gt;= 2秒 移动浏览器或App终端的可视空间 问：如何保障品牌安全，需要规避哪些类别？答：品牌广告对于 投放环境 有很高的要求，不能让广告受众对品牌产生负面影响，品牌安全 要求每个广告验证服务商维护并公开潜在规避类别1. IAB 内容分级法规定的规避类别 **** · 成人内容（Audit Content）** · 协助非法活动（Facilitation of Illeagl Activities）** · 有争议的主题，即违反现有的社会规范，如神秘、禁忌、反常的生活方式（Controversial Subjects, contrary to existing social norms, such as OccultTaboos、Unusual Lifestyles）** · 侵犯版权（Copyright Infringement）** · 药物\酒精\受管制药品（Drugs\Alcoho\Controlled Substances）** · 极端的图像\明显暴力内容（Extreme Graphic\Explicit Violence）** · 诱导篡改度量衡的问题（Incentivized Manipulation of Measurements）** · 仇恨\亵渎（Hate\Profanity）** · 骚扰\间谍软件\恶意软件\盗版软件（Nuisance\SpywareMalware\Warez） **** · 政治\宗教（Political\Religion）** · 未经认证的有用户生成的内容（Unmoderated User Generated Content）***2. MMA China 品牌安全和流量质量小组的规避类别 *** · 分裂言论 **** · 讣告 **** · 邪教相关言论 **** · 恐怖主义言论 **** · 党和国家领导人（包括不限于文字、影像等信息）旁 **** · 革命烈士（包括不限于文字、影像等信息）旁 **** · 在广告主媒介计划以外 **]]></content>
      <categories>
        <category>计算广告</category>
      </categories>
      <tags>
        <tag>程序化广告</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo-next-github-page]]></title>
    <url>%2F2018%2F07%2F29%2F%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2Fhexo-next-github-page%2F</url>
    <content type="text"><![CDATA[安装 Hexo安装 Hexo 的前提你已经安装以下安装程序 1. Node.js 2. Git安装 Node.js1brew install node 或者从 [官网](https://nodejs.org/en/ 官网) 下载安装包，傻瓜式安装 安装 Git1brew install git 或者从 [官网](https://git-scm.com/downloads 官网) 下载安装包，傻瓜式安装 安装 Hexo1npm install -g hexo-cli 查看安装版本 1234567891011121314151617➜ ~ hexo -vhexo-cli: 1.1.0os: Darwin 16.3.0 darwin x64http_parser: 2.8.0node: 10.7.0v8: 6.7.288.49-node.15uv: 1.22.0zlib: 1.2.11ares: 1.14.0modules: 64nghttp2: 1.32.0napi: 3openssl: 1.1.0hicu: 62.1unicode: 11.0cldr: 33.1tz: 2018e Hexo 使用简介初始化 Hexo 文件夹12mkdir /path/to/my/blog/source/code ## 传建一个保存博客源代码的目录hexo init ## 初始化一个 Hexo 文件夹 然后我们来看看初始化后的 hexo 文件夹 . ├── _config.yml ## 配置文件 ├── node_modules ## npm 依赖文件夹 ├── package-lock.json ## 根据 package.json 文件生成的版本依赖锁定文件，指定了依赖的确定版本 ├── package.json ## 声明 hexo 的所有依赖机器版本，详见 https://docs.npmjs.com/getting-started/using-a-package.json ├── scaffolds ## 存放模板的文件夹，hexo new &apos;file&apos; 指令创建新文档的时候会使用 scaffolds 中的模板 ├── source ## hexo 源文件 └── themes ## hexo 使用的主题文件夹存放位置创建一个新文档1hexo new 'newfile' ➜ ... hexo new &apos;new file&apos; INFO Created: ~/.../source/_posts/new-file.md我们可以看到在 source 文件夹下面新建了一个新的 markdown 文件 生成 Hexo 静态文件1hexo generate ## 可以使用简写指令 [hexo g] 下面我们可以看到 Hexo 文件夹下多了一个 public 文件夹 . ├── _config.yml ├── db.json ├── node_modules ├── package-lock.json ├── package.json ├── public ## 存放生成的静态文件，包含 js、css、html、图片 ├── scaffolds ├── source └── themes启动 Hexo 服务1hexo server ## 简写指令 hexo s 现在我们可以通过 http://localhost:4000/ 来访问我们搭建的网站 一键部署到 GitHub修改 _config.yml 文件deploy: type: git repo: &lt;repository url&gt; branch: [branch] message: [message]安装 hexo-deployer-git1npm install hexo-deployer-git --save 安装 hexo-deployer-git 遇到的问题&gt; npm install hexo-deployer-git --save npm WARN deprecated swig@1.4.2: This package is no longer maintained + hexo-deployer-git@0.3.1 added 31 packages from 36 contributors and audited 2296 packages in 10.148s found 1 low severity vulnerability run `npm audit fix` to fix them, or `npm audit` for details让我们执行 npm audit 指令来查看具体问题 ➜ hexo.test.blog npm audit === npm audit security report === ┌──────────────────────────────────────────────────────────────────────────────┐ │ Manual Review │ │ Some vulnerabilities require your attention to resolve │ │ │ │ Visit https://go.npm.me/audit-guide for additional guidance │ └──────────────────────────────────────────────────────────────────────────────┘ ┌───────────────┬──────────────────────────────────────────────────────────────┐ │ Low │ Regular Expression Denial of Service │ ├───────────────┼──────────────────────────────────────────────────────────────┤ │ Package │ uglify-js │ ├───────────────┼──────────────────────────────────────────────────────────────┤ │ Patched in │ &gt;=2.6.0 │ ├───────────────┼──────────────────────────────────────────────────────────────┤ │ Dependency of │ hexo-deployer-git │ ├───────────────┼──────────────────────────────────────────────────────────────┤ │ Path │ hexo-deployer-git &gt; swig &gt; uglify-js │ ├───────────────┼──────────────────────────────────────────────────────────────┤ │ More info │ https://nodesecurity.io/advisories/48 │ └───────────────┴──────────────────────────────────────────────────────────────┘ found 1 low severity vulnerability in 2296 scanned packages 1 vulnerability requires manual review. See the full report for details.解决方案添加淘宝 npm 镜像源 1npm config set registry https://registry.npm.taobao.org 然后我们可以继续安装 hexo-deployer-git 了 部署网站1hexo deploy ## 简写指令 hexo d 部署指令将生成 .deploy_git 文件加，我们需要在 .deploy_git 文件中指定远程 git 链接 1git remote add origin giturl 清除缓存和已创建的静态文件1hexo clean 生成并部署 Hexo 网站12hexo d -ghexo g -d 这两个指令是等价的，都是先构建本地静态文件，再部署网站 使用 NexT 主题下载 NexT 主题12cd your-hexo-sitegit clone https://github.com/iissnan/hexo-theme-next themes/next 修改站点配置文件修改 _config.yml 文件中的 theme 配置 1theme: next # next 是 themes 文件下主题文件夹的名称，冒号后面必须有空格，这是 yaml 语法 现在你可以执行以下指令去构建静态文件并且部署网站了 123hexo cleanhexo ghexo s 然后你可以访问 http://localhost:4000/ 去访问你的博客了 配置 NexT 主题修改菜单栏修改 123456789menu: | menu: home: / || home | home: / || home # 主页，默认配置打开 #about: /about/ || user | #about: /about/ || user # 关于自己，可以配置 #tags: /tags/ || tags | tags: /tags/ || tags # 标签页，默认配置关闭，需要你打开注释 #categories: /categories/ || th | categories: /categories/ || th # 分类页，默认配置关闭，需要你打开注释 archives: /archives/ || archive | archives: /archives/ || archive # 归档页，默认配置打开 #schedule: /schedule/ || calendar | #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap | #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat # 公益404页，默认配置关闭，需要你打开注释 配置标签页 创建标签页 1hexo new page "tags" 修改 themes/next/_config.yml 文件去除 menu.tags 前的 # 修改标签页标题修改 source/tags/index.md 文件中的 title，写一个你喜欢的标题 配置分类页 创建标签页 1hexo new page "categories" 修改 themes/next/_config.yml 文件去除 menu.categories 前的 # 修改标签页标题修改 source/categories/index.md 文件中的 title，写一个你喜欢的标题 选择 schemethemes/next/_config.yml 文件中默认主题是 Muse，我选择 Mist # Schemes scheme: Muse # 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白 #scheme: Mist # Muse 的紧凑版本，整洁有序的单栏外观 #scheme: Pisces # 双栏 Scheme，小家碧玉似的清新 #scheme: Gemini自定义样式修改页面宽度编辑主题的 source/css/_variables/custom.styl 文件，新增变量： // 当屏幕宽度 &lt; 1600px, 修改成你期望的宽度 $content-desktop = 900px // 当视窗超过 1600px 后的宽度 $content-desktop-large = 1300px生成文章摘要 在文章中使用 &lt;!-- more --&gt; 手动进行截断，在 &lt;!-- more --&gt; 上方撰写摘要，Hexo 提供的方式 【推荐】 在文章的 front-matter 中添加 description，并提供文章摘录，我选择这种 修改作者名称、描述、语言、时区修改 _config.yml 文件 12345678# Site | # Sitetitle: Ice summer bug's notes | title: Hexosubtitle: | subtitle:description: About technology and about life. | description:keywords: | keywords:author: Liam Chen | author: John Doelanguage: zh-Hans | language:timezone: Asia/Shanghai 修改作者头像修改 themes/next/_config.yml 1avatar: /images/headPicture.png 使用 GitHub PagesGitHub Pages 使用教程有很多，这里不做赘述，主要是将 .deploy_git 文件夹托管到 GitHub 上，并设置成 GitHub Pages _config.yml 文件中的 deploy.repo 设置成 github url 我们还可以再建一个 github repository 来管理 Hexo 文件夹]]></content>
      <categories>
        <category>GitHub Pages</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>NexT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd 使用入门]]></title>
    <url>%2F2018%2F05%2F21%2F%E6%8A%80%E6%9C%AF%2Fetcd%2F2018-05-21-etcd%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[etcd 使用入门前言介绍 etcd 和 raft协议 etcd 安装etcd 的安装有两种方式 直接从官网下载 release 版本的二进制文件 下载源码手动编译安装 个人倾向于使用 release 版本安装，下面开始下载安装 找到下载的压缩包，解压 1unzip etcd-v3.3.8-darwin-amd64.zip 将解压的文件夹的移动到安装目录中 1sudo mv /path/to/etcd /usr/local/etcd 此时我们可以先看看解压后有什么？ ➜ etcd-v3.3.8-darwin-amd64 ll total 113064 drwxr-xr-x 22 ? staff 748B 6 16 00:55 Documentation -rw-r--r-- 1 ? staff 38K 6 16 00:55 README-etcdctl.md -rw-r--r-- 1 ? staff 7.1K 6 16 00:55 README.md -rw-r--r-- 1 ? staff 7.7K 6 16 00:55 READMEv2-etcdctl.md drwx------ 3 ? staff 102B 7 16 23:45 default.etcd -rwxr-xr-x 1 ? staff 30M 6 16 00:55 etcd -rwxr-xr-x 1 ? staff 25M 6 16 00:55 etcdctl可以看到这里有两个可执行程序: etcd 和 etcdctl etcd: etcd 服务端程序etcdctl: etcd 客户端程序 启动程序 3.1 使用默认配置启动程序 1./etcd 3.2 来一些启动配置，启动一个集群 1234567891011121314151617181920212223242526nohup ./etcd --name test1 --initial-advertise-peer-urls http://localhost:2380 \ --listen-peer-urls http://localhost:2380 \ --listen-client-urls http://localhost:2379,http://127.0.0.1:2379 \ --advertise-client-urls http://localhost:2379 \ --initial-cluster-token test-cluster \ --initial-cluster test1=http://localhost:2380,test2=http://localhost:2390,test3=http://localhost:2400 \ --initial-cluster-state new \ --data-dir /app/etcd/data1 &amp;nohup ./etcd --name test2 --initial-advertise-peer-urls http://localhost:2390 \ --listen-peer-urls http://localhost:2390 \ --listen-client-urls http://localhost:2389,http://127.0.0.1:2389 \ --advertise-client-urls http://localhost:2389 \ --initial-cluster-token test-cluster \ --initial-cluster test1=http://localhost:2380,test2=http://localhost:2390,test3=http://localhost:2400 \ --initial-cluster-state new \ --data-dir /app/etcd/data2 &amp;nohup ./etcd --name test3 --initial-advertise-peer-urls http://localhost:2400 \ --listen-peer-urls http://localhost:2400 \ --listen-client-urls http://localhost:2399,http://127.0.0.1:2399 \ --advertise-client-urls http://localhost:2399 \ --initial-cluster-token test-cluster \ --initial-cluster test1=http://localhost:2380,test2=http://localhost:2390,test3=http://localhost:2400 \ --initial-cluster-state new \ --data-dir /app/etcd/data3 &amp; ETCD 简单指令操作 ETCD 数据插入 1./etcdctl put key value 插入文件数据到 ETCD 1cat file | ./etcdctl put key 数据查看 1./etcdctl get key 集群健康度查看 1./etcdctl --endpoints=[endpoint1, endpoint2, endpoint3] endpoint health 得到结果 123endpoint1 is healthy: successfully committed proposal: took = 932.637µsendpoint2 is healthy: successfully committed proposal: took = 1.058401msendpoint3 is healthy: successfully committed proposal: took = 1.127266ms 集群节点状态查询 1./etcdctl --endpoints=[endpoint1, endpoint2, endpoint3] endpoint status 得到结果 123endpoint1, 180821f2462664c9, 3.2.12, 555 MB, true, 169, 12167260endpoint2, b2b4375ce5b9bb02, 3.2.12, 555 MB, false, 169, 12167260endpoint3, e4927ddc8eb44d9e, 3.2.12, 555 MB, false, 169, 12167260 注意：`ETCD` 的API 分为 `V2` 和 `V3` 两个版本，两者之间差距很大，上述 `etcdctl` 客户端的使用都是 `V3` API，在执行之前，请执行命令 `export ETCDCTL_API=3`]]></content>
      <categories>
        <category>etcd</category>
      </categories>
      <tags>
        <tag>etcd</tag>
        <tag>中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang GC]]></title>
    <url>%2F2018%2F05%2F16%2F%E6%8A%80%E6%9C%AF%2Fgolang%2F2018-05-16-golang-GC%2F</url>
    <content type="text"><![CDATA[golang GC 浅谈Garbage collector (GC). The GC runs concurrently with mutator threads, is type accurate (aka precise), allows multiple GC thread to run in parallel. It is a concurrent mark and sweep that uses a write barrier.golang 的 GC 和工作线程并行运行，?类型准确？，允许多个GC 线程并发运行, 通过 write barrier 实现并发的标记清除。 It is non-generational and non-compacting. Allocation is done using size segregated per P allocation areas to minimize fragmentation while eliminating locks in the common case.golang GC 不分代，不压缩。 golang 在分配内存的时候是预先将内存划分为固定大小的内存块，以尽量减少碎片，同时消除常见情况下的锁定。 The algorithm decomposes into several steps. This is a high level description of the algorithm being used. For an overview of GC a good place to start is Richard Jones&apos; gchandbook.org.gc 算法被分解成多个步骤这里是对 垃圾回收 算法的高度概括，对于 垃圾回收 算法的综述和入门学习的资料是 Richard Jones 的 gchandbook.org The algorithm&apos;s intellectual heritage includes Dijkstra&apos;s on-the-fly algorithm, see Edsger W. Dijkstra, Leslie Lamport, A. J. Martin, C. S. Scholten, and E. F. M. Steffens. 1978. On-the-fly garbage collection: an exercise in cooperation. Commun. ACM 21, 11 (November 1978), 966-975. For journal quality proofs that these steps are complete, correct, and terminate see Hudson, R., and Moss, J.E.B. Copying Garbage Collection without stopping the world. Concurrency and Computation: Practice and Experience 15(3-5), 2003. 1. GC performs sweep termination.gc 执行清除 a. Stop the world. This causes all Ps to reach a GC safe-point.SWT(Stop the word)：使所有用户线程到达一个 GC 安全点 b. Sweep any unswept spans. There will only be unswept spans if this GC cycle was forced before the expected time. 2. GC performs the &quot;mark 1&quot; sub-phase. In this sub-phase, Ps are allowed to locally cache parts of the work queue. a. Prepare for the mark phase by setting gcphase to \_GCmark (from \_GCoff), enabling the write barrier, enabling mutator assists, and enqueueing root mark jobs. No objects may be scanned until all Ps have enabled the write barrier, which is accomplished using STW. b. Start the world. From this point, GC work is done by mark workers started by the scheduler and by assists performed as part of allocation. The write barrier shades both the overwritten pointer and the new pointer value for any pointer writes (see mbarrier.go for details). Newly allocated objects are immediately marked black. c. GC performs root marking jobs. This includes scanning all stacks, shading all globals, and shading any heap pointers in off-heap runtime data structures. Scanning a stack stops a goroutine, shades any pointers found on its stack, and then resumes the goroutine. d. GC drains the work queue of grey objects, scanning each grey object to black and shading all pointers found in the object (which in turn may add those pointers to the work queue).write barrier: 写屏障]]></content>
      <categories>
        <category>golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>gc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang 内存管理浅析]]></title>
    <url>%2F2018%2F05%2F16%2F%E6%8A%80%E6%9C%AF%2Fgolang%2Fgolang%20%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言从一个问题和回复开始了解 go语言内存管理 How do I know whether a variable is allocated on the heap or the stack?From a correctness standpoint, you don&apos;t need to know. Each variable in Go exists as long as there are references to it. The storage location chosen by the implementation is irrelevant to the semantics of the language. The storage location does have an effect on writing efficient programs. When possible, the Go compilers will allocate variables that are local to a function in that function&apos;s stack frame. However, if the compiler cannot prove that the variable is not referenced after the function returns, then the compiler must allocate the variable on the garbage-collected heap to avoid dangling pointer errors. Also, if a local variable is very large, it might make more sense to store it on the heap rather than the stack. In the current compilers, if a variable has its address taken, that variable is a candidate for allocation on the heap. However, a basic escape analysis recognizes some cases when such variables will not live past the return from the function and can reside on the stack.从 go语言官网 的问题列表中有上面这个问题，简单翻译如下 我们如何知道变量是分配在堆上还是栈上？明确地说，你不需要知道答案。 go语言的每个变量当它存在引用的时候它就会一直存在，语言对存储位置的选择与语言的语义无关。 存储位置确实会影响编写高效的程序。如果可能，Go编译器将在函数的栈中给本地变量分配存储空间。但是，如果编译器在函数返回后无法证明变量未被引用，则编译器必须在堆上分配变量以避免空指针。此外，如果局部变量非常大，将它存储在堆而不是栈上可能更有意义。 在当前主流的编译器中，如果变量有其他访问地址，则该变量是堆上分配的候选变量。但是，基本的逃逸分析可以识别某些情况，这些生命周期只在函数周期内的变量将分配在栈上。这个问题和答案告诉我们两个事情，一个是简单介绍了 逃逸分析，另一个就是 go语言官方开发者不认为我们需要了解 内存分配，现在我们需要去了解 逃逸分析 和 内存管理。 逃逸分析先看看 维基百科 上关于 逃逸分析 的说明 「逃逸分析」是编译程序优化理论中确定指针动态范围的方法 ———— 分析程序哪些地方可以访问指针，它涉及到指针分析和形状分析。 当一个变量在子程序中被分配时，一个指向变量的指针可能会逃逸到其它执行程序中，或者去调用子程序。如果使用尾递归优化（通常在函数编程语言中是需要的），对象也可能逃逸到被调用的子程序中。 如果一个子程序分配一个对象并返回一个该对象的指针，该对象可能在程序中的任何一个地方被访问到——这样指针就成功“逃逸”了。如果指针存储在全局变量或者其它数据结构中，它们也可能发生逃逸，这种情况是当前程序中的指针逃逸。 逃逸分析需要确定指针所有可以存储的地方，保证指针的生命周期只在当前进程或线程中。golang 逃逸分析示例1234567891011121314151617181920// go run -gcflags="-m -l" escape_ayalysis_demo.gofunc main() &#123; s1 := returnString1() s2 := returnStringPrt1() println("s1:", s1, ", addr: ", &amp;s1, "\ns2:", s2)&#125;//go:noinlinefunc returnString1() (rs1 string) &#123; rs1 = "this is a variable in func will return" println(rs1) return&#125;//go:noinlinefunc returnStringPrt1() (sp1 *string) &#123; rspt1 := "this is a variable in func will return" println(rspt1) return &amp;rspt1&#125; 这里我们使用 go:noinline 来禁止编译器使用内联代码来替换函数调用，然后我们使用 gcflags=&quot;-m -m&quot; 来查看编译器报告，gcflags 最多可以有四个 -m, 一般来说两个 -m 的信息已经足够了 &gt; go run -gcflags=&quot;-m -m&quot; escape_ayalysis_demo.go # command-line-arguments ./escape_ayalysis_demo.go:11:6: cannot inline returnString1: marked go:noinline ./escape_ayalysis_demo.go:18:6: cannot inline returnStringPrt1: marked go:noinline ./escape_ayalysis_demo.go:4:6: cannot inline main: non-leaf function ./escape_ayalysis_demo.go:21:9: &amp;rspt1 escapes to heap ./escape_ayalysis_demo.go:21:9: from sp1 (return) at ./escape_ayalysis_demo.go:21:2 ./escape_ayalysis_demo.go:19:2: moved to heap: rspt1 ./escape_ayalysis_demo.go:7:33: main &amp;s1 does not escape this is a variable in func will return this is a variable in func will return s1: this is a variable in func will return , addr: 0xc42005ff68 s2: 0xc42000e020这里可以看到函数 returnString1 中没有发生逃逸，因为 returnString1 的返回值类型是 string, main 函数通过值复制将 rs1 的值复制一份而得到 s1；而函数 returnStringPrt1 的返回值是 string 类型的指针，在 main 函数中调用了 returnStringPrt1 时，可以访问指针 sp1 指向的变量，此时指针 sp1 和指针 s2 指向同一个变量，因此变量 rspt1 分配在堆空间中。 内存分配Memory allocator.内存分配 This was originally based on tcmalloc, but has diverged quite a bit.golang 的内存分配的方法是在 TCMalloc 的基础上进行了一些改动，TCMalloc(Thread-caching Malloc) 介绍请看http://goog-perftools.sourceforge.net/doc/tcmalloc.html TCMalloc(Thread-caching Malloc)TCMalloc 分配的内存主要是两个地方：线程私有缓存(Thread Local Cache) 和 全局缓存堆（Central Heap）；TCMalloc 为每个线程分配一个私有缓存，把 &lt;=32k 的对象视为 小对象(Small Object)，小对象的的内存分配先尝试从线程私有缓存上分配，如果线程私有缓存空间不足，再从全局缓存堆中申请一部分空间作为线程私有缓存，而周期性的垃圾回收会将线程私有缓存的空间归还给全局缓存堆；而大于 32k 的对象视为 大对象(Large Object)，大对象的分配直接在全局缓存堆上； 小对象内存分配（Small Object Allocation）线程本地缓存是一个单向链表 L1，链表 L1 的每个元素也是一个具有相同大小的空间对象的链表 ln TCMalloc 将线程本地缓存拆分成约170种大小类型的空闲对象列表，依次以 8bytes、16bytes、32bytes等大小递增 小对象内存分配具体步骤如下： 计算对象大小，找到大于对象大小的最小的大小类型 m； 在当前线程中找到该大小类型的空闲对象链表 lm； 如果链表 lm 不为空，从链表中删除第一个空闲对象用于内存分配；这个操作过程中，TCMalloc 不需要加锁，这个对提高内存分配的效率很有帮助； 如果链表 lm 为空，从所有线程共享的 Central Free List 中获取一堆空闲对象，放到当前线程的私有缓存队列上去，在重复步骤3，从线程私有缓存中返回一个空闲对象 The main allocator works in runs of pages.Small allocation sizes (up to and including 32 kB) arerounded to one of about 70 size classes, each of whichhas its own free set of objects of exactly that size.Any free page of memory can be split into a set of objectsof one size class, which are then managed using a free bitmap. 主要的内存分配器是工作在运行中的 ?内存页? 上。小对象分配的 golang 将内存切分成约70种固定大小的内存块，me The allocator’s data structures are: fixalloc: a free-list allocator for fixed-size off-heap objects, used to manage storage used by the allocator.mheap: the malloc heap, managed at page (8192-byte) granularity.mspan: a run of pages managed by the mheap.mcentral: collects all spans of a given size class.mcache: a per-P cache of mspans with free space.mstats: allocation statistics. Allocating a small object proceeds up a hierarchy of caches: Round the size up to one of the small size classesand look in the corresponding mspan in this P’s mcache.Scan the mspan’s free bitmap to find a free slot.If there is a free slot, allocate it.This can all be done without acquiring a lock. If the mspan has no free slots, obtain a new mspanfrom the mcentral’s list of mspans of the required sizeclass that have free space.Obtaining a whole span amortizes the cost of lockingthe mcentral. If the mcentral’s mspan list is empty, obtain a runof pages from the mheap to use for the mspan. If the mheap is empty or has no page runs large enough,allocate a new group of pages (at least 1MB) from theoperating system. Allocating a large run of pagesamortizes the cost of talking to the operating system. Sweeping an mspan and freeing objects on it proceeds up a similarhierarchy: If the mspan is being swept in response to allocation, itis returned to the mcache to satisfy the allocation. Otherwise, if the mspan still has allocated objects in it,it is placed on the mcentral free list for the mspan’s sizeclass. Otherwise, if all objects in the mspan are free, the mspanis now “idle”, so it is returned to the mheap and no longerhas a size class.This may coalesce it with adjacent idle mspans. If an mspan remains idle for long enough, return its pagesto the operating system. Allocating and freeing a large object uses the mheapdirectly, bypassing the mcache and mcentral. Free object slots in an mspan are zeroed only if mspan.needzero isfalse. If needzero is true, objects are zeroed as they areallocated. There are various benefits to delaying zeroing this way: Stack frame allocation can avoid zeroing altogether. It exhibits better temporal locality, since the program isprobably about to write to the memory. We don’t zero pages that never get reused. P: 运行是管理 G 并把他们映射到 Logical Processor，称之为P；P可以看作是一个抽象的资源或者一个上下文，它需要获取以便操作系统线程(称之为M)可以运行G。M: 操作系统线程G: goroutine，golang 中比线程还要轻量级的协程 fixalloc: 固定大小的 1234567891011type fixalloc struct &#123; size uintptr // 固定 first func(arg, p unsafe.Pointer) // called first time p is returned arg unsafe.Pointer list *mlink chunk uintptr // use uintptr instead of unsafe.Pointer to avoid write barriers nchunk uint32 inuse uintptr // in-use bytes now stat *uint64 zero bool // zero allocations&#125;]]></content>
      <categories>
        <category>golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重读计算广告]]></title>
    <url>%2F2018%2F05%2F14%2F%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%2F%E9%87%8D%E8%AF%BB%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%2F</url>
    <content type="text"><![CDATA[前言看了一遍刘鹏老师的《计算广告》，获益匪浅，但是部分地方还是不够明确，决定重读一番，学习笔记记录如下，如有错漏敬请斧正。 在线广告在线广告开创了以人群为投放目标，以产品为导向的技术型投放模式，得到了广告主的青睐。在线广告开启了大规模自动化的利用数据改善产品和提高收入的先河，提供了一种盈利模式。 什么是大数据如果有的数据处理问题无法通过数据采样的方法来降低温处理的复杂程度，就必须利用一些专门为海量数据处理而设计的计算和存储技术来实现。 大数据问题的 4V特征 Volume (规模) Variety (多样性) Velocity (高速) Value (价值) 大数据问题的特征：当数据的采用率的递增叫显著提升解决问题的效果，而且两者基本成正比，这就是典型的大数据问题。 为什么要学习计算广告计算广告作为目前最成熟的得到充分商业化和规模化的大数据应用1） 提供了规模化的将用户行为数据转为可衡量的商业价值的完整产品线和解决方案；2） 孕育和孵化了较为成熟的数据加工和交易产业链，并对其中的用户隐私边界有深入的探讨；3） 由于商业上的限制条件，计算广告的技术和产品逻辑比个性推荐更加复杂周密； 什么是广告广告是由已经确定的`出资人`通过各种`媒介`进行的有关产品（商品、服务和观点）的，通常是有偿的、有组织的、综合的、劝服性的非人员的信息传播活动。 ————《当代广告学》这个定义中有两个主动参与方 ———— 出资人 和 媒介, 还有一个被动的参与方, 被劝服的广告的观看者。在计算广告中术语分别是：需求方(Demand)(广告主、广告主代理商、其他形式的采买方)、供给方(Supply)(媒体、其他形式的变现平台)、 广告受众(Adversiter) 传统广告主要是电视、报纸广告，在大量投放和优化效果广告的能力显然是缺乏的，只能是宣传品牌形象、提升中长期购买率和利润空间；而在线广告可以实现低成本的投放个性化广告，而且部分数字数字（如搜索、电子商务）可以明确的知道用户的意图，能够很好的优化广告效果。 广告的根本目的是广告主通过媒体达到低成本的用户接触 这个定义中的 底成本 需要一个评价指标，这就是 ———— 投入产出比（Return On Investment, ROI），即某次广告活动的总产出与总投入的比例。总投入很容易确定，但是总产出无法确定，但是我们我们也可以通过对各个广告渠道的对比，评估广告成本是否令人满意。 一切付费的信息、产品或服务的传播渠道，都是广告。在线广告创意类型 横幅广告 文字链接广告 富媒体广告 视频广告 前插片广告： 视频播放前的 暂停广告：视频播放中，暂停时播放的广告 社交广告：社交产品中的原生广告 移动广告 邮件定向营销广告 在线广告简史在线媒体出现并发展到一定的流量规模之后，就在页面上直接插入广告位，这种照搬报纸广告在互联网上展示广告的产品形式称为 展示广告(display advertising) , 而售卖广告的模式是采用合同的方式确定某一广告位在某一时间段为某特定广告主所独占，并且根据双方的要求，确定广告创意和投放策略，称为 合约广告。 随着在线媒体的流量快速增长以及在线广告逐渐被广告主了解接受，在线媒体提价的行为被接受，但是媒体流量和品牌认知度都相对稳定之后，提价就不再是提高收入的合适方案了。但是很快在线广告不同于传统媒体广告的本质特点就被发现了：在线广告可以对不同的广告受众展示不同的广告创意 这种广告投放的方式就是 定向广告(trageted advertising)。 而要做到这一点，首先要从技术上获取用户的属性标签，例如用户的性别、年龄等标签，这就是受众定向；而且这时候的广告不能再是静态的嵌入到网页中，而是响应前端的实时请求，根据用户标签自动选择并返回合适的广告， 这就是 广告投放(ad serving)。此时广告的售卖方式还是以合同的形式进行，并且在合同中注明媒体需要保证广告的投放量以及投放量未完成时赔偿方案。这种合约广告的交易方式被称为担保式投放（Guaranteed Delivery）, 这种合约还是主要面对品牌广告主，计费方式是按千次展示收费(Cost per Mille, CPM)。 合约广告有两个技术难点： 各个合约的目标受众会有交叉覆盖，如何有效的将流量分配到交叉覆盖上； 在在线环境下实时的完成每一次展示决策；这两个难点的问题称为 在线分配， 可以使用 带约束优化 的数学框架来探索这个问题。 定向投放的目的是供给方为了拆分流量提高营收，但是如果一开始就提供非常精细的定向，反而会造成售卖率下降，品牌广告主都喜欢优质的流量，如果定向十分精确了略差的流量就卖不出去了。 受众定向的产生使得市场发展呈现两个趋势 定向标签越来越精准； 广告主的数量不断膨胀； 上述两个趋势增加了 在线分配 问题的处理难度，降低了流量的变现能力。从业者开始考虑放弃量的保证，供给方只向广告主保证单位流量的成本，对每次展现都基本按照收益最高的原则来决策，这就是 竞价广告。 竞价广告 的产生原因如上，然而现实中它的出现来自于 搜索广告，搜索引擎的关键词就是一个精准的定向，很自然的就采用的竞价的售卖方式将搜索关键词换成浏览页面中的关键词就成了 上下文广告（信息流广告的鼻祖） 从宏观市场上看 竞价广告 摆脱了 合约广告 中合约的约束，让广告主能够充分竞争；但是微观上的最优方案并不是整个市场的最大收益。竞价机制的发展，诞生了 广义第二高价 (Generalized Second Price, GSP) 竞价理论。 有了竞价机制和受众定向，不被品牌广告主看好的中小互联网媒体将流量打包给一个组织，组织将媒体资源按照人群或者上下文标签打包售卖，用竞价的方式决定流量分配。这个组织就是 ———— 广告网络(ad Network, ADN)。 ADN 采用的计费方式一般是 按点击收费(Cost Per Click, CPC)。最重要的是，ADN 的出现使得不受品牌广告主青睐的比较差流量有了变现的途径。 ADN 只通过出价接口提供价格约定，那么由谁来保证量呢？这就催生了一种需求方产品；此时流量采买发生了几点变化： 流量采买更多的面向受众而非媒体或广告位进行采买； 需求方产品需要通过技术手段保证广告主量的要去，并在此基础上帮助广告主优化效果。 问题：ADN 的竞价流程是什么样的？ AND 定义好定向标签，将流量分类打包出售，而不会去控制每次展示的出价；需求方选择了合适的标签组合并阶段性的调整出价来间接控制效果。而需求方产品对接多个 ADN 或媒体按人群一站式采买广告并优化投入产出比，这就是 交易终端(Trading Desk, TD)。 ADN 将媒体的广告受众打上了用户标签，然后批量售卖，但这不能满足需求方越来越明确的利益要求。例如：某电商需要通过给它的忠实用户投放广告来推广某产品，此时需求方需要按照自己的人群定义来挑选流量；流量拍卖的过程从广告主预先出价批量购买，变成了每次展示时实时出价，媒体方提供广告展示的页面的上下文页面URL和用户标识等信息给需求方，需求方就可以完成定制化的人群选择和出价，这就是 实时竞价(Real Time Bidding, RTB)。市场诞生了一个聚合大量媒体的剩余流量并且采用实时竞价方式为他们变现的产品形态 ———— 广告交易平台（ad Exchange, ADX）。 通过实时竞价的方式，按照定制化的人群标签购买广告，这样的产品就是需求方平台(Demand Side Platform, DSP)。DSP 需要尽可能准确地估计每次展示带来的期望价值，而因为充分的环境信息使得深入计算和估计成为可能。基于 DSP 的广告采买方式叫作 程序化交易(programmatic trade), 除了 RTB 外还有 优选(perfered deals) 以及 私有交易市场(Private Market Place, PMP)。 在在线广告的发展历史上，定向技术和交易形式的进化是一条主线。从固定位置合约交易到受众定向、担保投放，再到竞价交易方式，最后发展成开放的实时竞价交易市场。这一主线的核心驱动力是越来越多的数据源为广告决策提供支持，从而提升广告的效果。广告发展的另外一条主线是产品展现逻辑上的发展：从广告位和内容相对独立，到通过搜索广告认识到内容和广告对立起来未必是好的选择，搜索广告和信息流广告突出的效果就是因为内容的展现和触发逻辑高度一致，因此产生了将内容和广告以某种方式统一决策或排序的广告产品 ———— 原生广告。原生广告 的思路在移动设备上很有前景。 泛广告商业产品泛广告商业产品 的本质都是 付费推广，这些付费推广模式的表现方式更加多样化，用户的感知程度和参与程度也大不相同，但是产品的销售模式却与狭义的广告基本相同，但这些商业产品本质上也是在线广告。 团购 游戏联运 固定位导航 返利购买 计算广告基础计算广告的主要特点有 可衡量的效果以及相应的计算优化。 利润优化问题的概念框架 广告产品的收入衡量指标,也是计算广告中最核心的可衡量指标 ———— 千次展现期望收入(eCPM, excepted Cost Per Mille)。eCPM 可分解为 点击率(Click Through Rate, CTR) 和 点击价值 在线广告产品有很多种计费方式，不同的计费方式反映了不同的市场分工; 也就是说供给方和需求方如何分工估计点击率和点击价值，与整个市场的资源优化配置有关。 广告信息接收过程 曝光：广告在页面上展示了 关注： 广告受众关注到了页面上的广告 理解： 广告受众理解了广告的诉求 接受： 广告表达的诉求得到广告受众的认同 保持： 追求长期转化的品牌广告，希望传达的信息给用户留下深刻的印象 决策： 广告最终带来的实际转化行为，例如购买 互联网广告的技术特点 技术和计算导向： 数字媒体的特点使得在线广告可以进行精确的受众定向；“由于在线广告有独特的竞价交易方式，可以对广告效果进行有效的预估和优化”。 效果的可衡量性： 广告的展示、点击数据直接可以衡量广告的效果 创意和投放方式的标准化：标准化的驱动力来自于受众定向和程序化购买 媒体概念的多样化：数字媒体的交互功能越来越丰富多彩，和线下媒体已经有了本质差别。不同在线媒体在转化的链条上位置不一样，有些更接近最后的决策，有些是为了吸引潜在用户。 数据驱动的投放决策：在线广告是目前最成熟的大数据引用。在线广告投放流程是：广泛收取用户的行为数据和广告反馈数据，利用云计算基础设施给用户打上合适的标签，同样根据数据在多个广告竞争同一次展示是作出决策，再将投放结果的统计数据反馈广告操作人员以调整投放策略。 计算广告的核心问题计算广告的核心问题，是为一系列用户和环境的组合找到最适合的广告投放策略以优化整体广告活动的利润。上述问题的公式化 max E (ri - qi) 其中 i 表示从第 1 次到第 T 次之间的一次广告展示。优化目标是在这 T 次展示的总收入(r)与总成本(q)的差。]]></content>
      <categories>
        <category>计算广告</category>
      </categories>
      <tags>
        <tag>计算广告</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang 基础学习笔记（-）———— 基本语法]]></title>
    <url>%2F2018%2F03%2F01%2F%E6%8A%80%E6%9C%AF%2Fgolang%2F2018-03-01-golang%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[Go 基础语法Go 程序的基本构成Hello world1234567package mainimport "fmt"func main() &#123; fmt.Println("hello, world")&#125; Go 程序的基本构成，每个程序都有自己所在的 package, 包这个概念类似于其他编程语言的命名空间 或者 库名, 一般用一个简单的单词，与其他功能区分开即可, 命名规范要求全小写，或者&#39;_&#39;连接的多个单词 Go 程序是通过 import 关键字将一组包链接在一起。import &quot;fmt&quot; 表示该程序需要使用 package fmt（函数或者其他元素），其中 “fmt” 是一个包名 格式是 import “package_name” import( “package_name”) functionGo 程序的重要组成部分是函数，格式是 func 函数名(参数列表) (返回值列表) {} main.mainGo 程序的入口是 main.main 方法, 第一个main是该程序的 package; 第二个 main 是该程序的 main 函数，main 函数要求没有参数列表和返回值列表。 基础数据类型int包含int8, int16, int32, int64，分别8bit, 16bit, 32bit, 64bit的有符号整数，而int类型占的bit数决定于编译环境所在平台的位数，如果是32位环境则是32bit，如果是64位环境则是64bit。除了有符号数之外还要无符号数，Go 语言中，无符号型整数有 uint8, uint16, uint32, unit64， int8 的取值范围是 [-128127], 而uint8的取值范围是 [0255], 其他有符号型和无符号型整数的取值范围和这个类型类似。 bytebyte is an alias for uint8 and is equivalent to uint8 in all ways. It is used, by convention, to distinguish byte values from 8-bit unsigned integer values.byte 是 uint8 的别名（alias）， 或者说完全等同于 uint8。按照惯例用于区分8bit无符号数和byte 类型数据 runerune is an alias for int32 and is equivalent to int32 in all ways. It is used, by convention, to distinguish character values from integer values. rune 是 `int32` 类型的别名, 或者说完全等同于 `int32`. 按照惯例用于区分字符类型和整数类型rune 用数字表示字符的 ASCII编码 float浮点类数据类型包含float32和float64, Go 语言基于 IEEE 754标准实现的浮点类型数据，具体参见另一篇文章浮点数 bool布尔类型数据，取值有 true 和 false complex复数，有对应float32 和 float64 两个精度的数据类型 complex64 和 complex128,complex(1, 2)`` 等同于1 + 2i` string字符串类型，底层实现是byte 类型的 slice，不可变（immutable） iota1const iota = 0 iota 是无类型的int 类型的常量计数器，在const关键字出现时配置为0，从0开始，const 中每增加一行iota 自增一次（+1） iota 的使用示例如下, 在 const 关键字修饰的括号中， iota 首先被重置成0， 然后 a = 0, b = 1, c = 2 12345const ( a = iota // 0 b // 1 c // 2) 从上面的特性来看，我们可以使用 iota 来定义一个简单的枚举，来个小例子 12345678type chargeType intconst ( CPC chargeType = iota // 0 CPM // 1 CPT // 2 CPD // 3) 设想你在处理消费者的音频输出。音频可能无论什么都没有任何输出，或者它可能是单声道，立体声，或是环绕立体声的。这可能有些潜在的逻辑定义没有任何输出为 0，单声道为 1，立体声为 2，值是由通道的数量提供。所以你给 Dolby 5.1 环绕立体声什么值。一方面，它有6个通道输出，但是另一方面，仅仅 5 个通道是全带宽通道（因此 5.1 称号 - 其中 .1 表示的是低频效果通道）。不管怎样，我们不想简单的增加到 3。我们可以使用下划线跳过不想要的值。 12345678910type AudioOutput intconst ( none AudioOutput = iota // 0 mone // 1 stereo // 2 _ _ Dolby // 5) 控制结构if-else基础用法 1234567891011121314151617if condition &#123; // do something&#125;if condition &#123; // do something&#125; else &#123; // do something&#125;if condition1 &#123; // do something&#125; else if condition2 &#123; // do something&#125; else &#123; // do something&#125; 关键字 if 和 else 之后的左大括号 { 必须和关键字在同一行，如果你使用了 else-if 结构，则前段代码块的右大括号 } 必须和 else-if 关键字在同一行。这两条规则都是被编译器强制规定的。 除了上述情况之外还有如下使用方法 123if initialization; condition &#123; // do something&#125; Example: 123if i := getValue(); i &gt; 10 &#123; // do something&#125; for 常见用法 123for i := 0; i &lt; count; i++ &#123; //do something&#125; go 中没有while 循环，用for 循环代替 123for i &lt; 100 &#123; //do something&#125; 无限循环，没有 condition 1for &#123;&#125; 或 1for index := 0; ; index ++ &#123;&#125; for range 123for idx, element := range intArr &#123; // idx 是数组下表，element 是数组下标所在元素 //do something&#125; 或 123for idx := range intArr &#123; // idx 是数组下表 //do something&#125; 注意：for a := range arr 这种遍历的时候 a 是数组下标 或 123for key, value := range map1 &#123; // Key map 中的key value 是map 中的值 //do something&#125; break 和 continue 12345for idx := 0 ;; idx ++ &#123; if idx &gt; 10 &#123; break; // 退出当前for 循环 &#125;&#125; 12345for idx := 0 ;; idx ++ &#123; if (idx % 2) == 0 &#123; continue; // 进入下一个循环 &#125;&#125; switch case相比其他语言来说go 语言的 switch case 更强大，它支持各种形式的表达式，而且匹配上一个 case 执行完分支代码后，程序会自动switch代码块，不需要使用 break 标明结束 12345678switch var1 &#123;case val1: // do somethingcase val2: // do somethingdefault: // 不符合之前所有已给出条件的时候走到这里，建议写到最后 // do something&#125; 上面例子中var1 可以是任何类型， 而 val1 和 val2 是同类型的任意值 多个case 合并 12345678switch i &#123;case 1, 2: //do somethingcase 3: //do somethingdefault: //do something&#125; or 123456789switch i &#123;case 1:case 2: //do somethingcase 3: //do somethingdefault: //do something&#125; fallthrough 当执行完一个 case 分支后还想继续执行下一个 case 分支，可以使用关键字 fallthrough example： 12345678910111213141516func main() &#123; var i = 2 switch i &#123; case 0: case 1: fmt.Printf("Value 1 or 0, val:%v \n", i) case 2: fmt.Printf("Value 2, val:%v \n", i) fallthrough case 3: fmt.Printf("Value 3, val:%v \n", i) default: fmt.Printf("Value default, val:%v \n", i) &#125;&#125;` 上面程序的输出结果是： Value 2, val:2 Value 3, val:2 Process finished with exit code 0array 数组数组是固定长度的同一类型元素组成的序列，可以通过数组下标访问来访问元素，下标从0开始 数组的长度是数组类型的一个组成部分，因此[3]int和[4]int是两种不同的数组类型，数组的长度必须是常量表达式，因为数组的长度需要在编译阶段确定。 golang 提供内建函数 append(arr []Type, element... Type) 用于往数组中添加元素，也可用于 slice数组相关的内建函数 还有 len() 和 cap(), 对于数组而言 len() 和 cap() 是一样的 len() 函数的参数也可以是 nil, len(nil) == 0 数组的声明格式是 1var arr [length]Type 刚声明的数组长度是 length，每个元素都是零值（数字是0， 字符串是’’，引用类型是nil）也可以使用 make 定义一个数组 1var arr = make([]Type, len) 还可以在定义的是时候设定初始值 1var arr = [3]int64&#123;1,2,3&#125; 或者设定指定下标的元素 1var arr = [3]int64&#123;2:1000&#125; // arr := []int64&#123;0, 0, 1000&#125; 如果数组元素本身可以比较的，那么数组也是可以用 == 比较的，反则会编译失败 slice（切片）slice(切片) 是对数组的一个连续片段的使用 切片也是可索引的，也可以使用 len() 函数的来获取长度切片除了长度之外，还有一个属性是容量， 通过内建函数 cap() 函数来获取容量，容量的含义是切片开始位置到底层数组结束位置的数组长度例如切片 s 是数组 a 的一部分，s = a[3:], 那么 cap[s] = len(a) - 3对于切片来说，始终需要保证 0 &lt;= len(s) &lt;= cap(s)， 如果 len(s) &gt; cap(s) 就会出现越界异常 从上面这段文字总结出 slice 的组成元素是： 指针（指向底层数组中切片的第一个元素） 长度 容量 下图描述了切片的构成 slice 的初始化方式是 12345678var slice1 [...]Type // 不需要指明长度，编译器会自动构建一个长度合适的底层数组var slice2 []Type // _..._可以不写var arr [length]Type // 声明一个数组slice3 := arr[start:end] // 声明一个由数组中 [start, end) 元素组成的切片slice4 := arr[start:] // end 可以省略不写，表示 end = len(arr)声明一个由数组中 [start, len(arr)) 元素组成的切片slice5 := arr[:end] // start 可以省略不写，表示 start = 0 声明一个由数组中 [0, end) 元素组成的切片slice6 := make([]Type, len, cap) // 声明一个具体类型，长度是len，容量是cap 的切片， make 关键字适用于 array, slice, map, channel 的内存分配slice7 := new([]Type) // 也可以用 new 关键词定义一个 len == cap == 0 的slice，但是返回一个指向类型为 T，值为 0 的地址的指针，它适用于值类型如数组和结构体；它相当于 &amp;T&#123;&#125;。强烈建议不要使用 new 关键字声明 slice 由于slice 是共享的底层数组，当一个slice 改变了底层数组的时候，和它共享底层数组的其他slice 也会受影响 1234567891011121314func main() &#123; var arr [6]int64 for idx := range arr &#123; arr[idx] = int64(idx + 1) &#125; fmt.Printf("The underlying array:%v \n", arr) s := arr[3:5] fmt.Printf("The slice:%v, length:%v, capacity:%v \n", s, len(s), cap(s)) s[0] = 1000 fmt.Printf("The underlying array after set value for slice :%v \n", arr) fmt.Printf("The slice after set value:%v \n", s)&#125; 输出结果如下： The underlying array:[1 2 3 4 5 6 7 8 9 10] The slice 1:[4 5 6 7 8 9 10], length:7, capacity:7 The slice 2:[5 6 7 8 9 10], length:6, capacity:6 The underlying array after set value for slice :[1 2 3 4 5 1000 7 8 9 10] The slice 1 after set value:[4 5 1000 7 8 9 10] The slice 2 after set value:[5 1000 7 8 9 10]如果不想共享底层数据，可以使用内建的 copy(resourceSlice, targetSlice)函数 从原数组或者切片中拷贝一个新的切片，然后对新的切片的操作就不会因为共享底层数组影响其他切片了 1234567891011121314// 第一个集合中排除掉第二个集合后的结果func Subtract(c1 []interface&#123;&#125;, c2 []interface&#123;&#125;) (subtraction []interface&#123;&#125;, err error) &#123; subtraction = make([]interface&#123;&#125;, len(c1)) copy(subtraction, c1) // copy c1 to subtraction subtraction = c1[:] for _, elem2 := range c2 &#123; for idx, elem1 := range subtraction &#123; if elem2 == elem1 &#123; subtraction = append(subtraction[:idx], subtraction[idx+1:]...) &#125; &#125; &#125; return&#125; 在从数组或切片中生成新的切片的时候还可以指定新切片的容量语法是 1newSlice := arr[start:end:cap] 12345678910111213141516func testSliceCap() &#123; a := []int32&#123;1, 2, 3, 4&#125; s1 := a[2:] fmt.Printf("a: %v, s1: %v\n", a, s1) s2 := a[0:2:2] fmt.Printf("a: %v, s2: %v\n", a, s2) s2 = append(s2, 100) fmt.Printf("a: %v, s1: %v, s2: %v\n", a, s1, s2) s3 := a[:2:2] s3 = append(s3, 101) fmt.Printf("a: %v, s1: %v, s2: %v, s3:%v\n", a, s1, s2, s3)&#125; 输出结果 a: [1 2 3 4], s1: [3 4] a: [1 2 3 4], s2: [1 2] a: [1 2 3 4], s1: [3 4], s2: [1 2 100] a: [1 2 3 4], s1: [3 4], s2: [1 2 100], s3:[1 2 101]从上面的例子可以看到，限定了切片的容量之后，从同一数据产生的切片的 slice 的基本操作和数组一致slice 不可以用 == 比较的， bytes 提供了 []byte 的比较方法, strings 提供了 []string 的比较方法 map一个无序的K/V集合，其中所有的key 都是唯一的，在 go 中map 中的 key 必须是可比较的（支持 == 比较运算符）的数据类型, 所以 key 不能是 map、slice 或者 func, 而对于value 值没有任何要求 map 的定义和初始化 12var mapDemo map[Key]Valuem := make(map[Key]Value, len) map 的 key 是完全无序的， 使用 for 循环遍历map 的时候每次的顺序都是随机的 判断 map m1 中是否包含 k1 12345var m1 = make(map[Key][Value])val1, contains := m1[k1]if contains &#123; fmt.Println("the map m1 contains k1!")&#125; 注意：对于 slice 可以直接往为 nil 的 slice 中存放数据，但是map 必须先使用 make 分配内存 结构体(struct)123456789101112131415// 定义一个结构体type Student struct &#123; Name string id int64&#125;// 定义结构体的 String 函数func (s Student) String() string &#123; return fmt.Sprintf("name: %v, Id: %v", s.Name, s.id)&#125;// 实例化一个结构体var s Student// 访问结构体的数据s.Name = "test" 结构体还可以组合, 下面我们尝试定义两个结构体，Circle 和 Rectangle 1234567type Circle struct &#123; x, y, r int64&#125;type Rectangle struct &#123; x, y, len, width int64 // x,y 是左下角的点&#125; 可以看到 Rectangle 和 Circle 都具有点的属性，可以提取出一个结构体 Point下面的代码中我们定义了三个结构体，分别是 Point、Circle、Rectangle， 其中Circle 和 Rectangle 中都组合了一个 Point, 这是访问属性的时候就需要先访问被组合的结构体 12345678910111213141516171819202122232425262728type Point struct &#123; x, y int64&#125;type Circle struct &#123; c Point // 原点 r int64 // 半径&#125;// 矩形type Rectangle struct &#123; p Point // 左下角的点 len, width int64 // 长和宽&#125;func main() &#123; c1 := Circle&#123;c: Point&#123;x: 0, y: 0&#125;, r: 2&#125; fmt.Println(c1) c2 := Circle&#123;&#125; c2.c.x = 0 c2.c.y = 0 c2.r = 3 fmt.Println(c2) r1 := Rectangle&#123;p: Point&#123;x:0, y:0&#125;, len:4, width:3&#125; fmt.Println(r1)&#125; go还支持在结构体中只声明属性的数据类型，而不只限定属性的名称, 这就是匿名成员，从下面的 代码中也能看到，匿名成员也不是没有名字，而是直接把数据类型作为了成员的名字 1234567891011121314151617181920212223242526272829type Point struct &#123; x int64 y int64&#125;type Circle struct &#123; Point // 原点 r int64 // 半径&#125;// 矩形type Rectangle struct &#123; Point // 左下角的点 len, width int64 // 长和宽&#125;func main() &#123; c1 := Circle&#123;Point: Point&#123;x: 0, y: 0&#125;, r: 2&#125; fmt.Println(c1) c2 := Circle&#123;&#125; c2.Point.x = 0 c2.Point.y = 0 c2.r = 3 fmt.Println(c2) r1 := Rectangle&#123;Point: Point&#123;x: 0, y: 0&#125;, len: 4, width: 3&#125; fmt.Println(r1)&#125; 结构体是多个类型数据聚合的数据类型， 可以包含任何类型的属性，结构体的访问权限通过大小写来控制，只有大写开头的属性和结构体具有包外可见性，使用json序列化和反序列化数组的时候，小写开头的属性会被忽略。 结构体的比较取决于结构体的属性，当结构体的所有属性都是可比较的时候，结构体就是可比较的，当使用 == 比较两个结构体的示例的时候，这就是。针对包含不可比较的结构体，也可以使用反射的 reflect.DeepEqual() 函数函数声明包括函数名、形式参数列表、返回值列表（可省略）以及函数体。 123func funcName(param1 Type, param2 Type) (result1 Type, result2 Type) &#123; // method body&#125; 从上面的例子中，我们可以看到go 函数的一个显著特点：多返回值，有时候一个方法的结果除了正常的结果外，还有不可预期的异常，这个时候就可以返回多个返回值，比如 encode/json 中的序列化的方法，正常情况下返回序列化的结果，而如果输入的参数无法序列化的时候则返回nil 和异常 12345678func Marshal(v interface&#123;&#125;) ([]byte, error) &#123; e := &amp;encodeState&#123;&#125; err := e.marshal(v, encOpts&#123;escapeHTML: true&#125;) if err != nil &#123; return nil, err &#125; return e.Bytes(), nil&#125; 如果我们在返回值列表中写明变量名，就可以在return 语句中省略操作数， 上面的方法就可以改写成下面这种 123456789func Marshal(v interface&#123;&#125;) (res []byte, err error) &#123; e := &amp;encodeState&#123;&#125; err = e.marshal(v, encOpts&#123;escapeHTML: true&#125;) if err != nil &#123; return // 等同于 return nil, err &#125; res = e.Bytes() return // 等同于 return e.Bytes(), nil&#125; method is first-class value在 go 语言中函数也是 first-class value， first-class value 的定义如下： 可以作为变量或者数据结构存储 可以作为参数传递给方法/函数 可以作为返回值从函数/方法返回 可以在运行期创建 有固有身份；“固有身份”是指实体有内部表示，而不是根据名字来识别，比如匿名函数，还可以通过赋值叫任何名字。大部分语言的基本类型的数值(int, float)等都是第一类对象；但是数组不一定，比如C中的数组，作为函数参数时，传递的是第一个元素的地址，同时还丢失了数组长度信息。对于大多数的动态语言，函数/方法都是第一类对象，比如Python，但是Ruby不是，因为不能返回一个方法。第一类函数对函数式编程语言来说是必须的。 也就是说在 go语言中函数拥有类型，可以被赋值，可以作为函数的参数或返回值，还可以有匿名函数 可变参数1234567891011121314func sum(args ...int) int &#123; total := 0 for _, a := range args &#123; total += a &#125; return total&#125;fmt.Println(sum()) // 0fmt.Println(sum(1)) // 1fmt.Println(sum(1,2,3,4)) // 10arr := []int&#123;1,2,3,4&#125;fmt.Println(sum(arr...)) // 10 defer有时候我们需要在函数执行结束之前释放资源（如数据库链接） 123456789101112131415161718192021222324func parseFromFile(fileName string) (res []string, err error) &#123; inputFile, err := os.Open(fileName) if err != nil &#123; fmt.Printf("An error occurred on opening the inputfile\n" + "Does the file exist?\n" + "Have you got acces to it?\n") return &#125; inputReader := bufio.NewReader(inputFile) for &#123; inputString, err := inputReader.ReadString('\n') fmt.Printf("The input was: %s", inputString) if err != nil &#123; inputFile.Close() return &#125; if (err = checkLine(inputString)); err != nil &#123; inputFile.Close() return &#125; res = append(res, inputString) &#125;&#125; 可以看到在每个 return 语句之前，我们都需要调用 inputFile.Close(), 这样做是在太麻烦，go 为我们提供了 refer 关键字 1234567891011121314151617181920212223func parseFromFile(fileName string) (res []string, err error) &#123; inputFile, err := os.Open(fileName) if err != nil &#123; fmt.Printf("An error occurred on opening the inputfile\n" + "Does the file exist?\n" + "Have you got acces to it?\n") return &#125; refer inputFile.Close() // 在函数执行到return 之前被调用 inputReader := bufio.NewReader(inputFile) for &#123; inputString, err := inputReader.ReadString('\n') fmt.Printf("The input was: %s", inputString) if err != nil &#123; return &#125; if (err = checkLine(inputString)); err != nil &#123; return &#125; res = append(res, inputString) &#125;&#125; refer 执行顺序123456789101112131415161718192021222324func func1() &#123; log.Printf("this is func 1\n")&#125;func func2() &#123; log.Printf("this is func 2\n")&#125;func func3() &#123; log.Printf("this is func 3\n")&#125;func func4() &#123; log.Printf("this is func 4\n")&#125;func main() &#123; log.Printf("begin\n") defer func1() defer func2() defer func3() defer func4() log.Printf("finished\n")&#125; 上面程序的数据输出结果是： 2018/04/17 08:11:41 begin 2018/04/17 08:11:41 finished 2018/04/17 08:11:41 this is func 4 2018/04/17 08:11:41 this is func 3 2018/04/17 08:11:41 this is func 2 2018/04/17 08:11:41 this is func 1可以看出来：defer 的执行顺序和定义顺序正好是相反的 循环中的 defer1234567891011// before process filefor _, filename := range filenames &#123; f, err := os.Open(filename) if err != nil &#123; return err&#125;defer f.Close() // NOTE: risky; could run out of file// ...process f&#125;// after process filereturn 在上面的程序中我们遍历文件名称数组，一个个的处理文件，并在处理完之后希望去关闭文件； 但是 defer 是在函数执行完最后一步才触发，如果文件比较多可能会耗光系统的文件描述符 这里的解决方案是把 for 循环的循环体抽取成函数 12345678910111213for _, filename := range filenames &#123; if err := doFile(filename); err != nil &#123; return err &#125;&#125;func doFile(filename string) error &#123; f, err := os.Open(filename) if err != nil &#123; return err &#125; defer f.Close() // ...process f…&#125; init 函数123func init() &#123; // init something&#125; 方法前面讲结构体的时候, 讲过可以给结构体定义方法, 事实上除了结构体我们还可以们可以给数值、字符串、map、数组定义一些自定义行为 1234567891011121314151617181920//自定义int 数组type array []int//给数组定义 sum 方法func (a array) sum() int &#123; total := 0 for _, i := range a &#123; total += i &#125; return total&#125;func main() &#123; var arr1 = array&#123;1, 2, 3, 4&#125; arrSum := arr1.sum() fmt.Println(arrSum) // 输出 10 arrSum2 := array.sum(arr1) // 常规调动函数的方法 fmt.Println(arrSum2) // 输出 10&#125; 接口接口类型具体描述了一系列方法的集合，一个实现了这些方法的具体类型是这个接口类型的实例。 内嵌结构体的函数 垃圾回收 协程池 并发 etcd]]></content>
      <categories>
        <category>golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次线上端口排查过程]]></title>
    <url>%2F2018%2F01%2F18%2F%E6%8A%80%E6%9C%AF%2Fmac%26linux%2Fnetstat%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[前言今天在查看线上应用的时候发现一个服务启动之后报错断开已经被占用，但是tomcat 容器的日志中只说了端口已经被占用，但是没说是哪个端口被占用了，我们只能看看这个服务在其他机器上占用了什么接口，这时候我们就想到 netstat 指令 netstat 查看端口占用情况常用 netstat 指令1234567$netstat -tlp | grep 99573tcp 0 0 0.0.0.0:24013 0.0.0.0:* LISTEN 99573/java tcp 0 0 0.0.0.0:17134 0.0.0.0:* LISTEN 99573/java tcp 0 0 0.0.0.0:sieve-filter 0.0.0.0:* LISTEN 99573/java tcp 0 0 0.0.0.0:18134 0.0.0.0:* LISTEN 99573/java tcp 0 0 localhost:19134 0.0.0.0:* LISTEN 99573/java tcp 0 0 0.0.0.0:commplex-main 0.0.0.0:* LISTEN 99573/java sieve-filter 这个是什么？commplex-main 又是什么？我们怎么才能看到这两个端口的具体数值呢？ netstat 指令常用参数netstat: 查看网络端口使用情况 -a: 打印索引的套接字连接 -t: 只打印TCP 连接信息 -u: 只打印UDP 连接信息 -p: 打印进程名称及进程ID -l: 打印所有在监听的服务端口 -n: 用数字的形式打印端口信息，不对端口进行解析到这里我们就知道如何去打印具体的端口信息，去解决上面遇到的问题了 1234567$netstat -tnlp | grep 99573tcp 0 0 0.0.0.0:24013 0.0.0.0:* LISTEN 99573/java tcp 0 0 0.0.0.0:17134 0.0.0.0:* LISTEN 99573/java tcp 0 0 0.0.0.0:2000 0.0.0.0:* LISTEN 99573/java tcp 0 0 0.0.0.0:18134 0.0.0.0:* LISTEN 99573/java tcp 0 0 localhost:19134 0.0.0.0:* LISTEN 99573/java tcp 0 0 0.0.0.0:5000 0.0.0.0:* LISTEN 99573/java 数据列说明 Proto: 通信协议，tcp, udp, raw 等 Rece-Q: 接收队列中上午读取的数据，单位字节，一般是 0 Send-Q: 发送到远端尚未收到 ACK 的数据，单位字节，一般是0 Local Address: 通信连接中的本机地址及端口，一般情况下是 FQDN（全限定域名） 格式显示的地址，端口的显示格式默认是端口对应的服务名称，只有指定 --numersic 的时候才会按照数字形式打印 Foreign Address: 套接字的远端信息，地址加端口 State: 套接字连接状态这里我们可以穿插讲解一下套接字连接状态 6.1 ESTABLISHED 传输中断状态6.2 SYN_SENT 客户端发起连接发送连接请求后的状态6.3 SYN_RECV 套接字服务端接收到连接请求后的状态6.4 FIN_WAIT1 客户端发起断开连接请求后的状态6.5 FIN_WAIT2 断开连接请求方接收到对方回复之后，等待对方发起断开连接之后的状态6.6 TIME_WAIT 客户端接收到服务端的断开连接的请求之后，向服务端发送ACK 信息后，进入此状态，是为了防止网络故障，进入 TIME_WAIT 后，如果服务端一直没有收到 ACK，会重新发送断开连接的请求6.7 CLOSED 连接关闭的状态6.8 CLOSED_WAIT 服务端接收到断开连接请求之后，发送ACK 之后6.9 LAST_ACK 服务端确认数据发送完毕后，发送断开连接请求之后，进入此状态6.10 LISTEN 暴露接口等待连接6.11 CLOSING 连接双方同时发起断开连接的时候，出现的一个短暂的状态6.12 UNKNOWN 未知的状态 netstat 指令参数说明12345678910111213141516171819202122232425262728293031323334353637$netstat -husage: netstat [-vWeenNcCF] [&lt;Af&gt;] -r netstat &#123;-V|--version|-h|--help&#125; netstat [-vWnNcaeol] [&lt;Socket&gt; ...] netstat &#123; [-vWeenNac] -I[&lt;Iface&gt;] | [-veenNac] -i | [-cnNe] -M | -s [-6tuw] &#125; [delay] -r, --route display routing table -I, --interfaces=&lt;Iface&gt; display interface table for &lt;Iface&gt; -i, --interfaces display interface table -g, --groups display multicast group memberships -s, --statistics display networking statistics (like SNMP) -M, --masquerade display masqueraded connections -v, --verbose be verbose -W, --wide don't truncate IP addresses -n, --numeric don't resolve names --numeric-hosts don't resolve host names --numeric-ports don't resolve port names --numeric-users don't resolve user names -N, --symbolic resolve hardware names -e, --extend display other/more information -p, --programs display PID/Program name for sockets -o, --timers display timers -c, --continuous continuous listing -l, --listening display listening server sockets -a, --all display all sockets (default: connected) -F, --fib display Forwarding Information Base (default) -C, --cache display routing cache instead of FIB -Z, --context display SELinux security context for sockets &lt;Socket&gt;=&#123;-t|--tcp&#125; &#123;-u|--udp&#125; &#123;-U|--udplite&#125; &#123;-w|--raw&#125; &#123;-x|--unix&#125; --ax25 --ipx --netrom &lt;AF&gt;=Use '-6|-4' or '-A &lt;af&gt;' or '--&lt;af&gt;'; default: inet List of possible address families (which support routing): inet (DARPA Internet) inet6 (IPv6) ax25 (AMPR AX.25) netrom (AMPR NET/ROM) ipx (Novell IPX) ddp (Appletalk DDP) x25 (CCITT X.25) 端口占用具体结论 端口 使用协议 服务 说明 5000 TCP/UDP commplex-main 2000 TCP/UDP sieve-filter 结论： 这里使用 commplex-main 和 sieve-filter 端口的是 crashub]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux指令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang 环境变量和 golang 脚本工具]]></title>
    <url>%2F2018%2F01%2F10%2F%E6%8A%80%E6%9C%AF%2Fgolang%2Fgolang%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%92%8C%E8%84%9A%E6%9C%AC%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[golang 环境变量想要了解 go 的环境变量，我们可以通过 go help environment 指令来查看详细介绍，这里尝试翻译这一些详细介绍，并给出一些个人的认识，如有错漏，欢迎指正 通用环境变量(General-purpose environment variables)GCCGOgccgo 属于 gcc 编译器集合，是 gcc 针对go 语言的前端实现；gccgo 的编译速度比gc较慢一点，但是可以生成更优的代码，因此程序执行速度会更快。golang 的默认编译器是 gc, gc 编译器已支持主流的处理器，而 gccgo 也对 gc 不支持的处理器进行了支持测试；通过Go正式版本安装的go命令已经可以支持 gccgo，需要使用 -compiler选项：go build -compiler=gccgo 。对于用户，如果需要更好编译优化，或者是使用 gc 所不支持的处理器或操作系统，gccgo 可能是一个更好的选择。 GOARCH编译源代码的机器的处理器架构，它的值可以是 386、amd64 或 arm。 GOBINgo install 编译出来的可执行文件的存放位置，GOBIN 的默认值是GOPATH/bin如果 GOBIN 设置了值，编译出来的可执行脚本将放置到到 GOBIN 设置的文件夹，而不是 .go 文件所在的src 文件夹同级的 bin 文件夹 GOOS编译源代码的机器的操作系统, 它的值可以是 linux, darwin, windows, netbsd。 GOPATHGOPATH 列举了机器上所有go代码所在位置，在Unix系统中，该值是以冒号分隔的字符串。在Unix 系统中GOPATH 默认值是 %HOME/go， windows 系统中默认值是 %USERPROFILE%\goGOPATH 中文件夹结构如下: GOPATH=/home/user/go /home/user/go/ src/ foo/ bar/ (go code in package bar) x.go quux/ (go code in package main) y.go bin/ quux (installed command) pkg/ linux_amd64/ foo/ bar.a (installed package object)src: 存放 go 源文件bin: package main 中的go 文件编译之后产生的可执行文件存放位置pkg: 非 package main 中的go 文件编译之后产生的静态库文件(*.a)存放位置 设计开发中，需要将所有存放go 代码的位置都添加到 GOPATH 中去 1export GOPATH=: GORACE竞争监测相关参数，详见 https://golang.org/doc/articles/race_detector.html.待补充 GOROOTgo 安装目录 GOTMPDIRThe directory where the go command will writetemporary source files, packages, and binaries. GOCACHE存放go 编译系统编译过程中产生的缓存文件，如果这个文件过大，可以执行 go clean --cache 去清理这个文件夹 cgo 相关环境变量CCcgo 编译 c语言代码时候使用的编译器, 需要用户额外安装 CGO_ENABLED是否支持 cgo， 取值是 0 或者 1 CGO_CFLAGScgo 编译 c 代码时传递的参数 CGO_CFLAGS_ALLOW出于安全考虑，cgo 编译 c 代码时只能允许有限的参数， CGO_CFLAGS_ALLOW 的取值是一个 正则表达式，涵盖所有允许的参数名称 CGO_CFLAGS_DISALLOW和 CGO_CFLAGS_ALLOW 相反，CGO_CFLAGS_ALLOW 的取值也是一个 正则表达式，涵盖所有不允许的参数名称 CGO_CPPFLAGS, CGO_CPPFLAGS_ALLOW, CGO_CPPFLAGS_DISALLOW类似于 CGO_CFLAGS, CGO_CFLAGS_ALLOW, CGO_CFLAGS_DISALLOW, 不过是用于 c 预处理器 CGO_CXXFLAGS, CGO_CXXFLAGS_ALLOW, CGO_CXXFLAGS_DISALLOW类似于 CGO_CFLAGS, CGO_CFLAGS_ALLOW, CGO_CFLAGS_DISALLOW, 不过是用于 c++ 编译器 CGO_FFLAGS, CGO_FFLAGS_ALLOW, CGO_FFLAGS_DISALLOW类似于 CGO_CFLAGS, CGO_CFLAGS_ALLOW, CGO_CFLAGS_DISALLOW, 不过是用于 Fortran 编译器 CGO_LDFLAGS, CGO_LDFLAGS_ALLOW, CGO_LDFLAGS_DISALLOW类似于 CGO_CFLAGS, CGO_CFLAGS_ALLOW, CGO_CFLAGS_DISALLOW, 不过是用于 c 链接器 CXXcgo 编译 C++的编译器 PKG_CONFIG取值是 指向 pkg_config 工具的绝对路径 架构相关的特殊目的的环境变量GOARM当 GOARCH=arm 时，arm 架构的处理器，它的取值是 5,6,7 GO386当 GOARCH=386 时，浮点指令集，它的取值是 387, sse2 GOMIPS当 GOARCH=mips{,le} 时，指定是软浮点还是硬浮点 特殊目的的环境变量GOROOT_FINAL当go 根目录和go 安装目录不一致时，将 GOROOT_FINAL 设置为go 的根目录 GIT_ALLOW_PROTOCOLgo get 指令使用 git fetch/clone 获取 go 代码的时候允许使用的 schema, 多个 schema 用冒号分割；如果 GIT_ALLOW_PROTOCOL 不包含某个 schema， go get 认为它是不安全的 GO_EXTLINK_ENABLEDWhether the linker should use external linking modewhen using -linkmode=auto with code that uses cgo.Set to 0 to disable external linking mode, 1 to enable it. go 提供的脚本工具gofmt格式化 go 语言代码 使用示例: gofmt [flags] [path ...] -cpuprofile string write cpu profile to this file -d 在控制台输出格式化后的代码和源代码的对比 -e report all errors (not just the first 10 on different lines) -l list files whose formatting differs from gofmt&apos;s -r string 重写规则，如 &apos;a[b:len(a)] -&gt; a[b:]&apos; 将 `a[b:len(a)]` 替换成 `a[b:len(a)]` -s 简化源代码 -w 将格式化结果写回源文件，而不是输出到控制台go installgo getgo build交叉编译 (cross compile)Golang 支持交叉编译，在一个平台上生成另一个平台的可执行程序，这里备忘一下。 Mac 下编译 Linux 和 Windows 64位可执行程序 CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build main.go CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build main.goLinux 下编译 Mac 和 Windows 64位可执行程序 CGO_ENABLED=0 GOOS=darwin GOARCH=amd64 go build main.go CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build main.goWindows 下编译 Mac 和 Linux 64位可执行程序 SET CGO_ENABLED=0 SET GOOS=darwin SET GOARCH=amd64 go build main.go SET CGO_ENABLED=0 SET GOOS=linux SET GOARCH=amd64 go build main.goGOOS：目标平台的操作系统（darwin、freebsd、linux、windows）GOARCH：目标平台的体系架构（386、amd64、arm）交叉编译不支持 CGO 所以要禁用它 gdb]]></content>
      <categories>
        <category>golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka 中的偏移量]]></title>
    <url>%2F2017%2F12%2F10%2F%E6%8A%80%E6%9C%AF%2Fkafka%2F2017-12-10-kafka%E5%A6%82%E4%BD%95%E6%8A%8A%E5%AE%9E%E7%8E%B0consumer%E6%8E%A7%E5%88%B6offset%2F</url>
    <content type="text"><![CDATA[kafka 中的偏移量首先来了解一下Kafka所使用的基本术语：Topic: Kafka将消息种子(Feed)分门别类，每一类的消息称之为一个主题(Topic). Producer: 发布消息的对象称之为主题生产者(Kafka topic producer) Consumer: 订阅消息并处理发布的消息的种子的对象称之为主题消费者(consumers) Broker: 已发布的消息保存在一组服务器中，称之为Kafka集群。集群中的每一个服务器都是一个代理(Broker). 消费者可以订阅一个或多个主题（topic），并从Broker拉数据，从而消费这些已发布的消息。 话题和日志 (Topic和Log)让我们更深入的了解Kafka中的Topic。 Topic是发布的消息的类别或者种子Feed名。对于每一个Topic，Kafka集群维护这一个分区的log，就像下图中的示例： 每一个分区都是一个顺序的、不可变的消息队列， 并且可以持续的添加。分区中的消息都被分了一个序列号，称之为偏移量(offset)，在每个分区中此偏移量都是唯一的。 Kafka集群保持所有的消息，直到它们过期， 无论消息是否被消费了。 实际上消费者所持有的仅有的元数据就是这个偏移量，也就是消费者在这个log中的位置。 这个偏移量由消费者控制：正常情况当消费者消费消息的时候，偏移量也线性的的增加。但是实际偏移量由消费者控制，消费者可以将偏移量重置为更老的一个偏移量，重新读取消息。 可以看到这种设计对消费者来说操作自如， 一个消费者的操作不会影响其它消费者对此log的处理。 消费者(Consumers)通常来讲，消息模型可以分为两种， 队列 和 发布-订阅 式。 队列的处理方式是 一组消费者从服务器读取消息，一条消息只有其中的一个消费者来处理。在发布-订阅模型中，消息被广播给所有的消费者，接收到消息的消费者都可以处理此消息。Kafka为这两种模型提供了单一的消费者抽象模型： 消费者组 （consumer group）。 消费者用一个消费者组名标记自己。 一个发布在Topic上消息被分发给此消费者组中的一个消费者。 假如所有的消费者都在一个组中，那么这就变成了queue模型。 假如所有的消费者都在不同的组中，那么就完全变成了发布-订阅模型。 更通用的， 我们可以创建一些消费者组作为逻辑上的订阅者。每个组包含数目不等的消费者， 一个组内多个消费者可以用来扩展性能和容错。正如下图所示：2个kafka集群托管4个分区（P0-P3），2个消费者组，消费组A有2个消费者实例，消费组B有4个。 正像传统的消息系统一样，Kafka保证消息的顺序不变。 再详细扯几句。传统的队列模型保持消息，并且保证它们的先后顺序不变。但是， 尽管服务器保证了消息的顺序，消息还是异步的发送给各个消费者，消费者收到消息的先后顺序不能保证了。这也意味着并行消费将不能保证消息的先后顺序。用过传统的消息系统的同学肯定清楚，消息的顺序处理很让人头痛。如果只让一个消费者处理消息，又违背了并行处理的初衷。 在这一点上Kafka做的更好，尽管并没有完全解决上述问题。 Kafka采用了一种分而治之的策略：分区。 因为Topic分区中消息只能由消费者组中的唯一一个消费者处理，所以消息肯定是按照先后顺序进行处理的。但是它也仅仅是保证Topic的一个分区顺序处理，不能保证跨分区的消息先后处理顺序。 所以，如果你想要顺序的处理Topic的所有消息，那就只提供一个分区。 偏移量(Offsets)的管理如上文所述，kafka为分区中的每条消息保存一个 偏移量（offset），这个偏移量是该分区中一条消息的唯一标示符。也表示消费者在分区的位置。例如，一个位置是5的消费者(说明已经消费了0到4的消息)，下一个接收消息的偏移量为5的消息。实际上有两个与消费者相关的“位置”概念： 消费者的位置给出了下一条记录的偏移量。它比消费者在该分区中看到的最大偏移量要大一个。 它在每次消费者在调用poll(long)中接收消息时自动增长。 “已提交”的位置是已安全保存的最后偏移量，如果进程失败或重新启动时，消费者将恢复到这个偏移量。消费者可以选择定期自动提交偏移量，也可以选择通过调用commit API来手动的控制(如：commitSync 和 commitAsync)。 消费者如何提交 偏移量(Offsets) 自动提交 这种方式只要在启动时配置属性 enable.auto.commit 为 true 即可， 示例代码如下： 12345678910111213141516171819private static void main (String[] args) &#123; Properties props = new Properties(); props.put("bootstrap.servers", "localhost:9092"); props.put("group.id", "test"); props.put("enable.auto.commit", "true"); props.put("auto.commit.interval.ms", "1000"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList("test")); System.out.println(consumer); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); &#125; &#125;&#125; 手动提交 在一些场景中，消费者 需要自行判定消息是否被消费了，如果没有判断为没有消费（ps:可能是消费失败了，需要重试），消费者 不会改变 offset；只有 消费者 判定消费成功是，才手动调用 commitSync() 或 commitAsync() 方法去提交 偏移量； 当然此时我们需要把 enable.auto.commit 置为 false。 下面给出个小例子： 12345678910111213141516171819202122Properties props = new Properties(); props.put("bootstrap.servers", "localhost:9092"); props.put("group.id", "test"); props.put("enable.auto.commit", "false"); // 主动提交置为false props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList("test")); System.out.println(consumer); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); // 逻辑处理 // ... &#125; if (success) &#123; // 如果判定消费成功，则手动提交offset到 broker consumer.commitSync(); &#125; &#125; 消费者如果控制偏移量在一些场景下，消费者需要控制自己要读取的偏移量，此时用户可以通过API手动设置开始读取的 偏移量 API 中提供以下方法： 指定到某个分区的具体 offsetorg.apache.kafka.clients.consumer.KafkaConsumer#seek(TopicPartition partition, long offset) 指定到某些分区的开始org.apache.kafka.clients.consumer.KafkaConsumer#seekToBeginning(Collection partitions) 指定到某些分区的结束，从上次结束的位置开始消费org.apache.kafka.clients.consumer.KafkaConsumer#seekToEnd(Collection partitions) 此时我们需要知道当前的 Topic 的偏移量信息，Kafka 为我们提供了很友好的工具 Get Offset Shell Get Offset Shellget offsets for a topic 1bin/kafka-run-class.sh kafka.tools.GetOffsetShell required argument [broker-list], [topic] Option Description –broker-list [hostname:port,….] REQUIRED: The list of hostname and [hostname:port] port of the server to connect to. –max-wait-ms [Integer: ms] The max amount of time each fetch request waits. (default: 1000) –offsets [Integer: count] number of offsets returned (default: 1) –partitions [partition ids] comma separated list of partition ids. If not specified, will find offsets for all partitions (default) –time [Long: timestamp in milliseconds] -1(latest) / -2 (earliest) timestamp; offsets will come before this timestamp, as in getOffsetsBefore –topic [topic] REQUIRED: The topic to get offsets from. 示例： 查询最近的offset ➜ kafka_2.12-1.0.0 bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 -topic test --time -1输出 test:0:52查询开始的的offset ➜ kafka_2.12-1.0.0 bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 -topic test --time -2输出 test:0:0偏移量的存储新版本的 Kafka 将偏移量信息存储在名为 __consumer_offsets 的topic 中,也支持将 偏移量 信息存储在 Zookeeper 中通过设置属性 offsets.storage 控制，offsets.storage 属性可选值有 kafka 和 zookeeper 消费者也可以不使用 Kafka 提供的偏移量存储方案，可自定义存储方式，详见官方文档]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浮点数学习笔记]]></title>
    <url>%2F2017%2F12%2F06%2F%E6%8A%80%E6%9C%AF%2Fgolang%2F2017-12-06-%E6%B5%AE%E7%82%B9%E6%95%B0%2F</url>
    <content type="text"><![CDATA[浮点数学习笔记前言最近在工作中遇到汇率换算的问题，需要将用户订单中的的本币换算成美元，用 golang 写了个脚本进行换算，当把汇率变量的类型定义为 float32 的时候，计算结果存在不能接受的误差查看了一下 golang 的官方文档，发现 golang 实现的浮点型数据是基于 IEEE 754标准，这一标准在存储小数的时候先天就 存在误差，下面会一一介绍。 float 精度问题先来个小例子，抛出问题 123456789import "fmt"func main() &#123; var a float32 = 64.35 var b = 64.35 // 默认类型是 float64 fmt.Printf("this is a float32 %f \n", a) fmt.Printf("this is a float64 %f \n", b)&#125; 程序执行结果： this is a float32 64.349998 this is a float64 64.350000可以看到, 使用 float32 类型的时候， 小数 64.35 存储的并不精准；不难想象，对 float32 进行逻辑计算的时候肯定会产生误差；那么使用 float64 类型数据的时候，计算结果就一定精准吗？答案显然是否定的，下面给将给出答案 为什么叫 浮点数为什么叫 浮点数， 浮点数 这个名词是相对 定点数来说的，从这两个名词中可以看出，这两个概念的差别就在于 点， 这里的 点 指的是小数中的 小数点； 大家都知道，计算机都是使用 二进制 的形式来存储和计算数据的，对于小数的处理也是如此； 存储小数的时候，计算机将小数分为 整数 和 小数 两个部分进行处理： 定点数 就是将小数点的位置固定，分别分配固定的位数用于存储 整数 和 小数 部分，例如，我们用 32bit 存储小数，第31位存储符号，2330位存储 整数，022 位存储小数，如下图， 0 0000 1111 0100 0000 0000 0000 0000 000 15.25 31bit符号位 23~30bit保存整数部分 0~22bit保存小数部分 十进制小数 上面这个例子中 定点数 将小数点固定在 22bit 和 23bit 之间；可以很明显的看出，这种存储方式受到位数的限制，能表示的数字范围很小，上例中小数的范围就是 -255.xxx ~ 255.xxx (ps：寡人太懒了，不想算[1/2 + 1/4 + … + 1/2-23])也正是这个原因，计算机放弃了这种方式，采用了 浮点数 的方式。 浮点数 从名称上来解释的就是，小数点的位置是浮动的；简单来说浮点数就是将一个数字用科学计数法表示，先将数字分为 基数、指数； 再将基数分为整数部分和小数部分，例如:12345 = 1.2345 x 104；当然这里是十进制，而计算机在存储浮点数的时候当然还是二进制 15.25 = 1111.01 = 1.11101 x 23 让我们用32bit 保存小数： 0 1000 0010 1110 1000 0000 0000 0000 000 15.25 31bit符号位 23~30bit保存指数部分 0~22bit保存小数部分 十进制小数 你能从这个二进制中看出小数点的位置吗？这里大家要注意： 浮点数不仅仅可以保存小数！整数也是可以的，但是用浮点数表示整数这种行为不鼓励，毕竟浮点数表示数字是不精确的 IEEE 754 存储 浮点数上面讲浮点数的时候也基本介绍了 IEEE 754 标准IEEE 754 标准依赖于 科学计数法，将一个数字用二进制科学计数法表示，将一个数字分为指数和基数，用一位表示符号位，再将基数分为整数（二进制的整数部分当然是1了）和小数部分；IEEE 754规定了四种表示浮点数值的方式：单精确度（32位）、双精确度（64位）、延伸单精确度（43比特以上，很少使用）与延伸双精确度（79比特以上，通常以80位实现）；如下： 表示方式 符号位（Sign,S） 指数部分(Exponent,E) 小数部分(Fraction,F) 单精确度（32位） 1bit 8bit 23bit 双精确度（64位） 1bit 11bit 52bit 一个单精确度（32位）浮点数的表达公式如下:ｘ＝(－1)S×(1.F)×2Ｅ－127 e＝Ｅ－127 Sign: 简写为S，符号位，很简单就是 1:负数 0:正数Exponent: 简写为E，指数部分，计算公式是：指数+接码偏移量(127)Fraction: 简写为F，小数部分 到这里大家就很自然的会想到一个问题 阶码偏移量为何用127? 这个问题也困扰我很久，看了维基百科等资料，最后在知乎上找到了一个比较靠谱的答案，引用如下： 主要是为了让表示的范围能够对称起来这个算一算就清楚了。当阶码E 为全0且尾数M 也为全0时，表示的真值x 为零，结合符号位S 为0或1，有正零和负零之分。当阶码E 为全1且尾数M为全0时，表示的真值x 为无穷大，结合符号位S 为0或1，也有+∞和-∞之分。这样在32位浮点数表示中，要除去E，用全0和全1(255)10表示零和无穷大的特殊情况，指数的偏移值不选128(10000000)，而127(01111111)。对于规格化浮点数，阶码E范围是1254。 分两种情况计算如下： 1）偏移值为127时，绝对值范围大致是：1.2*10^(-38)3.410^(+38)； 2）如果偏移值取为128时， 绝对值范围大致是：5.910^(-39)~1.7*10^(+38)； 可见偏移值取127时，上下范围基本对称，相对合理点。 作者：yuanyuany链接：https://www.zhihu.com/question/24784136/answer/144601879来源：知乎 如果是双精确度（64位） 十进制 和 IEEE 754浮点数 相互转化下面来个小例子， 我们将 64.35 转化成IEEE 754浮点数 现将 64.35 用转化成 二进制 先看整数部分 64 = 0100 0000 再来转化小数部分转二进制 0.35x2 = 0.7 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 // 取计算结果整数部分0.70x2 = 1.4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 // 取上一计算结果的小数部分乘以2 0.40x2 = 0.8 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 0.80x2 = 1.6 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 0.60x2 = 1.2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 0.20x2 = 0.4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 0.40x2 = 0.8 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 0.80x2 = 1.6 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 0.60x2 = 1.2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 0.40x2 = 0.8 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 // 到这里已经是循环重复了，这里永远不会算出1.0，所以用 IEEE 754 表示的时候是无限循环 小数部分： 0.35 = 01 0110 0110 ... 二进制结果 64.35 = 1000000.0101100110... = 1.0000 0001 0110 0110 0110… x 26 用 32bit IEEE 754 存储 64.35s = 0expr = 127 + 6 = 133 = 1000 0101 frag = 0000 0001 0110 0110 0110 011 IEEE 754 32位存储 64.35 的情况如下64.35 = 0 | 1000 0101 | 0000 0001 0110 0110 0110 011 用 64bit IEEE 754 存储 64.35s = 0expr = 1023 + 6 = 1029 = 1000 0000 0101 frag = 0000 0001 0110 0110 0110 0110 0110 0110 0110 0110 0110 0110 0110 IEEE 754 64位存储 64.35 的情况如下64.35 = 0 | 1000 0000 0101 | 0000 0001 0110 0110 0110 0110 0110 0110 0110 0110 0110 0110 0110 我们再将IEEE 754浮点数 还原成小数, 先看看32bit 的情况看看，算出指数 1000 0101 - 1111111 = 133 - 127 = 6然后这个浮点数的二进制表示就是 1.0000 0001 0110 0110 0110 011 x 2&lt;sup&gt;6&lt;/sup&gt;下面就是换算成十进制1.0000 0001 0110 0110 0110 011 x 26 = 100 0000.0101 1001 1001 1001 1 先看整数部分 0100 0000 = 64在看小数部分 0.0101 1001 1001 1001 1 转十进制 1/22 + 1/24 + 1/25 + 1/28 + 1/29 + 1/212 + 1/213 + 1/216 + 1/217 = 0.349052429199219 结论: 0 | 1000 0101 | 0000 0001 0110 0110 0110 011 = 64.349052429199219 再看 64bit 的情况：1.0000 0001 0110 0110 0110 0110 0110 0110 0110 0110 0110 0110 0110 x 26 = 100 0000.0101 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 10 0.0101 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 10 =1/22 + 1/24 + 1/25 + 1/28 + 1/29 + 1/212 + 1/213 + 1/216 + 1/217 + 1/220 + 1/221 + 1/224 + 1/225 + 1/228 + 1/229 + 1/232 + 1/233 + 1/236 + 1/237 + 1/240 + 1/241 + 1/244 + 1/245 = 0.349999999999994 结论: 0 | 1000 0000 0101 | 0000 0001 0110 0110 0110 0110 0110 0110 0110 0110 0110 0110 0110 = 64.349999999999994 从这里也可以看出浮点数都不精确！当对精确有要求的时候尽量避免使用浮点数。]]></content>
      <categories>
        <category>golang</category>
      </categories>
      <tags>
        <tag>浮点数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker常用指令]]></title>
    <url>%2F2017%2F11%2F23%2F%E6%8A%80%E6%9C%AF%2Fdocker%2F2017-11-23-Docker%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Docker 常用指令作为一个开发最近才开始使用docker ，深感惭愧！最近在学习 CloudMan 大佬的 《每天5分钟玩转 Docker 容器技术》, 通俗易懂，每天积累一点点，积累，这里记录一下常用的 docker 相关知识，仅做笔记用。 查看本地的所有 docker镜像 docker images 查看指定镜像信息 docker images ubuntu 查看指定tag 的镜像信息 docker images ubuntu:latest 从 Docker Hub 下载镜像 docker pull hello-world 交互运行一个 container docker run -it ubuntu]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算广告学习笔记之名词汇总]]></title>
    <url>%2F2017%2F09%2F01%2F%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%2F%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%90%8D%E8%AF%8D%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[前言最近看了刘鹏老师的《计算广告》，叹为惊止，对于接触广告投放业务不到一年的我产生了很大帮助，决定好好研读这本广告 圣经 计算广告名词汇总广告主（Advisiter）： 买广告位的个人或公司媒体（media）：卖广告位的个体受众（audience）：看广告的人 在线广告创意类型 横幅广告 文字链广告 富媒体广告 视屏广告 社交广告：社交广告是指几月社交信息，在用户的社交信息流中展示的广告，如你的还有对汽车很感兴趣，你们有很多的相似点，你将在你的朋友圈看到汽车广告 移动广告：移动端的广告 邮件定向营销广告：类似产品有通过广告、imessage等方式投放的广告 展示广告（Display Advertising）：纸媒上的广告方式迁移到网页中合约广告（Agreement-based Advertising）: 采用合同方式约定某一个广告位在某一时间段为某特定广告主主所独占； CPT 结算方式受众广告（Target Advertising）：针对不同用户投放不同的广告受众定向（Audience Targeting）：收集用户信息，进行归类、贴标签广告投放 （Advertising Serving）：从静态页面变为实时从服务器获取要展示的广告信息 担保式投放（Guaranteed Delivery, GD）：在合约广告的基础上，媒体向广告主保证，某个投放量，并确定投放量未完成的情况下 的赔偿方案。按千次展现付费 (Cost per Mille, CPM)在线分配(online allocation)：对于每一次在线广告的展现进行实时决策，有效的将流量分配到各个合约互相交叉的人群覆盖上 竞价广告（auction-based advertising）搜索广告（search advertising）：用户输入关键词，根据关键词精确定位广告；广告主通过购买关键词来购买广告位资源疑问：搜索广告和上下文广告中，广告主都是针对关键词出价，如果两个广告主都买了同一个广告词时，如果选择展示哪个广告？按照 eCPM 排序吗？上下文广告（search advertising）：将搜索广告中的搜索词换成页面的关键词，通过页面关键词定位广告 广义第二高价(Generalized Second Price, GSP) ADN（AD Network）：广告网络，批量运营媒体的广告位资源，按照人群或者上下文标签售卖给需求方，并用竞价的方式决定流量分配；在广告市场中广告网络的出现是为了批量聚合各媒体方的剩余流量，批量卖给需求方实现变现，因此广告网络提供的流量一般质量较差，广告网络先天性不适合投放品牌广告；国内主要公司产品：百度网盟 竞价广告网络: 竞价售卖方式的广告网络，淡化广告位标的，售卖标的是根据标签划分的人群。产品特点如下 竞价方式不向广告主做出量的约定，使得广告网络可以专注于对eCPM的估计 广告网络的出现就是为了批量聚合各媒体的剩余流量，淡化广告位，按人群售卖，很难拿到品牌溢价高的广告位，一般不适合品牌类广告 竞价广告的结算方式首推CPC, 屏蔽了广告位的概念之后，广告主无法估计每次展现出价，当使用CPC结算时，广告主只需要根据点击出价 竞价广告需求方产品: 1. 挑选广告合适的目标人群 2.针对目标人群给出合理的出价； 上述两个问题的根本就是优化ROIa) 搜索引擎营销(SEM Search Engine Marketing)：选择合适的关键词；对关键词出价 CPC（Cost per click）：按点击收费TD（Trading Desk）：交易终端，面对多个ADN或者媒体按人群一站式采买的广告并优化投入产出比的需求方产品 RTB（Real Time Bidding）：实时竞价，针对每一次广告展现，向需求方产品发送广告位及用户标识，从需求方获取报价，比较各需求方的价格，选出最优价格，展示该需求方的广告ADX (Advertising Exchange)：广告交易平台，大量聚合媒体的剩余流量，并且通过实时竞价的方式将它们变现的产品形态DSP（Demand Side Platform）：需求方平台，通过实时竞价的方式，按照定制化的人群标签购买广告； 需求方平台将非竞价广告中需要广告交易平台来实现的对人群进行定制化标签划分工作给接管了，DSP 根据自身的第一方数据（来自DMP)面对市场提供的开放竞价接口，出价采买合适的广告DMP (Data Management Platform)：数据管理平台，收集第一方数据，为自身平台收集数据信息数据交易平台 (Data Exchange Platform, DEP) 数据交易平台，收集管理交易第三方数 据 第一方数据：广告主自身的数据第二方数据：广告平台的数据第三方数据：不直接参与广告交易的其他数据提供方的数据 重定向: 针对在广告主网站有过操作的用户，精确投放操作相关的广告，提高转化率；缺点是只能提高ROI，无法获取新用户 个性化重定向， 类似于站外推荐，依赖根据用户信息的精确推荐引擎，对于电商系统动态库存查询接口也是必要的 新客推荐: 在第一方数据的基础上，依赖第二方数据，针对“对自己的产品可能感兴趣”的用户进行广告投放，针对的是没有关注广告主网站的用户。 ADN 和 ADX 的差别在于，ADN将媒体资源分类后打包出售，而 ADX 针对每一次广告展现通过实时竞价的方式，单次售卖每一次广告展现 TD 和 DSP 的差别在于，TD 批量的从ADN 或者媒体采买广告，而 DSP 需要尽可能的估计每一次广告展现带来的期望价值，计算出一个合理的报价，通过实时竞价从ADX 购买广告 程序化交易(programmatic trade) 优选(preferred deals): 允许单个广告主按照自己的意愿挑选广告 私有交易市场(Private Market Place, PMP)：邀请部分优质广告主，通过竞价方式，进行广告交易 媒介采买平台(media buying Platform) 原生广告(Native Advertising)：将广告内容和页面信息尽量做到意向近似或展示形式近似 eCPM(expected Cost per Mille)：千次展现期望收入，点击率和点击价值的乘积, eCPM的基础是对每次广告展现进行点击率预估 广告安全(AD safety)：让合适的广告出现在合适的媒体上 CTR (Click Through Rate)：点击率，广告点击和广告展现的比率 CVR (Conversion Rate)：转化率，转化次数和到达次数的比例 eCPM = r(a,u,c) = μ(a,u,c) * v(a,u,c) (a:广告 u:用户 c:环境 r: 总收入 μ:点击率 v:点击价值)从上述表达式得出: 当点击价值恒定是，CTR == eCPMeCPM代表DSP的出价，为了快速花完预算，可以将eCPM 预估的高于市场价；为了获得性价比高的流量，将eCPM定位在市场价较低的地方 eCPM 一般指的是估计的千次展现收益; RPM 指的是千次展现收入; CPM 是千次展示成本 广告结算方式 CPM 结算：按照千次展示结算，供给方和需求方约定好前次展示的计费标准，展示的收益效果由需求方把控 CPC 结算：即按点击结算，供给方（或中间市场）提供点击率数据，需求方估计点击价值，并且通过点击出价的方式向市场通知自己的估价。优点是，供给方通过大量收集用户行为数据可以相对准确的估计点击率；广告主通过分析用户的站内行为数据，对点击价值作出评估。 CPS(Cose per sale)/ CPA(Cost Per Action)/ROI(Return Of Investment) 结算：需求方按照转化收益结算，对需求方来说极大的规避了风险，对供给方来说既要估计点击率又要估计点击价值；在一些场景中供给方无法准确估计点击价值，该结算方式要求供给方能够较准确的估计点击价值或者能借助完善的第三方转化监测估计点击价值。 CPT(Cost per Time)：将某个广告位在某段时间内以独占的方式售卖给广告主，价格由双方约定，无需计量；是品牌广告的不二选择 垂直广告网络(vertical ad network)：特定主题的广告网络，适用CPS(Cose per sale)/ CPA(Cost Per Action)/ROI(Return Of Investment) 结算 合约广告产品: 按照时段售卖的CPT 广告和按照约定展示量售卖的CPM广告竞价广告产品: 主要是搜索广告 流量预测: 在担保式投放中，合约中明确约定了投放的量；那么对于供应方系统和广告交易系统来说，精准的流量预测能够让供应方在售卖流量的时候不会因为低估流量而导致流量浪费，也不会因为高估流量导致无法完成合同约定的投放量，总而言之，就是指导媒体方售卖流量；媒体方大多数情况下会和多个广告主之间签订担保投放合同，当一次广告展示符合多个合约的要求，如何决策分配给那个合约能达到整体满足所有合约的目的？在线分配强依赖于流量预测的结果，提高在线分配的效率和准确性；而在竞价广告中供应方不在担保投放量，此时需要广告主根据自己估计的流量预估出价； 流量塑形(traffic shaping): 在有些情况下我们需要主动地影响流量，如修改首页链接入口（待续！） 在线分配: 在担保式投放中，多个合约覆盖的人群可能有……待续 位置拍卖: 针对一次展现的一组广告位，按照其经验价值排名，在某次广告请求中去除前S个高出价的广告一次放到排序号的S个广告位上，此时可以假设点击率仅与位置s有关，点击价值仅与广告a有关 第二高价: 在一个广告位的多轮竞价中，广告主会倾向于逐步降低自己的出价，降到比第二高价高一点点即可成为最高价，为了避免这一现象，我们按照竞价对垒中的第二高价进行收费，此时广告主A出价1元，广告主B出价2元时，广告主A没有降价的动力，广告主B如果降价可能会比广告主A更低，而且降价后加个不会低于1元，保证了市场的收益。广义第二高价(GSP， Generalized Second Price): 将第二高价推广到一系列广告位的竞拍中，对赢得每个位置的广告主，都按照他的下一位的广告位置出价来收取费用。 VCG定价: 对于赢得了某个位置的广告主，其所付出的成本应该等于他占据的位置给其他市场参与者带来的价值损害。 市场保留价(MRP, Market Reserve Price):对拍的一个广告位的最低价格，当竞争充分、广告主深度足够时，MRP可以设置的比较高；针对不同流量可以设置不同的MRP。 价格挤压 r = μ^k * bid(cpc) k：一个大于0的实数，当k接近正无穷的时候，价格r只与点击率有关；当k接近于0的时候，价格只与cpc出价有关，称为价格挤压因子 单位流量变现能力(Revenue per Mille, RMP): 中小媒体没有优秀的品牌价值，无法销售优质品牌溢价广告，只能重点关注单位流量的变现能努力 广告投放引擎: 实时响应广告请求，并决策广告的投放，并且从全局优化的角度管理整体收益；采用的还是类搜索架构（检索+排序） 广告投放机: 结合广告投放引擎的其他模块，接受广告前端web服务器发送来的请求，完成广告投放决策并返回最后页面片段的主逻辑 广告检索: 在线时根据用户标签和页面标签从广告索引中查找符合条件的广告候选，送入广告排序模块 广告排序: 结合离线特征（CTR模型和特征）和在线特征（实时点击率特征）预估点击率，实时计算出eCPM，并按此排序 收益管理: 将部分广告排序的结果进一步调整，尽量调整到全局收益最优 广告请求接口: 接受广告请求，并且将决策好的广告返回给媒体 定制化用户划分: 根据广告主的逻辑来划分用户群；从广告主处收集用户信息和产品接口，数据经过复杂的加工之后，通过 数据高速公路导入受众定向模块 离线数据处理平台: 生成报表；通过数据挖掘、机器学习进行受众定向、点击率评估、分配策略规划 用户会话日志生成: 从各个渠道收集到的日志（点击日志等）先整理成已维护ID为键的统一存储格式。 行为定向: 根据日志中的行为给用户打上结构化标签库中的某些标签，并将结果存储在用户标签的在线缓存中，供广告投放机使用 上下文定向: 结合半在线页面抓取和上下文页面标签的缓存，与行为定向互相配合，给上下文页面打上标签，并将结果存储到页面标签的在线缓存中，供广告投放机使用 点击率建模: 在分布式计算平台上训练得到点击率的模型参数和相应特征，加载到缓存中供线上投放系统决策时使用 分配规划: 根据广告系统全局优化的具体要求，利用离线日志数据进行规划，得到适合线上执行分配方案 商业智能: 广告排期系统: 一般技术方案是将广告素材按预先确定的排期直接插入媒体页面，并通过内容分发网络(Content Delivery Network CDN)加速访问 奇异值分析潜在语义分析]]></content>
      <categories>
        <category>计算广告</category>
      </categories>
      <tags>
        <tag>计算广告</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql常用指令]]></title>
    <url>%2F2017%2F05%2F22%2F%E6%8A%80%E6%9C%AF%2Fmysql%2F2017-05-22-mysql%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[执行 .sql 文件方法1 mysql -h1.1.1.1 -P3306 -uadmin -ppwd db_name &lt; test.sql方法2 使用 mysql-client 登录到 mysql 服务器 source /root/path/to/.sql mysql 查看 binlogmysql 提供了 mysqlbinlog 指令，用于查看binlog 先来一个小例子, 查看某一段时间的 bin log mysqlbinlog -h[host] -P[port] -u[username] -p[password] --read-from-remote-server [binlog file name] --base64-output=decode-rows --start-datetime=&apos;2017-11-02 00:00:00&apos; --stop-datetime=&apos;2017-11-02 00:10:00&apos;看了上面这个小例子，发现一个小问题: binlog file name 从哪来？这个需要我们去mysql中去查看 mysql&gt; mysql -u[username] -p[password] ## 登录mysql mysql&gt; show variables like &apos;log_bin&apos;; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | log_bin | OFF | # 这个参数说明当前mysql 没有开启binlog 日志 +---------------+-------+这里又引入一个问题: 如何开启 mysql 的 binlog 在 mysql 的 官方文档 中，对于 binlog 的开启只有一段内容 Binary logging is enabled by default (the log_bin system variable is set to ON). The exception is if you use mysqld to initialize the data directory manually by invoking it with the --initialize or --initialize-insecure option, when binary logging is disabled by default, but can be enabled by specifying the --log-bin option. 二进制日志是默认启用（系统变量log_bin默认设置为ON）。唯一的例外是如果你使用mysqld来初始化数据目录手动调用它的 --initialize 或 --initialize-insecure 选项，当二进制日志默认是禁用的，但可以通过指定 `--log-bin` 选项启用。我们开始设置 mysql 的启动配置， 这里有两种方式 直接在启动命令行中添加参数 sudo /usr/local/mysql/bin/mysqld --basedir=/usr/local/mysql --log-bin=mysql-bin --user=\_mysql 在 mysql 配置文件中增加配置信息这个时候我们遇到一个问题: Mysql 的配置文件在哪？ &gt; sudo mysqld --verbose --help | grep -A2 &quot;Default options are read from the following files in the given order:&quot; Default options are read from the following files in the given order: /etc/my.cnf /etc/mysql/my.cnf /usr/local/mysql/etc/my.cnf ~/.my.cnf 从上面可以看到 mysql 的配置文件如下： /etc/my.cnf /etc/mysql/my.cnf /usr/local/mysql/etc/my.cnf ~/.my.cnf在 /etc/my.cnf 添加配置如下 [mysqld] basedir=/usr/local/mysql datadir=/usr/local/mysql/data plugin-dir=/usr/local/mysql/lib/plugin user=\_mysql log-bin=mysql-bin port=3306我们来尝试启动 mysql，但是却没能启动成功， 报错信息如下 [ERROR] You have enabled the binary log, but you haven&apos;t provided the mandatory server-id. Please refer to the proper server start-up parameters documentation遇到问题，我们再去看看官方文档，在官方文档中有如下介绍，在 mysql 5.7 及以上版本中，需要设置参数 server-id In MySQL 5.7, a server ID had to be specified when binary logging was enabled, or the server would not start. The server_id system variable is set to 1 by default. The server can be started with this default ID when binary logging is enabled, but a warning message is issued if you do not specify a server ID explicitly using the --server-id option. For servers that are used in a replication topology, you must specify a unique non-zero server ID for each server. 在MySQL 5.7中，当启用二进制日志时，必须指定服务器ID，否则服务器将无法启动。系统变量 server_id 默认设置为1。当启用二进制日志记录时，服务器可以以这个默认ID启动，但如果未显式使用服务器ID选项指定服务器ID，则会发出警告消息。对于复制拓扑中使用的服务器，必须为每个服务器指定唯一的非零服务器ID。将 /etc/my.cnf 配置修改如下： [mysqld] basedir=/usr/local/mysql datadir=/usr/local/mysql/data plugin-dir=/usr/local/mysql/lib/plugin user=\_mysql log-bin=mysql-bin port=3306 server-id=1再次启动mysql，成功了！但是有个问题，mysql 的日志直接输出在控制台中，需要输出到指定文件夹中，在 /etc/my.cnf 的 [mysqld] 中增加配置 log-error=/usr/local/mysql/data/local.err我们开启了binlog之后，可以使用 mysqlbinlog 来查看binlog了！回到第一个问题 binlog file name 从哪来？ mysql&gt; show master status; +------------------+-----------+--------------+------------------+-------------------------------------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+-----------+--------------+------------------+-------------------------------------------------+ | mysql-bin.000065 | 516819919 | | | 63c58e9d-e063-11e6-8a0e-6c92bf324c90:1-19804498 | +------------------+-----------+--------------+------------------+-------------------------------------------------+ 1 row in set (0.00 sec)这里第一列就是 binlog 日志文件的名称了！ mysqlbinlog -?, --help Display this help and exit. 查看帮助信息 --base64-output=name Determine when the output statements should be base64-encoded BINLOG statements: &apos;never&apos; disables it and works only for binlogs without row-based events; &apos;decode-rows&apos; decodes row events into commented pseudo-SQL statements if the --verbose option is also given; &apos;auto&apos; prints base64 only when necessary (i.e., for row-based events and format description events). If no --base64-output[=name] option is given at all, the default is &apos;auto&apos;. --bind-address=name IP address to bind to. --character-sets-dir=name Directory for character set files. -d, --database=name 指明查询binlog 的数据库，如：-dtest，查看test 数据库 --rewrite-db=name Rewrite the row event to point so that it can be applied to a new database -#, --debug[=#] This is a non-debug version. Catch this and exit. --debug-check This is a non-debug version. Catch this and exit. --debug-info This is a non-debug version. Catch this and exit. --default-auth=name Default authentication client-side plugin to use. -D, --disable-log-bin Disable binary log. This is useful, if you enabled --to-last-log and are sending the output to the same MySQL server. This way you could avoid an endless loop. You would also like to use it when restoring after a crash to avoid duplication of the statements you already have. NOTE: you will need a SUPER privilege to use this option. -F, --force-if-open Force if binlog was not closed properly. (Defaults to on; use --skip-force-if-open to disable.) -f, --force-read Force reading unknown binlog events. -H, --hexdump Augment output with hexadecimal and ASCII event dump. -h, --host=name Get the binlog from server. -i, --idempotent Notify the server to use idempotent mode before applying Row Events -l, --local-load=name Prepare local temporary files for LOAD DATA INFILE in the specified directory. -o, --offset=# Skip the first N entries. -p, --password[=name] Password to connect to remote server. --plugin-dir=name Directory for client-side plugins. -P, --port=# Port number to use for connection or 0 for default to, in order of preference, my.cnf, $MYSQL_TCP_PORT, /etc/services, built-in default (3306). --protocol=name The protocol to use for connection (tcp, socket, pipe, memory). -R, --read-from-remote-server Read binary logs from a MySQL server. This is an alias for read-from-remote-master=BINLOG-DUMP-NON-GTIDS. --read-from-remote-master=name Read binary logs from a MySQL server through the COM_BINLOG_DUMP or COM_BINLOG_DUMP_GTID commands by setting the option to either BINLOG-DUMP-NON-GTIDS or BINLOG-DUMP-GTIDS, respectively. If --read-from-remote-master=BINLOG-DUMP-GTIDS is combined with --exclude-gtids, transactions can be filtered out on the master avoiding unnecessary network traffic. --raw Requires -R. Output raw binlog data instead of SQL statements, output is to log files. -r, --result-file=name Direct output to a given file. With --raw this is a prefix for the file names. --secure-auth Refuse client connecting to server if it uses old (pre-4.1.1) protocol. Deprecated. Always TRUE --server-id=# Extract only binlog entries created by the server having the given id. --server-id-bits=# Set number of significant bits in server-id --set-charset=name Add &apos;SET NAMES character_set&apos; to the output. -s, --short-form Just show regular queries: no extra info and no row-based events. This is for testing only, and should not be used in production systems. If you want to suppress base64-output, consider using --base64-output=never instead. -S, --socket=name The socket file to use for connection. --ssl-mode=name SSL connection mode. --ssl Deprecated. Use --ssl-mode instead. (Defaults to on; use --skip-ssl to disable.) --ssl-verify-server-cert Deprecated. Use --ssl-mode=VERIFY_IDENTITY instead. --ssl-ca=name CA file in PEM format. --ssl-capath=name CA directory. --ssl-cert=name X509 cert in PEM format. --ssl-cipher=name SSL cipher to use. --ssl-key=name X509 key in PEM format. --ssl-crl=name Certificate revocation list. --ssl-crlpath=name Certificate revocation list path. --tls-version=name TLS version to use, permitted values are: TLSv1, TLSv1.1 --start-datetime=name Start reading the binlog at first event having a datetime equal or posterior to the argument; the argument must be a date and time in the local time zone, in any format accepted by the MySQL server for DATETIME and TIMESTAMP types, for example: 2004-12-25 11:25:56 (you should probably use quotes for your shell to set it properly). -j, --start-position=# Start reading the binlog at position N. Applies to the first binlog passed on the command line. --stop-datetime=name Stop reading the binlog at first event having a datetime equal or posterior to the argument; the argument must be a date and time in the local time zone, in any format accepted by the MySQL server for DATETIME and TIMESTAMP types, for example: 2004-12-25 11:25:56 (you should probably use quotes for your shell to set it properly). --stop-never Wait for more data from the server instead of stopping at the end of the last log. Implicitly sets --to-last-log but instead of stopping at the end of the last log it continues to wait till the server disconnects. --stop-never-slave-server-id=# The slave server_id used for --read-from-remote-server --stop-never. This option cannot be used together with connection-server-id. --connection-server-id=# The slave server_id used for --read-from-remote-server. This option cannot be used together with stop-never-slave-server-id. --stop-position=# Stop reading the binlog at position N. Applies to the last binlog passed on the command line. -t, --to-last-log Requires -R. Will not stop at the end of the requested binlog but rather continue printing until the end of the last binlog of the MySQL server. If you send the output to the same MySQL server, that may lead to an endless loop. -u, --user=name 链接数据库的用户名 -v, --verbose Reconstruct pseudo-SQL statements out of row events. -v -v adds comments on column data types. -V, --version 查看 `mysqlbinlog` 版本信息 --open-files-limit=# Used to reserve file descriptors for use by this program. -c, --verify-binlog-checksum Verify checksum binlog events. --binlog-row-event-max-size=# The maximum size of a row-based binary log event in bytes. Rows will be grouped into events smaller than this size if possible. This value must be a multiple of 256. --skip-gtids Do not preserve Global Transaction Identifiers; instead make the server execute the transactions as if they were new. --include-gtids=name 打印binlog 信息时，只打印指定的部分全局事务id对应的binglog 信息 --exclude-gtids=name 打印binlog 信息时，排除部分全局事务id对应的binglog 信息待解决问题如果在 mac os 的设置中启动了mysql 而且勾选了 Automatically Start MySQL Server on Startup, 当你想要关闭 mysql 的时候，mysql 会自动重启！！！这个问题找了好久才发现，至于这个不断重启功能的实现待研究… mysql 查看存储过程查看存储过程状态1show procedure status; 输出结果： mysql&gt; show procedure status\G *************************** 1. row *************************** Db: db_name Name: procedure_name Type: PROCEDURE Definer: root@localhost Modified: 2017-05-10 14:10:00 Created: 2017-05-10 14:10:00 Security_type: DEFINER Comment: character_set_client: utf8mb4 collation_connection: utf8mb4_general_ci Database Collation: utf8_general_ci 1 row in set (0.02 sec)查看存储过程具体定义1show create procedure %procedure_name%; 创建存储过程注意：在创建存储过程的时候，语句中的分号会导致报错，需要用 DELIMITER 重新定义分隔符 123DELIMITER $$-- to create procedureDELIMITER ; 删除存储过程1DROP &#123;PROCEDURE | FUNCTION&#125; [IF EXISTS] sp_name mysql 查看数据库和表容量大小information_schema 库中有个 TABLES 表 mysql&gt; show create table TABLES\G *************************** 1. row *************************** Table: TABLES Create Table: CREATE TEMPORARY TABLE `TABLES` ( `TABLE_CATALOG` varchar(512) NOT NULL DEFAULT &apos;&apos;, `TABLE_SCHEMA` varchar(64) NOT NULL DEFAULT &apos;&apos;, `TABLE_NAME` varchar(64) NOT NULL DEFAULT &apos;&apos;, `TABLE_TYPE` varchar(64) NOT NULL DEFAULT &apos;&apos;, `ENGINE` varchar(64) DEFAULT NULL, `VERSION` bigint(21) unsigned DEFAULT NULL, `ROW_FORMAT` varchar(10) DEFAULT NULL, `TABLE_ROWS` bigint(21) unsigned DEFAULT NULL, `AVG_ROW_LENGTH` bigint(21) unsigned DEFAULT NULL, `DATA_LENGTH` bigint(21) unsigned DEFAULT NULL, `MAX_DATA_LENGTH` bigint(21) unsigned DEFAULT NULL, `INDEX_LENGTH` bigint(21) unsigned DEFAULT NULL, `DATA_FREE` bigint(21) unsigned DEFAULT NULL, `AUTO_INCREMENT` bigint(21) unsigned DEFAULT NULL, `CREATE_TIME` datetime DEFAULT NULL, `UPDATE_TIME` datetime DEFAULT NULL, `CHECK_TIME` datetime DEFAULT NULL, `TABLE_COLLATION` varchar(32) DEFAULT NULL, `CHECKSUM` bigint(21) unsigned DEFAULT NULL, `CREATE_OPTIONS` varchar(255) DEFAULT NULL, `TABLE_COMMENT` varchar(2048) NOT NULL DEFAULT &apos;&apos; ) ENGINE=MEMORY DEFAULT CHARSET=utf8 1 row in set (0.00 sec)其中主要字段是 TABLE_SCHEMA: 数据库名称TABLE_NAME: 表名称DATA_LENGTH: 数据占用的空间大小，单位是字节INDEX_LENGTH: 索引占用的空间大小，单位是字节 查看数据库占用空间大小123SELECT (SUM(DATA_LENGTH) + SUM(INDEX_LENGTH)) / 1024 / 1024FROM information_schema.TABLESWHERE TABLE_SCHEMA = 'database_name'; 查看表占用空间大小123SELECT (SUM(DATA_LENGTH) + SUM(INDEX_LENGTH)) / 1024 / 1024FROM information_schema.TABLESWHERE TABLE_NAME = 'table_name'; mysql 求时间差距timestampdiff(unit,datetime_expr1,datetime_expr2) 比较两个 timestamp 时间间隔，返回单位是unit 的数值 例如 timestampdiff(second,&#39;2018-01-01 00:00:00&#39;,&#39;2018-01-01 00:00:02&#39;) 得到 2 表结构常用修改语句重置 AUTO_INCREMENT1ALTER TABLE &lt;Table Name&gt; AUTO_INCREMENT = 1;]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何正确的创建 prototype 类型的 bean]]></title>
    <url>%2F2017%2F03%2F01%2F%E6%8A%80%E6%9C%AF%2Fspring%2F2017-03-01-Spring%E4%BD%BF%E7%94%A8%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言这里把使用 Spring 过程中遇到的问题做个记录，有些给出了解决方案，有些没有，欢迎同道读者斧正！ 如何正确的创建 prototype 类型的 Spring bean这里先尝试的去创建一个 prototype 的 bean 1234567891011121314151617@Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE)@Componentpublic class ProtoTypeComponent &#123; private final Logger LOGGER = LoggerFactory.getLogger(this.getClass()); private int random; public ProtoTypeComponent() &#123; // random 值设置为一个随机值， this.random = ThreadLocalRandom.current().nextInt(100000); LOGGER.info("create a new ProtoTypeComponent with random:&#123;&#125;", random); &#125; public int getRandom() &#123; return random; &#125;&#125; 然后，我们在Controller 中注入这个 prototype bean 1234567891011121314@Controllerpublic class HelloController &#123; @Resource private ProtoTypeComponent protoTypeComponent; @RequestMapping(value = "/hello") @ResponseBody public Object hello() &#123; return "&#123;\"class\": \"" + protoTypeComponent + "\", \"random\":\"" + protoTypeComponent.getRandom() + "\"&#125;"; &#125;&#125; 然后我们开始请求这个 http 接口 不断的刷新页面，返回的结果都是一模一样！我们再来看看日志 从上面的日志上我们看到这个 prototype bean 的构造方法只被调用了一次！ 到这里，我感到奇怪了，为什么这个 prototype bean 成了 singleton bean 了？ 这个时候我想了 《Spring揭秘》 中看到的正解， 之前这个现象出现的原因是 HelloController 是 singleton 类型的，这个 bean 只会被初始化一次， 而 HelloController 依赖的 ProtoTypeComponent 也只会被注入一次！到这里，谜底揭晓： singleton bean 依赖的 prototype bean 会 “变成” singleton bean 这里，解决这个问题的第一个方案也出现了： 依赖 prototype bean 也必须是 prototype bean很明显这个方案不能解决很多状况， 我们继续这个问题 再来翻翻 《Spring 揭秘》 找找答案我们看到了两个答案 使用方法注入 继承 ApplicationContextAware 每次去获取bean 使用方法注入在传统的 xml 配置方式中我们需要这样配置 1234&lt;bean id="protoTypeBean" class="...impl.ProtoTypeService" singleton="false"/&gt;&lt;bean class = "...impl.SimpleServiceImpl"&gt; &lt;lookup-method name="getProtoTypeBean" bean="protoTypeBean"/&gt;&lt;/bean&gt; getProtoTypeBean 方法定义如下 12345private ProtoTypeService protoTypeBean;public ProtoTypeService getProtoTypeBean() &#123; return protoTypeBean;&#125; 现在我们更倾向于使用注解，那么基于注解怎么实现上面这种方式呢？我们这里引入 @Lookup 注解 123456789101112131415161718192021222324252627282930313233343536373839404142/** * An annotation that indicates 'lookup' methods, to be overridden by the container * to redirect them back to the &#123;@link org.springframework.beans.factory.BeanFactory&#125; * for a &#123;@code getBean&#125; call. This is essentially an annotation-based version of the * XML &#123;@code lookup-method&#125; attribute, resulting in the same runtime arrangement. * * &lt;p&gt;The resolution of the target bean can either be based on the return type * (&#123;@code getBean(Class)&#125;) or on a suggested bean name (&#123;@code getBean(String)&#125;), * in both cases passing the method's arguments to the &#123;@code getBean&#125; call * for applying them as target factory method arguments or constructor arguments. * * &lt;p&gt;Such lookup methods can have default (stub) implementations that will simply * get replaced by the container, or they can be declared as abstract - for the * container to fill them in at runtime. In both cases, the container will generate * runtime subclasses of the method's containing class via CGLIB, which is why such * lookup methods can only work on beans that the container instantiates through * regular constructors: i.e. lookup methods cannot get replaced on beans returned * from factory methods where we cannot dynamically provide a subclass for them. * * &lt;p&gt;&lt;b&gt;Concrete limitations in typical Spring configuration scenarios:&lt;/b&gt; * When used with component scanning or any other mechanism that filters out abstract * beans, provide stub implementations of your lookup methods to be able to declare * them as concrete classes. And please remember that lookup methods won't work on * beans returned from &#123;@code @Bean&#125; methods in configuration classes; you'll have * to resort to &#123;@code @Inject Provider&amp;lt;TargetBean&amp;gt;&#125; or the like instead. * * @author Juergen Hoeller * @since 4.1 * @see org.springframework.beans.factory.BeanFactory#getBean(Class, Object...) * @see org.springframework.beans.factory.BeanFactory#getBean(String, Object...) */@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Lookup &#123; /** * This annotation attribute may suggest a target bean name to look up. * If not specified, the target bean will be resolved based on the * annotated method's return type declaration. */ String value() default ""; 从上面的代码能看出：这个注解只能修饰方法，根据方法的返回值类型创建对应的Bean，底层机制是调用 org.springframework.beans.factory.BeanFactory#getBean(Class, Object...) 或 org.springframework.beans.factory.BeanFactory#getBean(String, Object...) 先来使用 @Lookup 注解改成一下我们尝试创建 有状态的 prototype bean 修正的代码如下: 12345678910111213141516171819@Controllerpublic class HelloController &#123; private final Logger LOGGER = LoggerFactory.getLogger(this.getClass()); private ProtoTypeComponent protoTypeComponent; @RequestMapping(value = "/hello") @ResponseBody public Object hello() &#123; ProtoTypeComponent component = getProtoTypeComponent(); return "&#123;\"class\": \"" + component + "\", \"random\":\"" + component.getRandom() + "\"&#125;"; &#125; @Lookup public ProtoTypeComponent getProtoTypeComponent() &#123; return protoTypeComponent; &#125;&#125; 让我们运行起 tomcat 看看结果： 很明显，我们成功的创建了 prototype bean! 继承 ApplicationContextAware 每次去获取bean这里不推荐使用 ApplicationContextAware 所以不给出方法了。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven 继承方式： scope import]]></title>
    <url>%2F2017%2F02%2F12%2F%E6%8A%80%E6%9C%AF%2Fmaven%2FMaven%20scope%20import%2F</url>
    <content type="text"><![CDATA[使用 scope import 解决Maven 单继承问题Maven 本身支持继承，很多时候我们会创建多模块项目，而多个模块会引入相同的依赖项，这个时候我们就能使用 Maven 的父子工程结构，创建一个父 pom 文件，其他子项目中的 pom 文件继承父pom 12345&lt;parent&gt; &lt;groupId&gt;base.parent&lt;/groupId&gt;--&gt; &lt;artifactId&gt;parent.management&lt;/artifactId&gt; &lt;version&gt;1.0.0.RELEASE&lt;/version&gt;&lt;/parent&gt; 这样我们就能是的依赖项的管理更加有条理 但是 Maven 父子项目结构和 Java继承一样，都是单继承，一个子项目只能制定一个父 pom 很多时候，我们需要打破这种 单继承 例如使用 spring-boot 的时候， 官方推荐的方式是继承父pom spring-boot-starter-parent 12345&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.1.RELEASE&lt;/version&gt;&lt;/parent&gt; 但是如果项目中已经有了其他父pom， 又想用 spring-boot 怎么办呢？ 12345678910&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;1.5.1.RELEASE&lt;/version&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 就是使用 scope import， 还需要指定 type pom 注意：scope import 只能在 dependencyManagement 中的使用]]></content>
      <categories>
        <category>Maven</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot 初涉]]></title>
    <url>%2F2017%2F02%2F12%2F%E6%8A%80%E6%9C%AF%2Fspring%2F2017-02-12-SpringBoot%2F</url>
    <content type="text"><![CDATA[使用 Maven 构建 Spring-Boot 项目 继承 父pmo 构建 Spring-Boot 项目 123456789101112131415161718192021222324252627282930313233343536373839&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- ...... the artifactId and groupId of the application --&gt; &lt;!-- 继承 spring-boot-starter-parent 使用 spring-boot的最优方法； 也可以不继承这个父pom，使用其他方法 --&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.1.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.7&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;!-- 设置插件，打包jar包 --&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- ...... the artifactId and groupId of the application --&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- Override Spring Data release train provided by Spring Boot --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-releasetrain&lt;/artifactId&gt; &lt;version&gt;Fowler-SR2&lt;/version&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;1.5.1.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;!-- --&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;version&gt;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.7&lt;/java.version&gt; &lt;/properties&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;!-- 设置插件，打包jar包 --&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; dev-tools123456789101112131415161718192021&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;!-- 设置插件，打包jar包 --&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;!-- 使用 dev-tools 需要禁用默认的 excludeDevtools --&gt; &lt;configuration&gt; &lt;excludeDevtools&gt;false&lt;/excludeDevtools&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; DevToolsPropertyDefaultsPostProcessor]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC 容器 和 Spring 容器]]></title>
    <url>%2F2017%2F01%2F21%2F%E6%8A%80%E6%9C%AF%2Fspring%2F2017-01-21-SpringMvc%E5%92%8CSpring%E4%BD%BF%E7%94%A8%E4%B8%80%E4%B8%AA%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[前言在使用 Spring MVC 的过程中，遇到一个问题，在 Spring 容器 中注册的属性文件， 在 SpringMVC 容器 中无法用 @Value 标签引入！按理说， SpringMVC 容器 应该能继承父容器中的所有Bean， 为什么不能使用父容器中引入的配置文件信息呢？ 很遗憾，这个问题暂时没能找到答案，如果哪位大神能解答，请告知 但是问题还是要解决的， 我们只能换个思路解决。 如果 SpringMVC 子容器 和 Spring 父容器 如果合一了，不就不存在需要重复引入的问题吗？那么问题来了： SpringMVC 容器和 Spring 容器能合一吗？先来看看一般我们使用 SpringMVC 的 web.xml 配置文件， 123456789101112131415161718192021222324252627282930313233343536373839&lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot; &gt;&lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;!-- spring 父容器配置文件路径 --&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring/spring-root.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;!-- listener：加载spring 父容器 --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- spring mvc 子容器配置文件路径，如果不配置默认为 /WEB-INF/*-servlet.xml --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring/spring-mvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;error-page&gt; &lt;error-code&gt;500&lt;/error-code&gt; &lt;location&gt;/error.jsp&lt;/location&gt; &lt;/error-page&gt;&lt;/web-app&gt; Tomcat 在解析 web.xml 的时候先后加载 listener -&gt; filter -&gt; servlet先记载 ContextLoaderListener 12345678/*** 覆写 `ServletContextListener` 中的 `contextInitialized` 方法，* 实现时，调用父类的 `initWebApplicationContext` 方法*/@Overridepublic void contextInitialized(ServletContextEvent event) &#123; initWebApplicationContext(event.getServletContext());&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374/** * * @param servletContext 当前 web 容器上下文 * @return spring 容器上下文 * @see #ContextLoader(WebApplicationContext) * @see #CONTEXT_CLASS_PARAM * @see #CONFIG_LOCATION_PARAM */ public WebApplicationContext initWebApplicationContext(ServletContext servletContext) &#123; if (servletContext.getAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE) != null) &#123; throw new IllegalStateException( "Cannot initialize context because there is already a root application context present - " + "check whether you have multiple ContextLoader* definitions in your web.xml!"); &#125; Log logger = LogFactory.getLog(ContextLoader.class); servletContext.log("Initializing Spring root WebApplicationContext"); if (logger.isInfoEnabled()) &#123; logger.info("Root WebApplicationContext: initialization started"); &#125; long startTime = System.currentTimeMillis(); try &#123; // 创建一个 WebApplicationContext 实例 if (this.context == null) &#123; this.context = createWebApplicationContext(servletContext); &#125; if (this.context instanceof ConfigurableWebApplicationContext) &#123; ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) this.context; if (!cwac.isActive()) &#123; // The context has not yet been refreshed -&gt; provide services such as // setting the parent context, setting the application context id, etc if (cwac.getParent() == null) &#123; // The context instance was injected without an explicit parent -&gt; // determine parent for root web application context, if any. ApplicationContext parent = loadParentContext(servletContext); cwac.setParent(parent); &#125; configureAndRefreshWebApplicationContext(cwac, servletContext); &#125; &#125; // 当前创建的spring 父容器注册到 web 容器中， 属性名称是： org.springframework.web.context.WebApplicationContext.ROOT servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, this.context); ClassLoader ccl = Thread.currentThread().getContextClassLoader(); if (ccl == ContextLoader.class.getClassLoader()) &#123; currentContext = this.context; &#125; else if (ccl != null) &#123; currentContextPerThread.put(ccl, this.context); &#125; if (logger.isDebugEnabled()) &#123; logger.debug("Published root WebApplicationContext as ServletContext attribute with name [" + WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE + "]"); &#125; if (logger.isInfoEnabled()) &#123; long elapsedTime = System.currentTimeMillis() - startTime; logger.info("Root WebApplicationContext: initialization completed in " + elapsedTime + " ms"); &#125; return this.context; &#125; catch (RuntimeException ex) &#123; logger.error("Context initialization failed", ex); servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, ex); throw ex; &#125; catch (Error err) &#123; logger.error("Context initialization failed", err); servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, err); throw err; &#125; &#125; 而 DispatcherServlet 继承 FrameworkServlet, FrameworkServlet 继承 HttpServletBeanHttpServletBean 继承 HttpServlet, 并且提供 init 方法 HttpServletBean 中的 init 方法调用抽象方法 initServletBean 1234567891011121314@Override public final void init() throws ServletException &#123; if (logger.isDebugEnabled()) &#123; logger.debug("Initializing servlet '" + getServletName() + "'"); &#125; // ...... // Let subclasses do whatever initialization they like. initServletBean(); if (logger.isDebugEnabled()) &#123; logger.debug("Servlet '" + getServletName() + "' configured successfully"); &#125; &#125; FrameworkServlet 实现 initServletBean 方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344@Override protected final void initServletBean() throws ServletException &#123; // ...... 忽略 this.webApplicationContext = initWebApplicationContext(); initFrameworkServlet(); // ...... 忽略 &#125; protected WebApplicationContext initWebApplicationContext() &#123; // 在 web 容器 ServletContext 中查找 属性为 org.springframework.web.context.WebApplicationContext.ROOT 的 WebApplicationContext Spring 父容器 WebApplicationContext rootContext = WebApplicationContextUtils.getWebApplicationContext(getServletContext()); WebApplicationContext wac = null; // 如果当前以及创建了 SpringMVC 子容器 if (this.webApplicationContext != null) &#123; wac = this.webApplicationContext; if (wac instanceof ConfigurableWebApplicationContext) &#123; // 如果 springmvc 子容器是 ConfigurableWebApplicationContext, 并且没有激活，设置 spring 父容器和 springmvc 子容器之前的父子容器关系 ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) wac; if (!cwac.isActive()) &#123; if (cwac.getParent() == null) &#123; cwac.setParent(rootContext); &#125; // 配置刷下spring mvc 子容器 configureAndRefreshWebApplicationContext(cwac); &#125; &#125; &#125; // 如果当前没有创建 springmvc 子容器 if (wac == null) &#123; // 在 web 容器 ServletContext 中查找 属性为 org.springframework.web.context.WebApplicationContext.ROOT 的 spring 容器， 将这个父容器作为 spring mvc 子容器， 这种情况下， spring mvc 子容器和 spring 父容器就合一了！ wac = findWebApplicationContext(); &#125; // 如果上一步没有在 web 容器 ServletContext 中查找到 属性为 org.springframework.web.context.WebApplicationContext.ROOT 的 WebApplicationContext Spring 容器；创建 contextConfigLocation 设置的配置文件注定的 springmvc 子容器 if (wac == null) &#123; // No context instance is defined for this servlet -&gt; create a local one wac = createWebApplicationContext(rootContext); &#125; // ...... 忽略 return wac;&#125; 在这里，我们找到了 两个容器合一的方法！ 果然是 Time is cheap, show me the code, 源码能告诉我们一切！到这里， 找到了配置方式： 12345678910111213141516171819202122232425262728293031&lt;!DOCTYPE web-app PUBLIC "-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN" "http://java.sun.com/dtd/web-app_2_3.dtd" &gt;&lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring/spring-root.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;!-- listener --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- spring mvc --&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;!-- 该servlet的spring上下文采用WebApplicationContext，不再重复生成上下文 --&gt; &lt;param-name&gt;contextAttribute&lt;/param-name&gt; &lt;param-value&gt;org.springframework.web.context.WebApplicationContext.ROOT&lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; 下面在回头来看看之前没有解决的问题： 为什么 SpringMVC 子容器 不能继承 Spring 父容器 引入的属性文件待解决…]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>Spring MVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql 避免重复插入]]></title>
    <url>%2F2017%2F01%2F14%2F%E6%8A%80%E6%9C%AF%2Fmysql%2F2017-01-14-mysql%E9%81%BF%E5%85%8D%E9%87%8D%E5%A4%8D%E6%8F%92%E5%85%A5%2F</url>
    <content type="text"><![CDATA[mysql 避免重复插入mysql 避免重复插入的方式有这样几种 使用 ignore 关键字 使用 replace 关键字 使用 on duplicate key update 使用 ignore 关键字当表中存在主键或者唯一索引的时候, 避免重复插入的时候，就可以使用以下命令 1insert ignore into table_name set name = ?, desc = ? 当表中已经存在数据的时候，本次 insert 会被忽略 注意：需要注意的是 insert ignore 在忽略某一次插入的时候自增id 还是会增长的，举例如下： 123456CREATE TABLE `tb1` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL COMMENT '名称', `modify_time` timestamp NOT NULL DEFAULT '2000-01-01 00:00:00' COMMENT '修改时间', PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='测试表'; 123456789101112131415mysql&gt; insert into tb1 value(1, "name1", now());Query OK, 1 row affected (0.02 sec)mysql&gt; insert into tb1 value(1, "name2", now());ERROR 1062 (23000): Duplicate entry '1' for key 'PRIMARY'mysql&gt; select * from tb1;+----+-------+---------------------+| id | name | modify_time |+----+-------+---------------------+| 1 | name1 | 2018-07-25 13:35:27 |+----+-------+---------------------+1 row in set (0.00 sec)mysql&gt; insert ignore into tb1 value(1, "name2", now());Query OK, 0 rows affected, 1 warning (0.01 sec) 使用 replace 关键字除了 insert ignore 之外还有 replace into 这里也要求表中必须存在 主键 或者 唯一索引 1replace into table_name set name = 'a', desc = 'b' 表中 name 必须存在 唯一索引 ， 如果表中已经存在 name = ‘a’ 的时候， desc 的值将变更为 ‘b’ 使用 on duplicate key update当表中存在主键或者唯一索引的时候, 避免重复插入的时候，还可以使用以下命令 1insert into table_name(uniq_name, desc) values('exists_name', 'desc') on duplicate key on update count = count+1; 上面的语句在避免重复插入的同时，还可以修改某一字段， 如果不需要修改任何字段，请使用一下语句 1insert into table_name(uniq_name, desc) values('exists_name', 'desc') on duplicate key on update desc = desc; 可以说 on duplicate key update 能够实现 insert ignore 和 replace into 的功能！]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Protobuf 简单使用]]></title>
    <url>%2F2017%2F01%2F01%2F%E6%8A%80%E6%9C%AF%2Fprotobuf%2F2017-01-01-Protobuf%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[一个简单的介绍Google Protocol Buffer( 简称 Protobuf) 是 Google 公司内部的混合语言数据标准，目前已经正在使用的有超过 48,162 种报文格式定义和超过 12,183 个 .proto 文件。他们用于 RPC 系统和持续数据存储系统。Protocol Buffers 是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。目前提供了 C++、Java、Python 等多种语言的 API。详见：GitHub 安装 protobuf首先我们需要安装 protobuf 这里我介绍一下我在 macOS 中使用 brew 安装 protobuf 首先我们看看可以安装的选项 1brew search protobuf 这里因为我已经安装了 protobuf250, 所以这里有个小 ✔️ 如果这里的搜索结果中没有查看到版本相关信息， 执行下面的指令 1brew tap homebrew/versions 这样以后 使用 brew search appName 的时候就能看到不同版本的应用了 使用 brew 安装 1brew install protobuf250 安装成功之后可以检验一下： 1protoc -version protoc 指令下面我们就能使用 protoc 指令来讲 .proto 文件编译成 protobuf 支持的其他语言的文件了，例如 .java 文件 指令如下 1protoc --java_out=/path/to/java/out/dir/ ./Demo.proto 将 Demo.proto 编译成 Demo.java 文件， 并且输入到 /path/to/java/out/dir 具体介绍见protoc 指令介绍 Intellij Idea 使用 Google Protocol Buffers Support当然，日常开发过程中，我们都是使用 IDE, 如 Intellij IdeaIntellij Idea 对 protobuf 的支持比较好 1 安装 Google Protocol Buffers Support 插件 2 添加 Protobuf Facet 3 点击 Build Project 按钮 .proto 文件]]></content>
      <categories>
        <category>protobuf</category>
      </categories>
      <tags>
        <tag>protobuf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[protoc 指令介绍]]></title>
    <url>%2F2017%2F01%2F01%2F%E6%8A%80%E6%9C%AF%2Fprotobuf%2F2017-01-01-protoc%E6%8C%87%E4%BB%A4%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[protoc 指令介绍protoc 指令使用示例： 1protoc [参数] .proto文件路径 使用 protoc 指令讲 .proto 文件编译成 .java 文件的简单使用如下 1protoc --java_out=./ /path_to_proto_file/Demo.proto protoc 指令参数翻译如下 注意： protoc 对文件路径不敏感，所有参数中的路径，必须是绝对路径 -I[PATH], –proto_path=[PATH]: 指定 import 修饰符扫描文件夹；可以指定多次，被指定的文件夹将按照先后制定顺序被扫描；如果没有指定，将使用当前文件夹作为扫描文件夹 注意： 这里的[PATH]必须是绝对路径 –version: 查看 protoc 指令版本 -h, –help: 查看帮助文档 –encode=MESSAGE_TYPE –decode=MESSAGE_TYPE –decode_raw -o[FILE], –descriptor_set_out=[FILE] –include_imports –include_source_info –error_format –java_out=[PATH]: 指定生成的java文件的输出文件夹 注意 .proto 文件中如果指定了 java_package， 会在输出文件夹中创建java 的package 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748➜ ~ : ✗ protoc -hUsage: protoc [OPTION] PROTO_FILESParse PROTO_FILES and generate output based on the options given: -IPATH, --proto_path=PATH Specify the directory in which to search for imports. May be specified multiple times; directories will be searched in order. If not given, the current working directory is used. --version Show version info and exit. -h, --help Show this text and exit. --encode=MESSAGE_TYPE Read a text-format message of the given type from standard input and write it in binary to standard output. The message type must be defined in PROTO_FILES or their imports. --decode=MESSAGE_TYPE Read a binary message of the given type from standard input and write it in text format to standard output. The message type must be defined in PROTO_FILES or their imports. --decode_raw Read an arbitrary protocol message from standard input and write the raw tag/value pairs in text format to standard output. No PROTO_FILES should be given when using this flag. -oFILE, Writes a FileDescriptorSet (a protocol buffer, --descriptor_set_out=FILE defined in descriptor.proto) containing all of the input files to FILE. --include_imports When using --descriptor_set_out, also include all dependencies of the input files in the set, so that the set is self-contained. --include_source_info When using --descriptor_set_out, do not strip SourceCodeInfo from the FileDescriptorProto. This results in vastly larger descriptors that include information about the original location of each decl in the source file as well as surrounding comments. --error_format=FORMAT Set the format in which to print errors. FORMAT may be 'gcc' (the default) or 'msvs' (Microsoft Visual Studio format). --plugin=EXECUTABLE Specifies a plugin executable to use. Normally, protoc searches the PATH for plugins, but you may specify additional executables not in the path using this flag. Additionally, EXECUTABLE may be of the form NAME=PATH, in which case the given plugin name is mapped to the given executable even if the executable's own name differs. --cpp_out=OUT_DIR Generate C++ header and source. --java_out=OUT_DIR Generate Java source file. --python_out=OUT_DIR Generate Python source file.]]></content>
      <categories>
        <category>protobuf</category>
      </categories>
      <tags>
        <tag>protobuf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC 中的 @ControllerAdvice]]></title>
    <url>%2F2016%2F12%2F10%2F%E6%8A%80%E6%9C%AF%2Fspring%2F2016-12-10-SpringMvc-%40ControllerAdvice%2F</url>
    <content type="text"><![CDATA[SpringMVC 中的 @ControllerAdviceSpringMVC 中常用的注解网上有很多介绍，@ControllerAdvice 这个注解相对来说少见一点 从名称上就能看出， @ControllerAdvice 是用来增强 @Controller 的 使用 `@ControllerAdvice` 注解增强控制器 带有 `@ControllerAdvice` 注解的类，可以包含 @ExceptionHandler、@InitBinder, 和 `@ModelAttribute` 注解的方法， 并且这些注解的方法会通过控制器层次应用到所有 `@RequestMapping` 方法中，而不用一一在控制器内部声明。看到上面这段话，大家有没有一种兴奋感？在没有 @ControllerAdvice 之前， 我们是怎么使用 @ExceptionHandler 的呢？ 很简单，我们给 Controller 定义父类，在父类中又一个方法，被 @ExceptionHandler 修饰的方法，在这里处理， 从上面这句话来说，我们现在并不需要定义什么父类，只要用 @ControllerAdvice 修饰一个类，它就能成为全局异常处理器！ Talk is cheap, show me the code! 先来一个 Controller: 12345678910@Controller@RequestMapping("/spring/demo")public class BaseController &#123; @RequestMapping("/hello") @ResponseBody public String hello() &#123; throw new RuntimeException("test exceptions"); &#125;&#125; 再来一个 ControllerAdvice, 直接返回一个 json 格式的异常信息 12345678910@ControllerAdvicepublic class BaseControllerAdvice &#123; @ExceptionHandler @ResponseBody public Object handlerException(Exception e, HttpServletRequest request) &#123; logger.info("exception: &#123;&#125;", e); return e; &#125;&#125; 再来贴一下 spring 配置信息: 12345678910111213141516171819&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:mvc="http://www.springframework.org/schema/mvc" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd"&gt; &lt;mvc:annotation-driven/&gt; &lt;context:component-scan base-package="com.springdemo.liam"&gt; &lt;context:include-filter type="annotation" expression="org.springframework.stereotype.Controller"/&gt; &lt;context:include-filter type="annotation" expression="org.springframework.web.bind.annotation.ControllerAdvice"/&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.stereotype.Service"/&gt; &lt;/context:component-scan&gt;&lt;/beans&gt; 看看运行结果，很不错，我们实现了全剧异常处理 到这里大家会比较疑惑，为什么 ControllerAdvice 可以被自动扫描呢？ 先来看看源码： 12345678910111213141516171819@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Componentpublic @interface ControllerAdvice &#123; @AliasFor("basePackages") String[] value() default &#123;&#125;; @AliasFor("value") String[] basePackages() default &#123;&#125;; Class&lt;?&gt;[] basePackageClasses() default &#123;&#125;; Class&lt;?&gt;[] assignableTypes() default &#123;&#125;; Class&lt;? extends Annotation&gt;[] annotations() default &#123;&#125;;&#125; 我们可以看到 @ControllerAdvice 是一个 @Component, 它当然能被扫描除了 @ExceptionHandler 之外，还有 @InitBinder、@ModelAttribute 修饰的方法，不是很常用 这里如果只是 这三个功能， @ControllerAdvice 还略显鸡肋 下面来看看 Spring 4.2 以来的新特性 ResponseBodyAdvice 和 RequestBodyAdvice ResponseBodyAdvice 的源码： 123456789101112131415161718192021222324252627public interface ResponseBodyAdvice&lt;T&gt; &#123; /** * Whether this component supports the given controller method return type * and the selected &#123;@code HttpMessageConverter&#125; type. * @param returnType the return type * @param converterType the selected converter type * @return &#123;@code true&#125; if &#123;@link #beforeBodyWrite&#125; should be invoked, &#123;@code false&#125; otherwise */ boolean supports(MethodParameter returnType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; converterType); /** * Invoked after an &#123;@code HttpMessageConverter&#125; is selected and just before * its write method is invoked. * @param body the body to be written * @param returnType the return type of the controller method * @param selectedContentType the content type selected through content negotiation * @param selectedConverterType the converter type selected to write to the response * @param request the current request * @param response the current response * @return the body that was passed in or a modified, possibly new instance */ T beforeBodyWrite(T body, MethodParameter returnType, MediaType selectedContentType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; selectedConverterType, ServerHttpRequest request, ServerHttpResponse response);&#125; 我们可以看到这里， 我们可以对 @ResponseBody 修饰的方法的返回值进行处理比如我们对返回结果进行国际化处理， 来个简单的例子: 12345678910111213141516171819202122232425262728293031@ControllerAdvice(annotations = RestController.class)public class SimpleResponseBodyAdvice implements ResponseBodyAdvice&lt;Result&gt; &#123; /** * 校验是否是需要的接入点 * @param returnType * @param converterType * @return */ public boolean supports(MethodParameter returnType, Class converterType) &#123; return returnType.getMethod().getReturnType().equals(Result.class); &#125; /** * 在往 outputStream 中写入返回结果之前，对返回结果进行处理 */ public Result beforeBodyWrite(Result body, MethodParameter returnType, MediaType selectedContentType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; selectedConverterType, ServerHttpRequest request, ServerHttpResponse response) &#123; return body.withRet(false).withData("被 ResponseBodyAdvice 修改的结果"); &#125;&#125;@RestControllerpublic class SimpleRestController &#123; @RequestMapping("/test/rest") public Result&lt;String&gt; testRest() &#123; return Result.success(true, "hello"); &#125;&#125; 看看结果：]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>Spring MVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC源码学习 —— MVC 配置加载过程]]></title>
    <url>%2F2016%2F12%2F01%2F%E6%8A%80%E6%9C%AF%2Fspring%2F2016-12-01-SpringMVC%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-mvc%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[SpringMVC 配置加载过程Spring 中加载配置文件的工具类是 xxxNamespaceHandler， 解析 SpringMVC 的配置文件，加载默认配置的工具类就是 MvcNameSpaceHandler。 下面来看看 MvcNameSpaceHandler 的源码： 12345678910111213141516171819202122public class MvcNamespaceHandler extends NamespaceHandlerSupport &#123; @Override public void init() &#123; // 注册 &lt;mvc:annotation-driven&gt; 配置标签解析类，具体解析内容见 [AnnotationDrivenBeanDefinitionParser] registerBeanDefinitionParser("annotation-driven", new AnnotationDrivenBeanDefinitionParser()); registerBeanDefinitionParser("default-servlet-handler", new DefaultServletHandlerBeanDefinitionParser()); registerBeanDefinitionParser("interceptors", new InterceptorsBeanDefinitionParser()); registerBeanDefinitionParser("resources", new ResourcesBeanDefinitionParser()); registerBeanDefinitionParser("view-controller", new ViewControllerBeanDefinitionParser()); registerBeanDefinitionParser("redirect-view-controller", new ViewControllerBeanDefinitionParser()); registerBeanDefinitionParser("status-controller", new ViewControllerBeanDefinitionParser()); registerBeanDefinitionParser("view-resolvers", new ViewResolversBeanDefinitionParser()); registerBeanDefinitionParser("tiles-configurer", new TilesConfigurerBeanDefinitionParser()); registerBeanDefinitionParser("freemarker-configurer", new FreeMarkerConfigurerBeanDefinitionParser()); registerBeanDefinitionParser("velocity-configurer", new VelocityConfigurerBeanDefinitionParser()); registerBeanDefinitionParser("groovy-configurer", new GroovyMarkupConfigurerBeanDefinitionParser()); registerBeanDefinitionParser("script-template-configurer", new ScriptTemplateConfigurerBeanDefinitionParser()); registerBeanDefinitionParser("cors", new CorsBeanDefinitionParser()); &#125;&#125; 下面是 AnnotationDrivenBeanDefinitionParser 中的 parse 方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169@Override public BeanDefinition parse(Element element, ParserContext parserContext) &#123; Object source = parserContext.extractSource(element); // 注册 `&lt;mvc:annotation-driven&gt;` 的组件信息 CompositeComponentDefinition compDefinition = new CompositeComponentDefinition(element.getTagName(), source); // 将 `&lt;mvc:annotation-driven&gt;` 的组建信息 push 到解析配置的上下文的 栈 中去 parserContext.pushContainingComponent(compDefinition); // 获取 `ContentNegotiationManager`, 如果 &lt;mvc:annotation-driven&gt; 中配置了 ContentNegotiationManager 的实现类名，则返回该实现的引用； // 如果没有配置，就会生成一个默认的 ContentNegotiationManager // ContentNegotiationManager: 1. 根据request 解析出 mediaType； 2. 根据 mediaType 解析出文件后缀名 RuntimeBeanReference contentNegotiationManager = getContentNegotiationManager(element, source, parserContext); // 创建 RequestMappingHandlerMapping 的Bean 定义 RootBeanDefinition handlerMappingDef = new RootBeanDefinition(RequestMappingHandlerMapping.class); handlerMappingDef.setSource(source); handlerMappingDef.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); // 默认 order 为0，优先使用这个 HandlerMapping handlerMappingDef.getPropertyValues().add("order", 0); handlerMappingDef.getPropertyValues().add("contentNegotiationManager", contentNegotiationManager); String methodMappingName = parserContext.getReaderContext().registerWithGeneratedName(handlerMappingDef); // 如果 &lt;mvc:annotation-driven&gt; 标签设置了 enable-matrix-variables 属性，在 RequestMappingHandlerMapping 中添加 removeSemicolonContent 属性 如果要使用 matrix-variales 属性， removeSemicolonContent 必须是 false if (element.hasAttribute("enable-matrix-variables")) &#123; Boolean enableMatrixVariables = Boolean.valueOf(element.getAttribute("enable-matrix-variables")); handlerMappingDef.getPropertyValues().add("removeSemicolonContent", !enableMatrixVariables); &#125; else if (element.hasAttribute("enableMatrixVariables")) &#123; Boolean enableMatrixVariables = Boolean.valueOf(element.getAttribute("enableMatrixVariables")); handlerMappingDef.getPropertyValues().add("removeSemicolonContent", !enableMatrixVariables); &#125; // 读取 &lt;mvc:annotation-driven&gt; 中的 path-matching 属性 // 读取 path-matching 中的 suffix-pattern、trailing-slash、registered-suffixes-only // 设置 RequestMappingHandlerMapping 的 useSuffixPatternMatch、useTrailingSlashMatch、useRegisteredSuffixPatternMatch // 读取 path-matching 中的 path-helper （UrlPathHelper 或其自定义子类的全限定名） // 设置 RequestMappingHandlerMapping 的 UrlPathHelper，如果 path-helper 不为空则在 ParserContext 中设置当前 UrlPathHelper 的别名为 mvcUrlPathHelper // 读取 path-matching 中的 path-helper // 设置 RequestMappingHandlerMapping 中的 pathMatcher 默认为 AntPathMatcher(ant 风格的路径匹配器)，如果 path-helper 不为空则在 ParserContext 中设置当前 UrlPathHelper 的别名为 mvcPathMatcher configurePathMatchingProperties(handlerMappingDef, element, parserContext); // RequestMappingHandlerMapping 设置 CorsConfiguration(跨域请求配置信息) 并且 将 corsConfigurations 注册到 ParserContext 中 RuntimeBeanReference corsConfigurationsRef = MvcNamespaceUtils.registerCorsConfigurations(null, parserContext, source); handlerMappingDef.getPropertyValues().add("corsConfigurations", corsConfigurationsRef); // 读取 path-matching 中的 conversion-service 获取 conversionService 并且注册到 ParserContext 中去 RuntimeBeanReference conversionService = getConversionService(element, source, parserContext); RuntimeBeanReference validator = getValidator(element, source, parserContext); RuntimeBeanReference messageCodesResolver = getMessageCodesResolver(element); RootBeanDefinition bindingDef = new RootBeanDefinition(ConfigurableWebBindingInitializer.class); bindingDef.setSource(source); bindingDef.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); bindingDef.getPropertyValues().add("conversionService", conversionService); bindingDef.getPropertyValues().add("validator", validator); bindingDef.getPropertyValues().add("messageCodesResolver", messageCodesResolver); // 获取配置信息中的 MessageConverter， 如果没有配置或者 register-defaults 为 true， 其实 register-defaults 默认为true // 也就是说不管你有没有配置 message-converters 属性 annotation-driven 都会为你注册这写 HttpMessageConverter: // ByteArrayHttpMessageConverter、StringHttpMessageConverter、ResourceHttpMessageConverter、SourceHttpMessageConverter、AllEncompassingFormHttpMessageConverter // 还有一些是否根据当前 classpath 中是否有对应的jar包才会添加的对应的 HttpMessageConverter ManagedList&lt;?&gt; messageConverters = getMessageConverters(element, source, parserContext); // 获取用户自定义的 HandlerMethodArgumentResolver， 解析方法参数 ManagedList&lt;?&gt; argumentResolvers = getArgumentResolvers(element, parserContext); // 获取用户自定义的 HandlerMethodReturnValueHandler， 处理方法的返回值 ManagedList&lt;?&gt; returnValueHandlers = getReturnValueHandlers(element, parserContext); String asyncTimeout = getAsyncTimeout(element); RuntimeBeanReference asyncExecutor = getAsyncExecutor(element); ManagedList&lt;?&gt; callableInterceptors = getCallableInterceptors(element, source, parserContext); ManagedList&lt;?&gt; deferredResultInterceptors = getDeferredResultInterceptors(element, source, parserContext); // 创建 RequestMappingHandlerAdapter 的 bean 定义 RootBeanDefinition handlerAdapterDef = new RootBeanDefinition(RequestMappingHandlerAdapter.class); handlerAdapterDef.setSource(source); handlerAdapterDef.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); handlerAdapterDef.getPropertyValues().add("contentNegotiationManager", contentNegotiationManager); handlerAdapterDef.getPropertyValues().add("webBindingInitializer", bindingDef); handlerAdapterDef.getPropertyValues().add("messageConverters", messageConverters); addRequestBodyAdvice(handlerAdapterDef); addResponseBodyAdvice(handlerAdapterDef); if (element.hasAttribute("ignore-default-model-on-redirect")) &#123; Boolean ignoreDefaultModel = Boolean.valueOf(element.getAttribute("ignore-default-model-on-redirect")); handlerAdapterDef.getPropertyValues().add("ignoreDefaultModelOnRedirect", ignoreDefaultModel); &#125; else if (element.hasAttribute("ignoreDefaultModelOnRedirect")) &#123; // "ignoreDefaultModelOnRedirect" spelling is deprecated Boolean ignoreDefaultModel = Boolean.valueOf(element.getAttribute("ignoreDefaultModelOnRedirect")); handlerAdapterDef.getPropertyValues().add("ignoreDefaultModelOnRedirect", ignoreDefaultModel); &#125; if (argumentResolvers != null) &#123; handlerAdapterDef.getPropertyValues().add("customArgumentResolvers", argumentResolvers); &#125; if (returnValueHandlers != null) &#123; handlerAdapterDef.getPropertyValues().add("customReturnValueHandlers", returnValueHandlers); &#125; if (asyncTimeout != null) &#123; handlerAdapterDef.getPropertyValues().add("asyncRequestTimeout", asyncTimeout); &#125; if (asyncExecutor != null) &#123; handlerAdapterDef.getPropertyValues().add("taskExecutor", asyncExecutor); &#125; handlerAdapterDef.getPropertyValues().add("callableInterceptors", callableInterceptors); handlerAdapterDef.getPropertyValues().add("deferredResultInterceptors", deferredResultInterceptors); String handlerAdapterName = parserContext.getReaderContext().registerWithGeneratedName(handlerAdapterDef); String uriCompContribName = MvcUriComponentsBuilder.MVC_URI_COMPONENTS_CONTRIBUTOR_BEAN_NAME; RootBeanDefinition uriCompContribDef = new RootBeanDefinition(CompositeUriComponentsContributorFactoryBean.class); uriCompContribDef.setSource(source); uriCompContribDef.getPropertyValues().addPropertyValue("handlerAdapter", handlerAdapterDef); uriCompContribDef.getPropertyValues().addPropertyValue("conversionService", conversionService); parserContext.getReaderContext().getRegistry().registerBeanDefinition(uriCompContribName, uriCompContribDef); RootBeanDefinition csInterceptorDef = new RootBeanDefinition(ConversionServiceExposingInterceptor.class); csInterceptorDef.setSource(source); csInterceptorDef.getConstructorArgumentValues().addIndexedArgumentValue(0, conversionService); RootBeanDefinition mappedCsInterceptorDef = new RootBeanDefinition(MappedInterceptor.class); mappedCsInterceptorDef.setSource(source); mappedCsInterceptorDef.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); mappedCsInterceptorDef.getConstructorArgumentValues().addIndexedArgumentValue(0, (Object) null); mappedCsInterceptorDef.getConstructorArgumentValues().addIndexedArgumentValue(1, csInterceptorDef); String mappedInterceptorName = parserContext.getReaderContext().registerWithGeneratedName(mappedCsInterceptorDef); // 创建 ExceptionHandlerExceptionResolver 的 Bean 定义 RootBeanDefinition exceptionHandlerExceptionResolver = new RootBeanDefinition(ExceptionHandlerExceptionResolver.class); exceptionHandlerExceptionResolver.setSource(source); exceptionHandlerExceptionResolver.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); exceptionHandlerExceptionResolver.getPropertyValues().add("contentNegotiationManager", contentNegotiationManager); exceptionHandlerExceptionResolver.getPropertyValues().add("messageConverters", messageConverters); exceptionHandlerExceptionResolver.getPropertyValues().add("order", 0); addResponseBodyAdvice(exceptionHandlerExceptionResolver); String methodExceptionResolverName = parserContext.getReaderContext().registerWithGeneratedName(exceptionHandlerExceptionResolver); RootBeanDefinition responseStatusExceptionResolver = new RootBeanDefinition(ResponseStatusExceptionResolver.class); responseStatusExceptionResolver.setSource(source); responseStatusExceptionResolver.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); responseStatusExceptionResolver.getPropertyValues().add("order", 1); String responseStatusExceptionResolverName = parserContext.getReaderContext().registerWithGeneratedName(responseStatusExceptionResolver); RootBeanDefinition defaultExceptionResolver = new RootBeanDefinition(DefaultHandlerExceptionResolver.class); defaultExceptionResolver.setSource(source); defaultExceptionResolver.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); defaultExceptionResolver.getPropertyValues().add("order", 2); String defaultExceptionResolverName = parserContext.getReaderContext().registerWithGeneratedName(defaultExceptionResolver); parserContext.registerComponent(new BeanComponentDefinition(handlerMappingDef, methodMappingName)); parserContext.registerComponent(new BeanComponentDefinition(handlerAdapterDef, handlerAdapterName)); parserContext.registerComponent(new BeanComponentDefinition(uriCompContribDef, uriCompContribName)); parserContext.registerComponent(new BeanComponentDefinition(exceptionHandlerExceptionResolver, methodExceptionResolverName)); parserContext.registerComponent(new BeanComponentDefinition(responseStatusExceptionResolver, responseStatusExceptionResolverName)); parserContext.registerComponent(new BeanComponentDefinition(defaultExceptionResolver, defaultExceptionResolverName)); parserContext.registerComponent(new BeanComponentDefinition(mappedCsInterceptorDef, mappedInterceptorName)); // Ensure BeanNameUrlHandlerMapping (SPR-8289) and default HandlerAdapters are not "turned off" // 创建默认的组件 BeanNameUrlHandlerMapping、HttpRequestHandlerAdapter、SimpleControllerHandlerAdapter MvcNamespaceUtils.registerDefaultComponents(parserContext, source); parserContext.popAndRegisterContainingComponent(); return null; &#125;]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>Spring MVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[几种分布式锁的实现方式]]></title>
    <url>%2F2016%2F10%2F26%2F%E6%8A%80%E6%9C%AF%2Fjava%2F2016-10-26-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[前言分布式是现在大多数程序必要的运行环境：单机服务如果服务器宕机了，向外提供的服务马上崩溃，整个系统陷入瘫痪；而如果存在多个服务器向外提供服务，只要还存在提供服务的服务器，系统就能正常运行 分布式系统向外提供服务时，很多情况下都会出现多个服务器共享某一资源的情况，存在自愿竞争； 如：商品系统提供了rpc 接口，功能是创建商品信息，这个rpc 接口的提供服务器有 A,B,C；这个创建商品信息的接口，在创建新的商品之前，会检测现在是否已经存在该商品，如果存在则返回已存在该商品，否则将新建商品rpc接口的消费者是E， 假设 E 第一次调用的时候调用的是A，在短时间内再次请求，创建相同的商品，这时候A 服务器还没处理完,假设请求分发到了B这时候 A 服务器还没有将商品信息持久化， B 服务器已经运行到了检查是否已经存在该商品， B 服务器没有查询到商品已存在，又创建了一个商品，此时商品的唯一性时效！ 123456789101112public Result createProduct() &#123; // 在持久化商品信息之前需要进行的操作 ...... // 检测商品信息是否存在 if (商品信息已存在) &#123; return Result.fail().withErrMsg("该商品已存在！"); &#125; else &#123; // 保存商品信息 saveProductInfo(); &#125; // ...... 商品保存后的逻辑操作&#125; 面对上述问题，在单机服务器上，我们很容易想到给 createProduct 加锁，使该方法实现线程安全； 将这个思路扩展到分布式系统，我们能否提供一个适用于分布式系统的锁？要实现分布式锁，我们需要借助工具 数据库 redis zookeeper 1. 依赖数据库实现分布式锁各种版本的数据库都实现了锁，这里以 mysql 的 InnoDB 存储引擎为例； InnoDB 的特性是：支持事务、支持行级锁、支持外键； InnoDB 提供了两种类型的行级锁： 1.共享锁(S): 允许一个事务去读一行数据，阻止其他事务获取相同数据集的排他写锁2.排他锁(X): 允许获得排他锁的事务修改一行数据，阻止其他事务获取相同数据集的共享读锁和排他写锁 对于 DELETE、UPDATE、INSERT语句，InnoDB会自动给涉及到的数据集加上排他锁；对于普通的 SELECT 语句，InnoDB 不会加上任务锁；而 事务 可以通过以下语句显式的给涉及到的数据集加上共享锁或者排他锁： 显式添加共享锁 1SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE; 显式添加排他锁 1SELECT * FROM table_name WHERE ... FOR UPDATE; FOR UPDATE 添加排他锁，后一个想要获取锁的事务，会等待前一个事务的完成之后才能获取排他锁 回到正题，我们上面所说的分布式锁，需要具备排他性；事务 通过 FOR UPDATE 添加排他锁正好满足我们的需求 123456def lock: set autocommit 0; // 取消事务的自动提交特性 select * from lock_table where lock_key = ? for update;def unlock: set autocommit 1; // 提交事务，解除行级锁 这里使用的 事务 + FOR UPDATE 添加排他锁 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.liam.distribute.lock.db;import com.liam.distribute.lock.DistributeLock;import com.liam.distribute.lock.LockException;import java.sql.*;import java.util.concurrent.TimeUnit;/** * Created by chaochun.chen on 16-10-27. */public class ForUpdateSimpleLock implements DistributeLock &#123; private ThreadLocal&lt;Connection&gt; localConnection = new ThreadLocal&lt;Connection&gt;(); private static final String FOR_UPDATE_SQL = "select * from lock_table where lock_key = ? for update"; public boolean tryLock(String lockKey) &#123; Connection connection = localConnection.get(); try &#123; connection.setAutoCommit(false); PreparedStatement statement = connection.prepareStatement(FOR_UPDATE_SQL); statement.setString(1, lockKey); ResultSet resultSet = statement.executeQuery(); if (resultSet.next()) &#123; return true; &#125; else &#123; return false; &#125; &#125; catch (SQLException e) &#123; // 忽略异常，默认为尝试加锁失败！ &#125; return false; &#125; public boolean lock(String lockKey) &#123; Connection connection = localConnection.get(); try &#123; connection.setAutoCommit(false); int count = 0; while (count &lt; 3) &#123; try &#123; PreparedStatement statement = connection.prepareStatement(FOR_UPDATE_SQL); statement.setString(1, lockKey); ResultSet resultSet = statement.executeQuery(); if (resultSet.next()) &#123; return true; &#125; &#125; catch (Exception e) &#123; // 此时如果抛出异常则重试！ &#125; TimeUnit.SECONDS.sleep(1); count++; &#125; &#125; catch (SQLException e) &#123; // 忽略异常，默认为尝试加锁失败！ &#125; catch (InterruptedException e) &#123; &#125; return false; &#125; public void unLock(String lockKey) &#123; Connection connection = localConnection.get(); try &#123; connection.setAutoCommit(true); &#125; catch (SQLException e) &#123; &#125; &#125; public void setConnection(Connection connection) &#123; localConnection.set(connection); &#125;&#125; 上面这种实现分布式锁的方法存在一些问题： 性能不是很高 这里加锁解锁依赖sql，必须面对sql超时问题，如果底层 jdbc 和 数据库之前的socket 超时了，此时connection 基本不可用，需要关闭；因此，在使用 Connection 的时候，推荐的使用方式是，将 Connection 的生命周期控制在一个方法内； 如果调用分布式锁的消费者宕机了，没有人去解锁，其他消费者将无法获取锁 没有可重入性 除了上面说的 依赖 事务 通过 FOR UPDATE 实现分布式锁； 我们还能通过 唯一索引 实现分布式锁 先看看建表语句： 1234567create table uniq_lock_table ( `id` INT (10) UNSIGNED NOT NULL AUTO_INCREMENT COMMENT '自增主键', `lock_key` VARCHAR (20) NOT NULL DEFAULT '' COMMENT '获取锁的key值', `expire_time` datetime NOT NULL DEFAULT '1970-01-01 00:00:00' COMMENT '失效时间', `create_time` datetime NOT NULL DEFAULT '1970-01-01 00:00:00' COMMENT '创建时间', UNIQUE KEY `uniq_lock_key`(`lock_key`)) ENGINE = INNODB CHARSET = 'utf8mb4' COMMENT = '使用唯一索引实现分布式锁的表' 简单定义一个dao 层接口 12345678910111213141516package com.liam.distribute.lock.mapper;import java.util.Date;/** * Created by chaochun.chen on 16-10-28. */public interface UniqLockTableMapper &#123; boolean insert(String lockKey, Date createTime, Date expireTime); boolean del(String lockKey); void deleteExpired();&#125; 分布式锁简单定义如下： 123456789def lock: exec sql: insert into uniq_lock_table(lock_key, create_time, expire_time) values(?, ?, ?); if result == 1: return ture; else : return false;def unlock: delete from uniq_lock_table where lock_key = ?; 用java 实现的代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package com.liam.distribute.lock.db;import com.liam.distribute.lock.DistributeLock;import com.liam.distribute.lock.mapper.UniqLockTableMapper;import com.liam.distribute.lock.mapper.UniqLockTableMapperImpl;import javax.naming.OperationNotSupportedException;import java.util.Date;import java.util.concurrent.TimeUnit;/** * Created by chaochun.chen on 16-10-27. */public class UniqIndexLock implements DistributeLock &#123; public boolean tryLock(String lockKey) &#123; UniqLockTableMapper uniqLockTableMapper = UniqLockTableMapperImpl.newInstance(); try &#123; boolean insert = uniqLockTableMapper.insert(lockKey, new Date(), new Date()); if (insert) &#123; return true; &#125; &#125; catch (Exception e) &#123; // 忽略此异常，视为加锁失败！ &#125; return false; &#125; public boolean lock(String lockKey) &#123; UniqLockTableMapper uniqLockTableMapper = UniqLockTableMapperImpl.newInstance(); try &#123; while (true) &#123; try &#123; boolean insert = uniqLockTableMapper.insert(lockKey, new Date(), new Date()); if (insert) &#123; return true; &#125; &#125; catch (Exception e) &#123; // 忽略此异常，视为加锁失败！ &#125; &#125; &#125; catch (Exception e) &#123; &#125; return false; &#125; public boolean tryLock(String lockKey, TimeUnit timeUnit, long expireTime) throws OperationNotSupportedException &#123; return false; &#125; public boolean lock(String lockKey, TimeUnit timeUnit, long expireTime) throws OperationNotSupportedException &#123; return false; &#125; public void unLock(String lockKey) &#123; UniqLockTableMapper uniqLockTableMapper = UniqLockTableMapperImpl.newInstance(); uniqLockTableMapper.del(lockKey); &#125;&#125; 上面这种方法的缺点很明显： 分布式锁不具备可重入性 如果某一消费者在获取锁之后宕机了，其他消费者无法获取锁 为了解决 上面反复提到的 失效的锁的问题，我们在获取锁的时候新增时效时间！ 12345678910111213def lock: exec clear_expire_time; exec sql: insert into uniq_lock_table(lock_key, create_time, expire_time) values(?, ?, ?); if result == 1: return ture; else : return false;def clear_expire_time: delete from uniq_lock_table where now() &gt; expire_time;def unlock: delete from uniq_lock_table where lock_key = ?; java 实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package com.liam.distribute.lock.db;import com.google.common.base.Function;import com.google.common.collect.FluentIterable;import com.google.common.collect.ImmutableList;import com.google.common.collect.Iterables;import com.liam.distribute.lock.DistributeLock;import com.liam.distribute.lock.mapper.UniqLockTableMapper;import com.liam.distribute.lock.mapper.UniqLockTableMapperImpl;import com.liam.distribute.lock.model.UniqLockTable;import javax.naming.OperationNotSupportedException;import java.util.Date;import java.util.List;import java.util.concurrent.TimeUnit;/** * Created by chaochun.chen on 16-10-28. */public class UniqIndexTimeLock implements DistributeLock &#123; private static final long DEFAULT_EXPIRE_TIMES = 2 * 1000; // 默认锁时效时间是两秒！ public boolean tryLock(String lockKey) &#123; try &#123; return tryLock(lockKey, TimeUnit.MILLISECONDS, DEFAULT_EXPIRE_TIMES); &#125; catch (OperationNotSupportedException e) &#123; return false; &#125; &#125; public boolean lock(String lockKey) &#123; try &#123; return lock(lockKey, TimeUnit.MILLISECONDS, DEFAULT_EXPIRE_TIMES); &#125; catch (OperationNotSupportedException e) &#123; return false; &#125; &#125; public boolean tryLock(String lockKey, TimeUnit timeUnit, long expireTime) throws OperationNotSupportedException &#123; UniqLockTableMapper uniqLockTableMapper = UniqLockTableMapperImpl.newInstance(); long expireMillis = timeUnit.toMillis(expireTime); try &#123; clearExpiredLock(); long cur = System.currentTimeMillis(); Date now = new Date(cur); Date expired = new Date(cur + expireMillis); boolean insert = uniqLockTableMapper.insert(lockKey, now, expired); if (insert) &#123; return true; &#125; &#125; catch (Exception e) &#123; // 忽略此异常，视为加锁失败！ &#125; return false; &#125; public boolean lock(String lockKey, TimeUnit timeUnit, long expireTime) throws OperationNotSupportedException &#123; UniqLockTableMapper uniqLockTableMapper = UniqLockTableMapperImpl.newInstance(); long expireMillis = timeUnit.toMillis(expireTime); long currentTimeMillis = System.currentTimeMillis(); Date now = new Date(currentTimeMillis); Date expired = new Date(currentTimeMillis + expireMillis); try &#123; clearExpiredLock(); while (true) &#123; try &#123; boolean insert = uniqLockTableMapper.insert(lockKey, now, expired); if (insert) &#123; return true; &#125; &#125; catch (Exception e) &#123; // 忽略此异常，视为加锁失败！ &#125; &#125; &#125; catch (Exception e) &#123; &#125; return false; &#125; public void unLock(String lockKey) &#123; UniqLockTableMapper uniqLockTableMapper = UniqLockTableMapperImpl.newInstance(); uniqLockTableMapper.del(lockKey); &#125; private void clearExpiredLock() &#123; UniqLockTableMapper uniqLockTableMapper = UniqLockTableMapperImpl.newInstance(); uniqLockTableMapper.deleteExpired(); &#125;&#125; 上面这些方法都没能实现分布式锁的可重入性，这里需要新增一个字段，标明分布式锁是谁加的, 再来看看新增分布式锁调用方标识之后的分布式锁定义 1234567891011def lock: clear_expire_time; exec sql: insert into uniq_lock_table(lock_key, create_time, expire_time, biz_uniq_code) values(?, ?, ?, ?); if result == 1 : return true; else : exec sql : select * from uniq_lock_table where lock_key = ? and biz_uniq_code = ?; if query_result &gt; 0 : return true; else : return false; 在使用上面这种方式实现可重入性的时候，需要将表中的唯一索引修改一下 1alter table add UNIQUE INDEX `uniq_lock_key_biz_uniq_code`(`lock_key`, `biz_uniq_code`); 注意： 上面说的这些依赖数据库实现分布式锁，都要避免单数据库示例的问题，如果只有一个数据库示例，而数据库宕机了，分布式锁将不能提供服务；因此，分布式锁依赖的数据库必须配置多数据库实例，利用数据库的主从复制逻辑，保证数据同步！2. 依赖redis实现分布式锁redis 作为内存数据存储系统，相比数据库具有更好的高可用性先来看看一个 redis 命令, setnx \&gt; help setnx SETNX key value TIME COMPLEXITY: O(1) RETURN VALUE: Integer reply, specifically: 1 if the key was set 0 if the key was not set从帮助信息能看明白， setnx 功能和 set指令类似， 不同在于 setnx 只有插入的key 不存在的时候才能插入成功，成功之后返回 1， 失败返回 0我们依赖 setnx 实现分布式锁，定义如下： 123456789def lock: exec cmd: setnx lock_key lock_value; if result == 1 : return true; else result = 0 : return false;def unlock: del lock_key; 上面的定义存在的问题有: 调用分布式锁的消费者如果在获取锁之后宕机了，这个失效锁将导致其他消费者无法获取锁 分布式锁不具备可重入性 为了提供更好用的分布式锁，我们必须给分布式锁加上时效性下面来看看 redis 的 set 指令 &gt; help set SET key value TIME COMPLEXITY: O(1) Options EX seconds -- Set the specified expire time, in seconds. PX milliseconds -- Set the specified expire time, in milliseconds. NX -- Only set the key if it does not already exist. XX -- Only set the key if it already exist. RETURN VALUE: Status code reply: OK if SET was executed correctly. Null multi-bulk reply: a Null Bulk Reply is returned if the SET operation was not performed becase the user specified the NX or XX option but the condition was not met.从这个可以看出来，set 指令可以做到 setnx 指令的目的，如下： 1set key value NX 再看看下面的指令 12set key value EX 10 NX; //插入一个 k-v 数据，只有key不存在的时候才能插入成功，插入成功好，10s 内数据有效，10s 后数据失效将被删除set key value PX 100 NX; //插入一个 k-v 数据，只有key不存在的时候才能插入成功，插入成功好，10 毫秒内数据有效，10 毫秒后数据失效将被删除 加入有效时间后，分布式锁不需要在每次加锁之前清除无效锁, 得到优化的分布式锁如下： 123456789def lock: exec: set lock_key lock_value PX expire_time NX; // 这里选择时效时间单位为毫秒，增加锁有效时间的精确性 if result == OK : return true; else : return false;def unlock: del lock_key; 上面的分布式锁还是没有实现可重入性， 改进如下： 12345678910def lock: exec: set lock_key biz_uniq_code PX expire_time NX; // 插入的value 是 biz_uniq_code, 可以唯一表示加锁的调用方 if result == OK : return true; else : exec: get lock_key if value == biz_uniq_code : return true; else : return false; 依赖 redis 实现 分布式锁 存在一个难以解决的问题，redis 不能保证数据的 强一致性， 因为； redis 集群使用异步复制如果在加锁的时候 redis 的 master 宕机了，异步复制到 slave 失败了，加锁就失败了！ 为了解决这个问题，redis 的作者提出了 redLock RedLock这里不对 RedLock 进行介绍，详情见 并发编程网 翻译http://ifeve.com/redis-lock/ 3. 依赖zookeeper 实现分布式锁ZooKeeper 能被用来实现分布式锁的原因取决于他的以下几个特性： ZooKeeper 的视图结构和 unix 系统的文件系统类似，都是采用树结构，不同的是 ZooKeeper 的树结构上是新定义的 数据节点 —— ZNode，ZNode 是 ZooKeeper 中数据的最小单元，可以保存数据，也可以挂靠子节点； ZooKeeper 中的 ZNode 的类型分为 持久节点、 临时节点、 顺序节点 三大类型；通过组合使用可以生成四种节点：1) 持久节点：是指该数据节点被创建之后，就一直存在于 ZooKeeper 服务器上，直到有删除操作来主动清除这个节点；2) 持久有序节点：和 持久节点 的基本特性一致，不同的特性在于顺序性上；在 ZooKeeper 中父节点会为它的第一级节点维护一份顺序，用于记录下每个子节点创建的先后顺序。3) 临时节点：和 持久节点 不同的是，临时节点的生命周期和客户端的会话相关，如果客户端会话失效了，临时节点 将被自动清除。4) 临时有序节点：和 临时节点 的基本特性一致，不同的特性也在于顺序性上； ZooKeeper 机制规定：同一个目录下只能有一个唯一的文件名。例如：我们在 Zookeeper 目录一个目录下创建，两个客户端创建一个名为 new_node 节点，只有一个能够成功。 ZooKeeper 提供 Watcher 机制，客户端可以在服务端注册一个 Watcher 监听，当服务端的一些指定事件触动了这个 Watcher，就会向客户端发送通知 获取锁依赖上述特性，我们可以在 ZooKeeper 的树结构上，创建一个临时节点 /distribute_lock/lock, 只要有一个客户端创建了节点，表示该客户端获取到了锁；而其他没有获取到锁的客户端，需要到 ZooKeeper 的 /distribute_lock 节点上注册一个子节点变更的 Watcher 监听，以便见听到子节点的变更情况。 — distribute_lock — lock 释放锁1) 客户端执行完业务逻辑后，主动删除自己创建的临时节点；2) 客户端宕机后，ZooKeeper 和客户端之间的对话失效、连接断开，客户端创建的临时节点将被自动删除3) 客户端释放锁之后，其他客户端通过 Watcher 监控得到锁被释放的通知，而来竞争获取锁]]></content>
      <categories>
        <category>分布式锁</category>
      </categories>
      <tags>
        <tag>分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis 源码阅读(一)]]></title>
    <url>%2F2016%2F09%2F20%2F%E6%8A%80%E6%9C%AF%2Fmybatis%2F2016-09-20-mybatis%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[前言我们开始从mybatis 启动流程开始阅读源码 mybatis 的配置信息层次结构： configuration properties settings typeAliases typeHandlers objectFactory objectWrapperFactory reflectorFactory plugins environments transactionManager dataSource databaseIdProvider mappers 无论是结合 Spring 使用 Mybatis 还是单独使用，在启动的时候都需要经过这个入口 SqlSessionFactoryBuilder 来构建 SqlSessionFactory 使用 SqlSessionFactoryBuilder 构建 SqlSessionFactory 有两种方：编码方式，Xml 配置方式； 获取 SqlSessionFactory 的主体调用流程如下：org.apache.ibatis.session.SqlSessionFactoryBuilder#build(Reader or InputStream);org.apache.ibatis.builder.xml.XMLConfigBuilder#XMLConfigBuilder();org.apache.ibatis.builder.xml.XMLConfigBuilder#parseorg.apache.ibatis.builder.xml.XMLConfigBuilder#parseConfigurationorg.apache.ibatis.session.SqlSessionFactoryBuilder#build(org.apache.ibatis.session.Configuration)得到的 SqlSessionFactory 的默认实现 DefaultSqlSessionFactory 编码方式这里不做赘述， 配置文件的方式主要流程如下得到 Configuration xml 配置方式主要依赖 XMLConfigBuilder 来解析配置文件 private void parseConfiguration(XNode root) { try { //issue #117 read properties first //解析 xml 中 &lt;properties&gt; 标签，主要是解析url 和 resource， 需要注意的是这两个配置不能同时存在 propertiesElement(root.evalNode(&quot;properties&quot;)); // 注册 type aliase 可以使用 package 扫描一个文件夹中的所有 带@Alias 注解的所有类 // [注意] alias 会忽略大小写 toLowCase typeAliasesElement(root.evalNode(&quot;typeAliases&quot;)); pluginElement(root.evalNode(&quot;plugins&quot;)); // MyBatis uses an `ObjectFactory` to create all needed new Objects. objectFactoryElement(root.evalNode(&quot;objectFactory&quot;)); // objectWrapperFactoryElement(root.evalNode(&quot;objectWrapperFactory&quot;)); reflectionFactoryElement(root.evalNode(&quot;reflectionFactory&quot;)); settingsElement(root.evalNode(&quot;settings&quot;)); // read it after objectFactory and objectWrapperFactory issue #631 // environmentsElement(root.evalNode(&quot;environments&quot;)); // 通过xml 中设置的type 别名构建 DatabaseIdProvider， 默认为 VendorDatabaseIdProvider // 再通过 DatabaseIdProvider 获取 databaseId， 设置到 Configuration.databaseId databaseIdProviderElement(root.evalNode(&quot;databaseIdProvider&quot;)); // TypeHander 对应一个 Class 以及一个 JdbcType // 注册 TypeHander 时可以一个个的注册，也可以注册一个 package 中所有实现了 `TypeHandler` 接口的类 // 通过继承 BaseTypeHandler 自定义的TypeHander // TypeHandler 如果没有通过 @MappedJdbcTypes 指定的JdbcType 则为null typeHandlerElement(root.evalNode(&quot;typeHandlers&quot;)); mapperElement(root.evalNode(&quot;mappers&quot;)); } catch (Exception e) { throw new BuilderException(&quot;Error parsing SQL Mapper Configuration. Cause: &quot; + e, e); } }mybatis 的重要组成部分是 mapper xml 文件，对于这些文件的解析很重要解析 package 中的接口时，获取 package 下的所有接口， 依次直接使用 MapperRegister 注册 Interface –&gt; MapperProxyFactory 使用 标签中的 mapperClass 配置接口信息， 也是 MapperRegister 注册 Interface –&gt; MapperProxyFactory 使用 标签中的 url 或 resource 配置 mapper xml 文件的路径， 使用 XmlMappperBuilder 解析 xml 文件 这里先介绍 MapperRegister MapperProxyFactory MapperRegister 中保存了 Interface –&gt; MapperProxyFactory 的map mybatis 中大量使用了设计模式， MapperProxyFactory 使用了工厂模式MapperProxyFactory 一看就是生产 MapperProxy 的工厂类， 提供 org.apache.ibatis.binding.MapperProxyFactory#newInstance(org.apache.ibatis.session.SqlSession)方法获取 Mapper Interface 的实现类 MapperProxy 是java 动态代理的使用，MapperProxy 实现了 InvocationHandler 接口 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * Copyright 2009-2015 the original author or authors. * * Licensed under the Apache License, Version 2.0 (the "License"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package org.apache.ibatis.binding;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.util.Map;import java.util.concurrent.ConcurrentHashMap;import org.apache.ibatis.session.SqlSession;/** * @author Lasse Voss */public class MapperProxyFactory&lt;T&gt; &#123; // MapperProxy 代理的 Mapper 接口 private final Class&lt;T&gt; mapperInterface; // 这里的Key 是 Mapper 接口中方法对象，Value 是方法对象的封装 private final Map&lt;Method, MapperMethod&gt; methodCache = new ConcurrentHashMap&lt;Method, MapperMethod&gt;(); public MapperProxyFactory(Class&lt;T&gt; mapperInterface) &#123; this.mapperInterface = mapperInterface; &#125; public Class&lt;T&gt; getMapperInterface() &#123; return mapperInterface; &#125; public Map&lt;Method, MapperMethod&gt; getMethodCache() &#123; return methodCache; &#125; @SuppressWarnings("unchecked") protected T newInstance(MapperProxy&lt;T&gt; mapperProxy) &#123; return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[] &#123; mapperInterface &#125;, mapperProxy); &#125; // 通过 Sqlsession 获取 Mapper 接口 public T newInstance(SqlSession sqlSession) &#123; final MapperProxy&lt;T&gt; mapperProxy = new MapperProxy&lt;T&gt;(sqlSession, mapperInterface, methodCache); return newInstance(mapperProxy); &#125;&#125; Configuration: mybatis 配置信息的根节点 public Configuration() { // 这里注册了很多类的别名，在需要的时候通过反射机制获取 类的实例 typeAliasRegistry.registerAlias(&quot;JDBC&quot;, JdbcTransactionFactory.class); typeAliasRegistry.registerAlias(&quot;MANAGED&quot;, ManagedTransactionFactory.class); typeAliasRegistry.registerAlias(&quot;JNDI&quot;, JndiDataSourceFactory.class); typeAliasRegistry.registerAlias(&quot;POOLED&quot;, PooledDataSourceFactory.class); typeAliasRegistry.registerAlias(&quot;UNPOOLED&quot;, UnpooledDataSourceFactory.class); typeAliasRegistry.registerAlias(&quot;PERPETUAL&quot;, PerpetualCache.class); typeAliasRegistry.registerAlias(&quot;FIFO&quot;, FifoCache.class); typeAliasRegistry.registerAlias(&quot;LRU&quot;, LruCache.class); typeAliasRegistry.registerAlias(&quot;SOFT&quot;, SoftCache.class); typeAliasRegistry.registerAlias(&quot;WEAK&quot;, WeakCache.class); typeAliasRegistry.registerAlias(&quot;DB_VENDOR&quot;, VendorDatabaseIdProvider.class); typeAliasRegistry.registerAlias(&quot;XML&quot;, XMLLanguageDriver.class); typeAliasRegistry.registerAlias(&quot;RAW&quot;, RawLanguageDriver.class); typeAliasRegistry.registerAlias(&quot;SLF4J&quot;, Slf4jImpl.class); typeAliasRegistry.registerAlias(&quot;COMMONS_LOGGING&quot;, JakartaCommonsLoggingImpl.class); typeAliasRegistry.registerAlias(&quot;LOG4J&quot;, Log4jImpl.class); typeAliasRegistry.registerAlias(&quot;LOG4J2&quot;, Log4j2Impl.class); typeAliasRegistry.registerAlias(&quot;JDK_LOGGING&quot;, Jdk14LoggingImpl.class); typeAliasRegistry.registerAlias(&quot;STDOUT_LOGGING&quot;, StdOutImpl.class); typeAliasRegistry.registerAlias(&quot;NO_LOGGING&quot;, NoLoggingImpl.class); typeAliasRegistry.registerAlias(&quot;CGLIB&quot;, CglibProxyFactory.class); typeAliasRegistry.registerAlias(&quot;JAVASSIST&quot;, JavassistProxyFactory.class); // 注册语言管理驱动，默认语言管理驱动为：`XMLLanguageDriver` languageRegistry.setDefaultDriverClass(XMLLanguageDriver.class); languageRegistry.register(RawLanguageDriver.class); }ObjectFactory: Reflector ObjectWrapper TransactionFactory TypeAliasRegistry LanguageRegistry]]></content>
      <categories>
        <category>MyBatis</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线上应用故障排查]]></title>
    <url>%2F2016%2F09%2F13%2F%E6%8A%80%E6%9C%AF%2Fjava%2F2016-09-13-CPU%E5%8D%A0%E7%94%A8%E8%BF%87%E9%AB%98%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[CPU 100% 处理流程当我们遇到 CPU 使用率过高的情况时(这里只考虑 Linux/Solaris 系统)首先想到了 top 指令 &gt; top先看看哪个进程使用的cpu 比较高, 当然这里只讲 JVM 使用 CPU 过高的情况; 这里的图片只是示例用，不存在cpu 使用率过高的问题 在查询到 vmid 后，查询虚拟机进程中哪个线程使用的 cpu 比较高 &gt; ps -mp pid -o THREAD,tid,time确定了使用 cpu 过高的虚拟机线程 id 后，记录下 threadId 然后再使用 jstack 记录当前虚拟机线程信息快照 $ jstack -l vmid &gt; outofcpu.threaddump 这里已经获得了 threadId 和 当前的虚拟机线程信息快照信息 我们可以考虑重启一下 tomcat 或者直接重启 JVM 对于保存的 threadId 和 outofcpu.threaddump 在 threaddump 文件中查看 threadId 线程信息定位代码问题，达到修复目的 内存使用过高处理流程当我们遇到 内存 使用率过高的情况时(这里只考虑 Linux/Solaris 系统)首先想到了还是 top 指令 我们看到这里的tomcat 占用内存大到了 60% 多， 记录下 vmid 下面我们尝试查看这个进程中具体线程的内存使用情况这里可以看出 ps 指令无法查询进程中具体线程的内存适用情况 不过我们还有 java 提供的工具 jmap &gt; sudo jmap -F -histo 12788 &gt; test.dump这里使用 jmap -histo 指令去 统计 JVM 堆中的对象信息 &gt; sudo jmap -F -dump:format=b,file=test.all.dump 12788这里使用 jmap -dump 指令去生成JVM 堆存储快照， 后面在用 jhat visualvm 等工具进行分析 在分析之前，我们可以考虑一下是否需要重启服务 这里在介绍一下 ps 的具体使用 -m: 显示当前 process id 进程下的线程的信息-o: 用户自定义显示列]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM 性能监控和故障处理工具]]></title>
    <url>%2F2016%2F09%2F12%2F%E6%8A%80%E6%9C%AF%2Fjava%2F2016-09-12-JVM%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E5%92%8C%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[jps (JVM Process Status Tool)显示指定系统中的所有虚拟机进程&gt; jps [ options ] [ hostid ] jps 选项 [ options ] 说明：-q 只输出LVMID 省略主类的名称；-m 输出虚拟机进程启东市传递给主类main函数的参数；-l 输出主类的名称，如果进程执行的是jar包，输出jar 路径；-v 输出虚拟机进程启动时的JVM 参数 [ 注意 ]使用 jps 指令得到的输出结果可能是下面这样的 pid -- process information unavailableJVM 会在本地文件系统中的 tmp 文件夹中新建 hsperfdata_{username}/{pid} 文件， 如果当前执行 jps 指令的用户没有属于自己的这个文件，就会出现这种提示 例如： 我们公司tomcat 的启动用户默认是 tomcat, JVM 新建的文件是 /tmp/hsperfdata_tomcat/pid(这是在linux系统中)，这样导致了如下问题 不管使用 root 用户权限还是 普通用户权限都不能执行 jps 指令上述问题解决方案sudo -u tomcat jps pid jstat (JVM Statistics Monitoring Tool)&gt; jstat [ option vmid [interval [s|ms] [count]] ]参数 interval, count 代表查询的间隔时间和次数，如果省略这两个参数表示只查询一次假设需要每 1s 查询一次进程 3234 的jvm gc信息，一共查询10次 &gt; jstat -gc 3234 1s 10 选项 作用描述 -class 监视类装载、卸载数量、总空间以及类装载所耗费的时间 -gc 监视 JVM 堆状况，包括 Eden区、两个survivor 区、老年代、永生代等的容量、已用空间、GC时间合计等信息 -gccapacity 监视内容与 -gc 基本相同，但输出主要关注java 堆各个区域使用到的最大、最小空间 -gcutil 监视内容与 -gc 基本相同，但输出主要关注已经使用空间占总空间的百分比 -gccause 与 -gcutil 功能一样，但是会额外输出导致上一次 GC 产生的原因 -gcnew 监视新生代 gc 状况 -gcnewcapacity 监视内容与 -gcnew 基本相同，但输出主要关注新生代已经使用到的最大、最小空间 -gcold 监视老年代 gc 状况 -gcoldcapacity 监视内容与 -gcold 基本相同，但输出主要关注老年代已经使用到的最大、最小空间 -gcpermcapacity 输出永久代已经使用到的最大、最小空间 -compiler 输出JIT 编译器编译过的方法、耗时等信息 -gccompilation 输出已经呗 JIT 编译的方法 jinfo (Configuration Info for java)实时查询和调整虚拟机各项参数&gt; jinfo [ option ] pid liam@liamchen-ubuntu:~$ jinfo -help Usage: jinfo [option] &lt;pid&gt; (to connect to running process) jinfo [option] &lt;executable &lt;core&gt; (to connect to a core file) jinfo [option] [server_id@]&lt;remote server IP or hostname&gt; (to connect to remote debug server) where &lt;option&gt; is one of: -flag &lt;name&gt; to print the value of the named VM flag -flag [+|-]&lt;name&gt; to enable or disable the named VM flag -flag &lt;name&gt;=&lt;value&gt; to set the named VM flag to the given value -flags to print VM flags -sysprops to print Java system properties &lt;no option&gt; to print both of the above -h | -help to print this help message jmap (Memory Map for Java)生成对存储快照&gt; jmap [ option ] vmid 选项 作用描述 -dump 生成 JVM 堆存储快照。 格式为: dump:[live,]format=b,file=，其中live参数说明是否只dump 出存活的对象 -finalizeinfo 显示在F-Queue 中等待Finalizer线程执行 finalize 方法的对象，只在 linux/solaris 平台下有效 -heap 显示 JVM 堆详细信息，如使用那种回收器、参数配置、分代情况等。只在 linux/solaris 平台下有效 -histo 显示堆中对象统计信息，包括类、实例数量、合计数量 -permstat 以ClassLoader为统计口径，显示永生代内存状态，只在 linux/solaris 平台下有效 -F 当 JVM 进程对 -dump 指令选项没有响应时，可使用这个选项强制生成dump 快照，只在 linux/solaris 平台下有效 使用示例： &gt; jmap -dump:format=b,file=test.dump pid jhat (JVM Heap Analysis Tool)与jmap搭配使用， 来分析 jmap 生成的 堆存储快照文件&gt; jhat test.dump当屏幕上出现 “Server is ready” 提示后，访问 http://localhost:7000 就能看到分析结果 [ 注意 ] : 分析工作比较消耗资源，可将dump 文件复制到其他机器上分析 jstack (Stack Trace for Java)生成JVM 当前时刻的线程快照&gt; jstack [ option ] vmid 选项 作用描述 -l 除堆栈外，显示关于锁的附加信息 -m 打印 -F 当 JVM 进程对指令没有响应时，可使用这个选项强制生成 threaddump 快照，只在 linux/solaris 平台下有效 GC 日志日志设置1JAVA_OPTS="-Xms8g -Xmx16g -XX:PermSize=512m -XX:MaxPermSize=512m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=../logs -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:../logs/gc.log"]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql explain 介绍]]></title>
    <url>%2F2016%2F09%2F08%2F%E6%8A%80%E6%9C%AF%2Fmysql%2F2016-09-08-mysql%20explain%20%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[mysql 为我们提供了一个很好用的 sql 语句分析工具 explain , 它可以帮助我们选择更好的索引和写出更优化的查询语句。 explain 会显示sql 语句如何使用索引 注意：在 mysql 5.6 之前 explain 只能分析 select 语句，其他类型的语句只能先改写为 select 语句再分析； 但是 mysql 5.6 及以后， explain 支持其他语句的解析！！！先来个使用范例 exlain select sql 如： explain select * from grade_info where teacher_id = &apos;1&apos;;下面看看 explain 的执行结果：explain 的结果有这些列 id, select_type, table, partitions, type, possible_keys, key, key_len, ref, rows, filtered, Extra explain 输出字段概要说明 列名 说明 id sql 语句中多个嵌套语句的唯一标识，如果只有一条 sql 始终为 1 select_type select 语句的类型，或其他语句的类型（较高版本中 explain 支持 delete,update,insert 等语句） table 表名称，或者 sql 中表的别名 partitions 匹配的分区 type 连接类型 possible_keys 可能被用到的索引名称 key 实际被使用的索引的名称 key_len 被选用的索引实际被使用的长度 ref 和索引比较的列，todo rows 将要被检查的行数的估计值 filtered 被“表”条件过滤的行数的百分比 Extra 附加信息 idsql 语句执行顺序号， 就是sql 语句的执行顺序这里可以看到 id 的变化 select_typeselect 语句的类型 取值 说明 SIMPLE 简单的查询语句（没有子查询，也没有 UNION） PRIMARY –]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql 索引]]></title>
    <url>%2F2016%2F09%2F07%2F%E6%8A%80%E6%9C%AF%2Fmysql%2F2016-09-07-mysql%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE%2F</url>
    <content type="text"><![CDATA[Mysql 索引索引的分类mysql 索引分为 聚簇索引 和 非聚簇索引聚簇索引：数据的物理存储顺序和索引的顺序是一致的非聚簇索引：数据的物理存储顺序和索引的顺序不一致很明显数据的物理存储顺序只有一种，所以 聚簇索引 只能有一个 关于联合索引123456789CREATE TABLE `grade_info` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT COMMENT '自增主键', `grade` double NOT NULL DEFAULT '0' COMMENT '成绩', `teacher_id` varchar(20) NOT NULL DEFAULT '' COMMENT '教师id', `class_id` varchar(20) NOT NULL DEFAULT '' COMMENT '课程id', `student_id` varchar(20) NOT NULL DEFAULT '' COMMENT '学生id', PRIMARY KEY (`id`), KEY `grade_info_teacher_id_IDX` (`teacher_id`,`class_id`,`student_id`)) ENGINE=InnoDB AUTO_INCREMENT=30 DEFAULT CHARSET=utf8mb4 先创建一个表，这个表中有个联合索引 grade_info_teacher_id_IDX 下面插入几条数据： 下面开始讨论下面几个问题: `grade_info_teacher_id_IDX` (teacher_id, class_id, student_id) 这个索引如果查询条件只有teacher_id，会用上索引么？&lt;br /&gt; 如果查询条件中有teacher_id, student_id会用上索引么？&lt;br /&gt; 如果查询条件中有class_id, student_id会用上索引么？&lt;br /&gt; 如果查询条件中有teacher_id, class_id会用上索引么？&lt;br /&gt; 很明显当查询条件中使用了联合索引的第一列的时候会用上索引 这里当查询条件使用联合索引的第一列和第三列的时候也会用上索引，但是这里有个问题， 图中的查询条件只会查询出一条，但是现在问题出现了， 但explain 结果中的row 是 2， 这里得知 索引只用到了第一列 很明显，这种查询条件没用上索引 这里使用到了索引，而且索引的长度和 只使用第一列的时候相比更长； 可以确定这里使用了索引的前两列 索引的限制这里说的索引的限制将能解释上面联合索引的问题 1. 如果不是按照索引的最左列开始查询，则无法使用索引如果查询条件中有class_id, student_id会用上索引么？这里就能回答这个问题，不会； 2. 不能跳过索引中的列如果查询条件中有teacher_id, student_id会用上索引么？这里跳过了第二列，这个时候只使用索引的第一列 3. 如果查询中有某列的范围查询，则其右边的所有列的查询都不能使用索引优化查找从 explain 中 row 是 3， 但是查询到结果只有一个，而只按照第一列范围查询结果是3，这个例子可以印证上述限制 索引的选择突然发现一个问题，mysql 是如何选择索引的？ 下面看看这个表： 1234567891011DROP TABLE IF EXISTS `school`;CREATE TABLE `school` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;主键&apos;, `name` varchar(20) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;学校名称&apos;, `phone` varchar(20) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;学校电话&apos;, `addr` varchar(255) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;学校地址&apos;, `email` varchar(30) NOT NULL DEFAULT &apos;&apos;, PRIMARY KEY (`id`), KEY `idx_name_phone_email` (`name`,`phone`,`email`), KEY `idx_name_phone` (`name`,`phone`)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8 COMMENT=&apos;学校&apos; idx_name_phone_email 和 idx_name_phone 这两个索引，设置的 极不合理 ，这里 不推荐使用 ， 但是这种情况下，mysql 会选择哪个索引呢？ 插入一条测试数据 1desc select * from school where name = '重庆邮电大学' and phone = '76822313'\G 执行一下上面这个 sql 语句，两个索引都可能被用到，会是哪一个呢？ 这里我们能看到，使用的是 idx_name_phone_email 这里做一个简单的猜想，为什么使用的是 idx_name_phone_email, 这和索引的声明顺序有关吗？ 重新建表看看： 1234567891011DROP TABLE IF EXISTS `school`;CREATE TABLE `school` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '主键', `name` varchar(20) NOT NULL DEFAULT '' COMMENT '学校名称', `phone` varchar(20) NOT NULL DEFAULT '' COMMENT '学校电话', `addr` varchar(255) NOT NULL DEFAULT '' COMMENT '学校地址', `email` varchar(30) NOT NULL DEFAULT '', PRIMARY KEY (`id`), KEY `idx_name_phone` (`name`,`phone`), KEY `idx_name_phone_email` (`name`,`phone`,`email`)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8 COMMENT='学校' 修改了索引定义顺序之后， 使用的索引也随之改变了 好像，我们的猜想被证实了 如果我们需要制定使用哪个索引呢？ force index(idx_name): 使用这个关键字强制指定要用的索引即可 还有其他方式吗？ 可以 use index(idx_name) 推荐使用的索引 use index 和 force index 的区别If you use USE INDEX then you RECOMMEND optimizer to use this index, but it can use a table scan if optimizer thinks it will be faster. If you use FORCE INDEX then you MAKE optimizer to use this index even if it thinks a table scan is more efficient. Optimizer will use a table scan only if there is no way to use index to find rows.也就是说， 使用 USE INDEX 时 sql 优化可能会进行全表扫描，如果全表扫描更快的话； 而使用 FORCE INDEX 时，只能是使用索引，除非查询条件没有命中索引 指定使用索引还有一个使用场景，就是解决之前说到的 范围查询时，索引无效 除了指定要使用的索引，还能指定不使用哪些索引 排除掉不使用的索引，就能使用想要使用的索引]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[synchronized 关键字]]></title>
    <url>%2F2016%2F07%2F27%2F%E6%8A%80%E6%9C%AF%2Fjava%2F2016-07-27-synchronized%20%E5%87%A0%E7%A7%8D%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[synchronized 几种使用方法比较synchronized 关键字在 Java 中出现的很早， Java 提供的实现原子性的内置锁机制，就是用这个关键字实现。synchronized 关键字可以修饰方法，也可以修饰一个代码块，实现一种互斥锁，这里就不做赘述。 下面讨论几个问题 1. synchronized 关键字修饰 非静态方法， 锁定的是对象实例还是类？123456789101112131415161718192021222324252627282930public synchronized void syncInnerReferenceLock1() &#123; for (int i = 0; i &lt; COUNT; i++) &#123; System.out.println("syncInnerReferenceLock1: [count] -- " + i); &#125;&#125;public synchronized void syncInnerReferenceLock2() &#123; for (int i = 0; i &lt; COUNT; i++) &#123; System.out.println("syncInnerReferenceLock2: [count] -- " + i); &#125;&#125;public static void main(String[] args) &#123; final SyncLockDemo syncLockDemo1 = new SyncLockDemo(); Thread t1 = new Thread(new Runnable() &#123; public void run() &#123; syncLockDemo1.syncInnerReferenceLock1(); &#125; &#125;); Thread t2 = new Thread(new Runnable() &#123; public void run() &#123; syncLockDemo1.syncInnerReferenceLock2(); &#125; &#125;); t1.start(); t2.start();&#125; 上面的代码中有两个用 synchronized 关键字修饰的非静态方法， 在两个线程中运行这两个方法， 结果如下 syncInnerReferenceLock2: [count] -- 0 syncInnerReferenceLock2: [count] -- 1 syncInnerReferenceLock2: [count] -- 2 syncInnerReferenceLock2: [count] -- 3 syncInnerReferenceLock2: [count] -- 4 syncInnerReferenceLock1: [count] -- 0 syncInnerReferenceLock1: [count] -- 1 syncInnerReferenceLock1: [count] -- 2 syncInnerReferenceLock1: [count] -- 3 syncInnerReferenceLock1: [count] -- 4 Process finished with exit code 0 或 syncInnerReferenceLock1: [count] -- 0 syncInnerReferenceLock1: [count] -- 1 syncInnerReferenceLock1: [count] -- 2 syncInnerReferenceLock1: [count] -- 3 syncInnerReferenceLock1: [count] -- 4 syncInnerReferenceLock2: [count] -- 0 syncInnerReferenceLock2: [count] -- 1 syncInnerReferenceLock2: [count] -- 2 syncInnerReferenceLock2: [count] -- 3 syncInnerReferenceLock2: [count] -- 4 Process finished with exit code 0从结果可以看出，两个线程是串行执行的，我们可以猜测： 修饰非静态方法的 synchronized 关键字锁定的是这个类的对象实例，下面我们来验证这个想法： 12345678910111213141516171819202122232425262728293031public synchronized void syncInnerReferenceLock1() &#123; for (int i = 0; i &lt; COUNT; i++) &#123; System.out.println("syncInnerReferenceLock1: [count] -- " + i); &#125;&#125;public synchronized void syncInnerReferenceLock2() &#123; for (int i = 0; i &lt; COUNT; i++) &#123; System.out.println("syncInnerReferenceLock2: [count] -- " + i); &#125;&#125;public static void main(String[] args) &#123; final SyncLockDemo syncLockDemo1 = new SyncLockDemo(); final SyncLockDemo syncLockDemo2 = new SyncLockDemo(); Thread t1 = new Thread(new Runnable() &#123; public void run() &#123; syncLockDemo1.syncInnerReferenceLock1(); &#125; &#125;); Thread t2 = new Thread(new Runnable() &#123; public void run() &#123; syncLockDemo2.syncInnerReferenceLock2(); &#125; &#125;); t1.start(); t2.start();&#125; 我们创建两个对象实例， 在两个线程中分别执行两个被 synchronized 关键字修饰的方法， 结果如下： syncInnerReferenceLock2: [count] -- 0 syncInnerReferenceLock1: [count] -- 0 syncInnerReferenceLock2: [count] -- 1 syncInnerReferenceLock1: [count] -- 1 syncInnerReferenceLock1: [count] -- 2 syncInnerReferenceLock1: [count] -- 3 syncInnerReferenceLock1: [count] -- 4 syncInnerReferenceLock2: [count] -- 2 syncInnerReferenceLock2: [count] -- 3 syncInnerReferenceLock2: [count] -- 4 Process finished with exit code 0这里可以看到，连个线程是并行执行的，没有丝毫互斥的现象，从而论证了上面的猜测！ 2. synchronized 关键字修饰 静态方法， 锁定的是对象实例还是类？看了第一个问题，我们能不能说， synchronized 关键字修饰 静态方法， 锁定的是类呢？ 让我们看看 1234567891011121314151617181920212223242526272829public static synchronized void syncLock1() &#123; for (int i = 0; i &lt; COUNT; i++) &#123; System.out.println("syncLock1: [count] -- " + i); &#125;&#125;public static synchronized void syncLock2() &#123; for (int i = 0; i &lt; COUNT; i++) &#123; System.out.println("syncLock2: [count] -- " + i); &#125;&#125;public static void main(String[] args) &#123; Thread t1 = new Thread(new Runnable() &#123; public void run() &#123; syncLock1(); &#125; &#125;); Thread t2 = new Thread(new Runnable() &#123; public void run() &#123; syncLock2(); &#125; &#125;); t1.start(); t2.start();&#125; 这里把 1 中的两个方法修改成静态方法， 在两个线程中分别执行两个方法，结果如下： syncLock1: [count] -- 0 syncLock1: [count] -- 1 syncLock1: [count] -- 2 syncLock1: [count] -- 3 syncLock1: [count] -- 4 syncLock2: [count] -- 0 syncLock2: [count] -- 1 syncLock2: [count] -- 2 syncLock2: [count] -- 3 syncLock2: [count] -- 4 Process finished with exit code 0 或 syncLock2: [count] -- 0 syncLock2: [count] -- 1 syncLock2: [count] -- 2 syncLock2: [count] -- 3 syncLock2: [count] -- 4 syncLock1: [count] -- 0 syncLock1: [count] -- 1 syncLock1: [count] -- 2 syncLock1: [count] -- 3 syncLock1: [count] -- 4 Process finished with exit code 0可以从结果中看出，两个线程是串行执行，存在明显的互斥，synchronized 关键字修饰 静态方法， 锁定的确实是类 3. synchronized(reference) {} 和 synchronized(Clazz.class) {}synchorinizable 修饰类和对象的区别是什么呢？ 在我看来， synchorinizable 修饰类和对象，可以抽象为 类锁 和 对象锁 下面上代码 这是一个 类锁 的范例 1234567public static void syncClazzLock () &#123; synchronized (SyncLockDemo.class) &#123; for (int i = 0; i &lt; COUNT; i++) &#123; System.out.println("syncClazzLock: [count] -- " + i); &#125; &#125;&#125; 这是个 对象锁 1234567public void syncReferenceLock() &#123; synchronized (this) &#123; for (int i = 0; i &lt; COUNT; i++) &#123; System.out.println("syncReferenceLock: [count] -- " + i); &#125; &#125;&#125; 调用上面的锁 123456789101112131415161718public static void main(String[] args) &#123; final SyncLockDemo syncLockDemo = new SyncLockDemo(); Thread t1 = new Thread(new Runnable() &#123; public void run() &#123; syncClazzLock(); &#125; &#125;); Thread t3 = new Thread(new Runnable() &#123; public void run() &#123; syncLockDemo.syncReferenceLock(); &#125; &#125;); t1.start(); t3.start(); &#125; 程序运行结果 syncReferenceLock: [count] -- 0 syncClazzLock: [count] -- 0 syncReferenceLock: [count] -- 1 syncClazzLock: [count] -- 1 syncReferenceLock: [count] -- 2 syncClazzLock: [count] -- 2 syncReferenceLock: [count] -- 3 syncReferenceLock: [count] -- 4 syncClazzLock: [count] -- 3 syncClazzLock: [count] -- 4 Process finished with exit code 0从结果可以看出， 这两个线程的运行顺序是并行的，并不互斥。 因为 syncClazzLock 中 synchronized 关键字修饰的是类， 这个 类锁 作用于类的范围， 作用于类的静态方法、类的class 对象、类的静态代码块；而 syncReferenceLock 方法中 synchronized 关键字修饰的是对象， 这个 对象锁 最用于对象的范围，作用的对象示例的方法或一个对象实例上]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JAVA 并发编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring揭秘读书笔记 —— 如何干涉IOC容器]]></title>
    <url>%2F2016%2F07%2F16%2F%E6%8A%80%E6%9C%AF%2Fspring%2F2016-07-16-Spring%E6%8F%AD%E7%A7%98%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%A6%82%E4%BD%95%E5%B9%B2%E6%B6%89IOC%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[前言Spring IOC 容器在加载 Bean 的时候，可以大致分为两个阶段 容器启动阶段 Bean 实例化阶段 插手容器启动阶段容器启动阶段简而言之就是将 通过注解或者在 xml 文件中配置信息，解析转化为 BeanDefinition， 再通过 BeanDefinitionRegister 注册到容器中这个阶段主要是一些收集准备工作 Spring 提供给我们插手这一阶段的方式是： 实现 BeanFactoryPostProcessor]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>《Spring揭秘》</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring揭秘读书笔记 —— IOC 基本概念]]></title>
    <url>%2F2016%2F07%2F15%2F%E6%8A%80%E6%9C%AF%2Fspring%2F2016-07-15-Spring%E6%8F%AD%E7%A7%98%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94IOC%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[前言一直在使用Spring 这一开源项目， 但是在学习spring 的过程中一直没遇到让我心旷神怡的好书，看过 《Spring技术内幕：深入解析Spring架构与设计原理（第2版）》， 从源码角度讲解spring， 虽然也很不错， 但是略感枯燥。 另外就是开涛大神的 跟我学Spring(http://jinnianshilongnian.iteye.com/blog/1482071) 和 跟我学Spring MVC(http://jinnianshilongnian.iteye.com/blog/1617451), 这两个系列的博客，主要是从使用Spring 的角度出发，很适合初学者系列的学习Spring 的使用以及一些Spring 的原理。 以上对两位大神的著作的评论仅属个人言论，欢迎大家指正最近接触到一本比较 古老 的 Spring 学习书籍 ———— 《Spring揭秘》 , 这本书貌似现在已经停刊了，讲的 Spring 也是将的 Spring 2.X , 但是 Spring 的主要思想，在这本书中被作者用一种通俗易懂的语言表达的让人能一边看，一边笑着点头， 甚是舒畅。 IoC的基本概念读书笔记IoC [控制反转] ———— 我们的理念是：让别人为你服务书中的比喻很形象 常见的 Ioc 实现方法 构造方法注入 setter方法注入 接口注入 三种方法中 接口注入 较为难理解被注入的对象要想 Ioc Service Provider 为其注入依赖对象， 就要实现一个特定接口，特定的接口提供一个方法，用来为其注入一个依赖对象，这个特性接口就如同是上图比喻中的 “拿衣服的女朋友” 示例：FxNewsProvider 希望能被注入依赖 IFXNewsListener, 使用接口注入时，实现 FXNewsListenerCallable 接口 FXNewListenerCallable 接口提供了 injectNewsListener 方法, 这个方法的参数的类型就是 IFXNewsListenr 接口注入这种注入方式目前已经过时，不提倡使用 构造方法注入 在对象构造完之后，就会立即进入就绪状态，可以马上使用。但是如果依赖的对象比较多，构造方法的参数列表会比较长，而且构造方法注入底层实现还是基于反射机制，而反射机制对于构造方法中相同类型的参数处理会有困难；而且构造方法不能被继承，不能设置默认值 setter方法注入 setter 方法参数单一，反射机制可以很好的支持，而且setter 方法能被继承，能够设置默认值]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>《Spring揭秘》</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring揭秘读书笔记 —— IOC BeanFactory]]></title>
    <url>%2F2016%2F07%2F15%2F%E6%8A%80%E6%9C%AF%2Fspring%2F2016-07-15-Spring%E6%8F%AD%E7%A7%98%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94IOC%E4%B9%8BBeanFactory%2F</url>
    <content type="text"><![CDATA[BeanFactorySpring 的Ioc 容器除了是 Ioc Service Provider 还提供了其他的功能， 这边笔记将介绍 Ioc 容器的 Ioc 相关支持以及衍生的高级特性 Spring 中提供两种IOC 容器， BeanFactory 和 ApplicationContext BeanFactory:基本类型的ICO 容器， 提供完整的IOC支持。 默认采用延迟初始化策略(lazy-load) 只有客户端对象需要访问容器中某个收管理的对象的时候， 才对该受管理的对象进行初始化以及依赖注入的操作。 ApplicationContext留待下章讲解 先看看 BeanFactory 的定义 1234567891011121314151617181920212223242526272829303132333435363738394041424344public interface BeanFactory &#123; /** * Used to dereference a FactoryBean instance and distinguish it from beans created by the FactoryBean. * 这个前缀用于区分FactoryBean， 当想从BeanFactory 中获取一个FactoryBean 对象的时候，会返回这个工厂类 * For example, if the bean named myJndiObject is a FactoryBean, getting myJndiObject will return the factory, not the instance returned by the factory. */ String FACTORY_BEAN_PREFIX = "&amp;"; Object getBean(String name) throws BeansException; &lt;T&gt; T getBean(String name, Class&lt;T&gt; requiredType) throws BeansException; &lt;T&gt; T getBean(Class&lt;T&gt; requiredType) throws BeansException; /** * @param name the name of the bean to retrieve * @param args arguments to use when creating a bean instance using explicit arguments * (only applied when creating a new instance as opposed to retrieving an existing one) * @since 2.5 */ Object getBean(String name, Object... args) throws BeansException; /** * @param args arguments to use when creating a bean instance using explicit arguments * @since 4.1 */ &lt;T&gt; T getBean(Class&lt;T&gt; requiredType, Object... args) throws BeansException; boolean containsBean(String name); boolean isSingleton(String name) throws NoSuchBeanDefinitionException; boolean isPrototype(String name) throws NoSuchBeanDefinitionException; boolean isTypeMatch(String name, ResolvableType typeToMatch) throws NoSuchBeanDefinitionException; boolean isTypeMatch(String name, Class&lt;?&gt; typeToMatch) throws NoSuchBeanDefinitionException; Class&lt;?&gt; getType(String name) throws NoSuchBeanDefinitionException; String[] getAliases(String name);&#125; 讲到了 BeanFactory 不得不提 BeanDefinitionRegister Spring IOC 中的 BeanDefinition 封装了一个被管理的Bean 的所有信息， 再通过 BeanDefinitionRegister 将 Bean 注册到 IOC 容器中去 本章章节比较易懂， 这里主要讲讲没这么提及的FactoryBean。 FactoryBean :FactoryBean 从命名上看跟 BeanFactory 很容易混淆。 FactoryBean 是 Spring 提供的一种可以扩展容器对象实例化逻辑的接口， 这个命名主语是Bean，定语是Factory； 也就是说它是 Spring 管理的一个普通的Bean， 只是它相对于生产对象来说，它是一个工厂。 1234567&lt;bean class="com.liam.learn.ioc.factorybean.DateWithFactoryBean" id="dateWithFactoryBean"&gt; &lt;property name="dateTimeFormatter"&gt; &lt;ref bean="dateTimeFormatter" /&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean id="dateTimeFormatter" class="org.springframework.format.datetime.standard.DateTimeFormatterFactoryBean"/&gt; 获取FactoryBean 的方法 1234public static void main(String[] args) &#123; ClassPathXmlApplicationContext applicationContext = new ClassPathXmlApplicationContext("spring-config.xml"); DateTimeFormatterFactoryBean dateTimeFormatter = (DateTimeFormatterFactoryBean) applicationContext.getBean("&amp;dateTimeFormatter");&#125; BeanFactory 中的 getObjectForBeanInstance 方法 12345678910111213141516171819202122232425262728293031323334protected Object getObjectForBeanInstance( Object beanInstance, String name, String beanName, RootBeanDefinition mbd) &#123; // Don't let calling code try to dereference the factory if the bean isn't a factory. // 如果 bean 的名称以'&amp;'开头 并且 bean的类型不属于 FactoryBean 将抛出异常 if (BeanFactoryUtils.isFactoryDereference(name) &amp;&amp; !(beanInstance instanceof FactoryBean)) &#123; throw new BeanIsNotAFactoryException(transformedBeanName(name), beanInstance.getClass()); &#125; // Now we have the bean instance, which may be a normal bean or a FactoryBean. // If it's a FactoryBean, we use it to create a bean instance, unless the // caller actually wants a reference to the factory. // 如果当前 Bean 的引用类型不是 FactoryBean 或者 bean 的名称以 '&amp;' 开头直接返回这个引用 if (!(beanInstance instanceof FactoryBean) || BeanFactoryUtils.isFactoryDereference(name)) &#123; return beanInstance; &#125; Object object = null; if (mbd == null) &#123; // 从缓存中获取FactoryBean 的引用 object = getCachedObjectForFactoryBean(beanName); &#125; // 如果缓存中没有这个FactoryBean 的引用； 将新建引用，存到缓存中，并返回 if (object == null) &#123; // Return bean instance from factory. FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) beanInstance; // Caches object obtained from FactoryBean if it is a singleton. if (mbd == null &amp;&amp; containsBeanDefinition(beanName)) &#123; mbd = getMergedLocalBeanDefinition(beanName); &#125; boolean synthetic = (mbd != null &amp;&amp; mbd.isSynthetic()); object = getObjectFromFactoryBean(factory, beanName, !synthetic); &#125; return object;&#125;]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>《Spring揭秘》</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jackson 中的 Features]]></title>
    <url>%2F2016%2F07%2F10%2F%E6%8A%80%E6%9C%AF%2Fjava%2F2016-07-10-jackson%E4%B8%AD%E7%9A%84Feature%2F</url>
    <content type="text"><![CDATA[Jackson 中的各种 Features先来看看 jackson 中的各种feature 首先是 序列化时候的Feature —— SerializaitonFeature WRAP_ROOT_VALUE(false) : 序列化的json是否显示根节点123456789101112131415public static void testWrapRoot() &#123; SimpleBean simpleBean = new SimpleBean(); simpleBean.setCode(1); simpleBean.setName("TEST_WRAP_ROOT_VALUE"); simpleBean.setDesc(Lists.newArrayList("test1", "test2", "test3")); ObjectMapper objectMapper = new ObjectMapper();// objectMapper.enable(SerializationFeature.WRAP_ROOT_VALUE); try &#123; String s = objectMapper.writeValueAsString(simpleBean); System.out.println(s); &#125; catch (JsonProcessingException e) &#123; e.printStackTrace(); &#125; &#125; 运行结果： {&quot;name&quot;:&quot;TEST_WRAP_ROOT_VALUE&quot;,&quot;code&quot;:1,&quot;desc&quot;:[&quot;test1&quot;,&quot;test2&quot;,&quot;test3&quot;]}去掉注释的运行结果： {&quot;SimpleBean&quot;:{&quot;name&quot;:&quot;TEST_WRAP_ROOT_VALUE&quot;,&quot;code&quot;:1,&quot;desc&quot;:[&quot;test1&quot;,&quot;test2&quot;,&quot;test3&quot;]}}从结果可以看出 jackson 会把类名作为根节点展示 INDENT_OUTPUT(false): 允许或禁止是否以缩进的方式展示json123456789101112131415public static void testIdentOutput() &#123; SimpleBean simpleBean = new SimpleBean(); simpleBean.setCode(1); simpleBean.setName("TEST_WRAP_ROOT_VALUE"); simpleBean.setDesc(Lists.newArrayList("test1", "test2", "test3")); ObjectMapper objectMapper = new ObjectMapper(); objectMapper.enable(SerializationFeature.INDENT_OUTPUT); try &#123; String s = objectMapper.writeValueAsString(simpleBean); System.out.println(s); &#125; catch (JsonProcessingException e) &#123; e.printStackTrace(); &#125;&#125; 输出的结果： { &quot;name&quot; : &quot;TEST_WRAP_ROOT_VALUE&quot;, &quot;code&quot; : 1, &quot;desc&quot; : [ &quot;test1&quot;, &quot;test2&quot;, &quot;test3&quot; ] } FAIL_ON_EMPTY_BEANS(true): 当类的一个属性外部无法访问(如：没有getter setter 的私有属性)，且没有annotation 标明需要序列化时，如果FAIL_ON_EMPTY_BEANS 是true 将会跑出异常，如果是false 则不会跑出异常12345678910111213141516public static void testFailOnEmptyField() &#123; TestBean simpleBean = new TestBean(); ObjectMapper objectMapper = new ObjectMapper();// objectMapper.disable(SerializationFeature.FAIL_ON_EMPTY_BEANS); try &#123; String s = objectMapper.writeValueAsString(simpleBean); System.out.println(s); &#125; catch (JsonProcessingException e) &#123; e.printStackTrace(); &#125; &#125; private static class TestBean &#123; private String name; &#125; 代码运行结果 com.fasterxml.jackson.databind.JsonMappingException: No serializer found for class com.liam.learning.Main$TestBean and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) at com.fasterxml.jackson.databind.JsonMappingException.from(JsonMappingException.java:275) at com.fasterxml.jackson.databind.SerializerProvider.mappingException(SerializerProvider.java:1109) at com.fasterxml.jackson.databind.SerializerProvider.reportMappingProblem(SerializerProvider.java:1134) at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:69) at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:32) at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:292) at com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue(ObjectMapper.java:3672) at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:3048) at com.liam.learning.Main.testFailOnEmptyField(Main.java:56) at com.liam.learning.Main.main(Main.java:15) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)取消注释的运行结果： {} FAIL_ON_SELF_REFERENCES(true)如果POJO 中有一个直接自我引用，在序列化的时候会抛出 com.fasterxml.jackson.databind.JsonMappingException WRAP_EXCEPTIONS(true)如果序列化过程中，如果抛出 Exception 将会被包装，添加额外的上下文信息 FAIL_ON_UNWRAPPED_TYPE_IDENTIFIERS(true),CLOSE_CLOSEABLE(false),FLUSH_AFTER_WRITE_VALUE(true),WRITE_DATES_AS_TIMESTAMPS(true),WRITE_DATE_KEYS_AS_TIMESTAMPS(false),WRITE_DATES_WITH_ZONE_ID(false),WRITE_DURATIONS_AS_TIMESTAMPS(true), WRITE_CHAR_ARRAYS_AS_JSON_ARRAYS(false): 序列化json 的时候对于 char[] 为true 则解析成数组形式； 为false 则解析成一个字符串(String)示例代码 12345678910111213private static void testWriteCharArraysAsJsonArrays() &#123; String t = "tester"; char[] arr = t.toCharArray(); ObjectMapper objectMapper = new ObjectMapper();// objectMapper.enable(SerializationFeature.WRITE_CHAR_ARRAYS_AS_JSON_ARRAYS); try &#123; String s = objectMapper.writeValueAsString(arr); System.out.println(s); &#125; catch (JsonProcessingException e) &#123; e.printStackTrace(); &#125; &#125; 运行结果如下： &quot;tester&quot;去掉注释后，运行结果如下 [&quot;t&quot;,&quot;e&quot;,&quot;s&quot;,&quot;t&quot;,&quot;e&quot;,&quot;r&quot;] WRITE_ENUMS_USING_TO_STRING(false): 序列化时， enable: 用枚举的 enum.toString() 表示枚举值 disable: 用枚举的 enum.name() 表示枚举值12345678910111213141516171819private static void testWriteEnumsUsingString() &#123; ObjectMapper objectMapper = new ObjectMapper();// objectMapper.enable(SerializationFeature.WRITE_ENUMS_USING_TO_STRING); try &#123; String s = objectMapper.writeValueAsString(CHILD.BOY); System.out.println(s); &#125; catch (JsonProcessingException e) &#123; e.printStackTrace(); &#125;&#125;private enum CHILD &#123; BOY, GIRL; @Override public String toString() &#123; return "+" + this.name() + "+"; &#125;&#125; 代码运行结果： &quot;BOY&quot;去掉注释后的代码运行结果： &quot;+BOY+&quot; WRITE_ENUMS_USING_INDEX(false): 序列化时，用枚举的 enum.index() 表示枚举值1234567891011121314private static void testWriteEnumsUsingIndex() &#123; ObjectMapper objectMapper = new ObjectMapper();// objectMapper.enable(SerializationFeature.WRITE_ENUMS_USING_INDEX); try &#123; String s = objectMapper.writeValueAsString(CHILD.BOY); System.out.println(s); &#125; catch (JsonProcessingException e) &#123; e.printStackTrace(); &#125;&#125;private enum CHILD &#123; BOY, GIRL;&#125; 代码运行结果： &quot;BOY&quot;取消注释后代码运行结果： 0 WRITE_NULL_MAP_VALUES(true): 对于 Map 中的 null 值 是否序列化1234567891011121314public static void testWriteNullMapValues() &#123; HashMap&lt;String, Object&gt; extMap = Maps.newHashMap(); extMap.put("test1", null); extMap.put("test2", "not null"); ObjectMapper objectMapper = new ObjectMapper();// objectMapper.disable(SerializationFeature.WRITE_NULL_MAP_VALUES); try &#123; String s = objectMapper.writeValueAsString(extMap); System.out.println(s); &#125; catch (JsonProcessingException e) &#123; e.printStackTrace(); &#125;&#125; 代码运行结果： {&quot;test1&quot;:null,&quot;test2&quot;:&quot;not null&quot;}去除注释后的运行结果： {&quot;test2&quot;:&quot;not null&quot;} WRITE_EMPTY_JSON_ARRAYS(true): 对于空的 Collection、 数组 ; 为 true 则序列化， 为 false 则跳过， 默认为 true示例代码： 123456789101112131415private static void testWriteEmptyJsonArrays() &#123; SimpleBean simpleBean = new SimpleBean(); simpleBean.setCode(1); simpleBean.setName("WRITE_EMPTY_JSON_ARRAYS"); simpleBean.setDesc(Collections.&lt;String&gt;emptyList()); ObjectMapper objectMapper = new ObjectMapper();// objectMapper.disable(SerializationFeature.WRITE_EMPTY_JSON_ARRAYS); try &#123; String s = objectMapper.writeValueAsString(simpleBean); System.out.println(s); &#125; catch (JsonProcessingException e) &#123; e.printStackTrace(); &#125;&#125; 运行结果如下： {&quot;name&quot;:&quot;WRITE_EMPTY_JSON_ARRAYS&quot;,&quot;code&quot;:1,&quot;desc&quot;:[],&quot;ext&quot;:null}取消注释后，运行结果如下： {&quot;name&quot;:&quot;WRITE_EMPTY_JSON_ARRAYS&quot;,&quot;code&quot;:1,&quot;ext&quot;:null} WRITE_SINGLE_ELEM_ARRAYS_UNWRAPPED(false): 序列化json时，对于只有单个元素的数组，不用中括号(‘[]’)括起来123456789101112131415public static void testWrietSingleElemArraysUnwrapped() &#123; SimpleBean simpleBean = new SimpleBean(); simpleBean.setCode(1); simpleBean.setName("TEST_WRAP_ROOT_VALUE"); simpleBean.setDesc(Lists.newArrayList("test1")); ObjectMapper objectMapper = new ObjectMapper();// objectMapper.enable(SerializationFeature.WRITE_SINGLE_ELEM_ARRAYS_UNWRAPPED); try &#123; String s = objectMapper.writeValueAsString(simpleBean); System.out.println(s); &#125; catch (JsonProcessingException e) &#123; e.printStackTrace(); &#125;&#125; 代码运行结果： {&quot;name&quot;:&quot;TEST_WRAP_ROOT_VALUE&quot;,&quot;code&quot;:1,&quot;desc&quot;:[&quot;test1&quot;]}去掉注释之后的，运行结果 {&quot;name&quot;:&quot;TEST_WRAP_ROOT_VALUE&quot;,&quot;code&quot;:1,&quot;desc&quot;:&quot;test1&quot;}效果一目了然！ WRITE_BIGDECIMAL_AS_PLAIN(false): @deprecated@see com.fasterxml.jackson.core.JsonGenerator.Feature#WRITE_BIGDECIMAL_AS_PLAIN WRITE_DATE_TIMESTAMPS_AS_NANOSECONDS(true): 序列化json的时候把时间类型值序列化成纳秒的形式`注意` 只有最新版本(`jdk8` 中的 `Date/Time` )的时间类型支持本特性, `jdk8` 之前的 `java.util.Date` 和`joda-time` 都不支持！ ORDER_MAP_ENTRIES_BY_KEYS(false): 序列化 Map 的时候， 为 true 则按照 Map 的 key 进行排序，否则不排序示例代码： 1234567891011private static void testOrderMapEntriesByKeys() &#123; ObjectMapper objectMapper = new ObjectMapper();// objectMapper.enable(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS); Map&lt;String, String&gt; data = ImmutableMap.of("1", "test1", "3", "test2", "2", "test3", "0", "test4"); try &#123; String dateStr = objectMapper.writeValueAsString(data); System.out.println(dateStr); &#125; catch (JsonProcessingException e) &#123; e.printStackTrace(); &#125;&#125; 运行结果： {&quot;1&quot;:&quot;test1&quot;,&quot;3&quot;:&quot;test2&quot;,&quot;2&quot;:&quot;test3&quot;,&quot;0&quot;:&quot;test4&quot;}取消注释的运行结果： {&quot;0&quot;:&quot;test4&quot;,&quot;1&quot;:&quot;test1&quot;,&quot;2&quot;:&quot;test3&quot;,&quot;3&quot;:&quot;test2&quot;} EAGER_SERIALIZER_FETCH(true): 序列化是是否应当预先抓取必要的 JsonSerializer ， 绝大多数情况下不应该关闭此特性 USE_EQUALITY_FOR_OBJECT_ID(false); 接下来是 反序列化时候的Feature —— DeserializationFeature先来个最常用的： FAIL_ON_UNKNOWN_PROPERTIES ： 反序列化遇到未知的字段的时候是否失败， 默认是 true 会失败]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Json</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MarkDown 基本语法]]></title>
    <url>%2F2016%2F07%2F06%2F%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2Fmarkdown%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言看到同事用 [git+github+markdown+jekyll] 搭建了一个个人博客， 感觉很不错，果断 fork 他的分支，给自己来一个，下面简单记一些 [markdown]的语法，或者说占位符的用法 MarkDown 基本语法 标题：1. 在文字标题之前加上 “#” 表示标题， 通过 “#” 的数量表示标题等，一个”#”表示一级，两个表示二级，最多六级例：# 用&quot;#&quot;表示标题 # 一级标题 ## 二级标题 ... ####### 六级标题一级标题二级标题… 六级标题 2. 在文字下方加上两个以上的 “-“ 表示标题, 等同于二级标题 (“##”) 例： 在文字下方加上两个以上 &quot;-&quot; 表示标题 ----------------------------------两个以上 ‘-‘ 表示二级标题3. 在文字下方加上两个以上的 “=” 表示标题，等同于一级标题 (“#”) 例：在文字下方加上 &quot;=&quot; 表示标题(两个以上&quot;=&quot;) =======================两个以上 ‘=’ 表示一级标题粗体1. 在 文字 前后 加上 两个 ‘_’ 或 ‘*’例：__n__ 或 **n**粗体 斜体1. 在文字前后加上一个 ‘_’ 或 ‘*’例： _斜体_ 或 *斜体*斜体 粗斜体1. 在文字前后加上三个 ‘_’ 或 ‘*’例： ___斜粗体___ 或 ***斜粗体***粗斜体 粗斜体 代码块1. 行首加上四个 Tab 或 四个空格 (代码块自带滚动条效果)+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ ------------------------------------------------------------- ------------------ -------区块引用1. 文字前面加上 “&gt;”例： &gt; 这个一个区块引用 区块引用效果如下 2. 区块引用可以嵌套使用例： &gt; 这是外层 &gt;&gt; 这是内层 &gt;&gt;&gt; 这里更内层示例效果： 这里外层 这是内层 这是更内层这也是内层 注意如果 &gt; 和 &gt;&gt; 嵌套使用的话，从 &gt;&gt; 退到 &gt; 时，必须之间要加一个空格或者 &gt; 作为过渡，否则默认为下一行和上一行是同一级别的引用。如示例所示。示例效果： 这里外层 这是内层 这是更内层 这也是内层 分割线1. 三个以上”-“例： ----示例效果如下 2. 三个以上”*”例： ***示例效果如下 代码块和语法高亮1. 代码块使用方法如下：1echo("this a js code Block");代码块效果如下 12var tip = "this a js code block";echo(tip); 2. 代码块语法高亮 在 “后加上 language1234```javapublic static void main(String[] args) &#123; System.out.println(&quot;hello markdown&quot;);&#125;3. 高亮部分文字`高亮部分`高亮效果示例 超链接1. “[链接显示文字](url “超链接标题”)“ url 和标题之间有空格例： [百度](http://www.baidu.com &quot;百度&quot;)示例效果: 百度 2. 指向本地文件的链接例: [icon.png](/assets/picture/headPicture.png)icon.png 图片引用1. 图片引用使用方法如下例： ![图片](图片路径 &quot;图片标题&quot;) 转义字符1. 在需要转义的字符前面加上’&#39;例： 这是用来 _演示_ 的 **文本** 这是用来 \_演示\_ 的 \**文本\**这是用来 演示 的 文本 这是用来 _演示_ 的 **文本** 删除线1. 在文字前后加上两个 ‘~’例： ~~删除线~~示例效果删除线 表格1. 使用 | 来分隔不同的单元格，使用 - 来分隔表头和其他行：例： | name | age | | ---- | --- | | zero | 24 | | liam | 23 |示例效果： name age zero 24 liam 23 2. 表格对其, 在表头和其他行的分隔符中加上 ‘:’‘:—‘ 左对齐‘:–:’ 居中‘—:’ 右对齐例： | left | center | right | | :--- | :----: | ----: | |左 |中 |右 |示例效果: left center right 左 中 右 上标和下标1. 上标 使用html标签 &lt;sup&gt;例： 来个上标 &lt;sup&gt;[1]&lt;/sup&gt;示例效果: 来个上标[1] 2. 下标 使用html 标签 &lt;sub&gt;例: 来个下标 &lt;sub&gt;[2]&lt;/sub&gt;示例效果: 来个下标[2]]]></content>
      <categories>
        <category>GitHub Pages</category>
      </categories>
      <tags>
        <tag>MarkDown</tag>
      </tags>
  </entry>
</search>
